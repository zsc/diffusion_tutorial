<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) - Diffusion Models Tutorial</title>
    <link rel="stylesheet" href="common.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script src="common.js"></script>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="chapter2.html">â† ä¸Šä¸€ç« </a>
            <span>ç¬¬3ç«  / å…±14ç« </span>
            <a href="chapter4.html">ä¸‹ä¸€ç«  â†’</a>
        </div>
        
        <h1>ç¬¬3ç« ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM)</h1>
        
        <div class="chapter-intro">
            2020å¹´ï¼ŒHoç­‰äººçš„è®ºæ–‡"Denoising Diffusion Probabilistic Models"è®©æ‰©æ•£æ¨¡å‹çœŸæ­£è¿›å…¥äº†å®ç”¨é˜¶æ®µã€‚DDPMä¸ä»…ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜è¾¾åˆ°äº†ä¸GANç›¸åª²ç¾çš„ç”Ÿæˆè´¨é‡ã€‚æœ¬ç« å°†æ·±å…¥å‰–æDDPMçš„æ•°å­¦åŸç†ã€è®­ç»ƒç®—æ³•å’Œå®ç°ç»†èŠ‚ã€‚é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†æŒæ¡å¦‚ä½•ä»é›¶å®ç°ä¸€ä¸ªå®Œæ•´çš„DDPMï¼Œå¹¶ç†è§£å…¶èƒŒåçš„æ¦‚ç‡è®ºåŸºç¡€ã€‚
        </div>
        
        <h2>3.1 DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼šç®€åŒ–ä¸ç»Ÿä¸€</h2>
        
        <p>åœ¨DDPMä¹‹å‰ï¼Œæ‰©æ•£æ¨¡å‹è™½ç„¶ç†è®ºä¼˜é›…ï¼Œä½†å®è·µå›°éš¾ã€‚2015å¹´Sohl-Dicksteinç­‰äººçš„å¼€åˆ›æ€§å·¥ä½œéœ€è¦ä¼°è®¡æ•´ä¸ªåå‘è¿‡ç¨‹çš„ç†µï¼Œè®­ç»ƒæå…¶å¤æ‚ã€‚DDPMçš„é©å‘½æ€§è´¡çŒ®åœ¨äºï¼š<strong>å°†å¤æ‚çš„å˜åˆ†æ¨æ–­ç®€åŒ–ä¸ºç®€å•çš„å»å™ªä»»åŠ¡</strong>ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">DDPMçš„ä¸‰ä¸ªå…³é”®ç®€åŒ–</div>
            <ol>
                <li><strong>å›ºå®šæ–¹å·®è°ƒåº¦</strong>ï¼šå‰å‘è¿‡ç¨‹ä½¿ç”¨é¢„å®šä¹‰çš„ $\beta_t$ åºåˆ—ï¼Œæ— éœ€å­¦ä¹ </li>
                <li><strong>ç®€åŒ–åå‘è¿‡ç¨‹</strong>ï¼šå‡è®¾åå‘è¿‡ç¨‹ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œåªéœ€å­¦ä¹ å‡å€¼ï¼ˆå®é™…ä¸Šæ˜¯å­¦ä¹ å™ªå£°ï¼‰</li>
                <li><strong>é‡å‚æ•°åŒ–ç›®æ ‡</strong>ï¼šå°†é¢„æµ‹å‡å€¼è½¬æ¢ä¸ºé¢„æµ‹å™ªå£°ï¼Œå¤§å¹…æå‡è®­ç»ƒç¨³å®šæ€§</li>
            </ol>
        </div>
        
        <h3>3.1.1 ä»å¤æ‚åˆ°ç®€å•ï¼šDDPMçš„æ´å¯Ÿ</h3>
        
        <p>è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç±»æ¯”æ¥ç†è§£DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>å¢¨æ°´æ‰©æ•£çš„ç±»æ¯”</h4>
            <p>æƒ³è±¡ä¸€æ»´å¢¨æ°´åœ¨æ°´ä¸­æ‰©æ•£ï¼š</p>
            <ul>
                <li><strong>å‰å‘è¿‡ç¨‹</strong>ï¼šå¢¨æ°´é€æ¸æ‰©æ•£ï¼Œæœ€ç»ˆå‡åŒ€åˆ†å¸ƒï¼ˆç‰©ç†è¿‡ç¨‹ï¼Œç¡®å®šçš„ï¼‰</li>
                <li><strong>åå‘è¿‡ç¨‹</strong>ï¼šå¦‚ä½•è®©æ‰©æ•£çš„å¢¨æ°´é‡æ–°èšé›†ï¼Ÿï¼ˆéœ€è¦å­¦ä¹ çš„ï¼‰</li>
            </ul>
            <p>DDPMçš„å…³é”®æ´å¯Ÿï¼š<strong>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“"å¢¨æ°´åº”è¯¥å‘å“ªä¸ªæ–¹å‘èšé›†"</strong>ï¼Œè€Œè¿™ä¸ªæ–¹å‘æ°å¥½ä¸æ·»åŠ çš„å™ªå£°æ–¹å‘ç›¸åï¼</p>
        </div>
        
        <h3>3.1.2 æ•°å­¦æ¡†æ¶æ¦‚è§ˆ</h3>
        
        <p>DDPMå®šä¹‰äº†ä¸¤ä¸ªè¿‡ç¨‹ï¼š</p>
        
        <div class="math-block">
            <strong>å‰å‘è¿‡ç¨‹ï¼ˆå›ºå®šï¼‰</strong>ï¼š<br>
            $q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$<br><br>
            
            <strong>åå‘è¿‡ç¨‹ï¼ˆå­¦ä¹ ï¼‰</strong>ï¼š<br>
            $p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2\mathbf{I})$
        </div>
        
        <p>å…³é”®åˆ›æ–°åœ¨äºå¦‚ä½•å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ï¼š</p>
        
        <div class="code-block">
<pre># æ—©æœŸæ–¹æ³•ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆä¸ç¨³å®šï¼‰
mean = model(x_t, t)

# DDPMåˆ›æ–°ï¼šé¢„æµ‹å™ªå£°ï¼ˆç¨³å®šä¸”æœ‰æ•ˆï¼‰
noise_pred = model(x_t, t)
mean = (x_t - beta_t / sqrt(1 - alpha_bar_t) * noise_pred) / sqrt(alpha_t)</pre>
        </div>
        
        <h3>3.1.3 ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿ</h3>
        
        <p>è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„æ”¹å˜å¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼š</p>
        
        <div class="definition">
            <div class="definition-title">é¢„æµ‹å™ªå£°çš„ä¼˜åŠ¿</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ–¹é¢</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">é¢„æµ‹å‡å€¼</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">é¢„æµ‹å™ªå£°</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¾“å‡ºèŒƒå›´</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">éœ€è¦åŒ¹é…æ•°æ®åˆ†å¸ƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ ‡å‡†é«˜æ–¯ï¼ˆå·²å½’ä¸€åŒ–ï¼‰</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">è®­ç»ƒä¿¡å·</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">éštå˜åŒ–å‰§çƒˆ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å„æ—¶é—´æ­¥ç›¸å¯¹ä¸€è‡´</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ¢¯åº¦æµ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å¯èƒ½æ¢¯åº¦æ¶ˆå¤±</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ¢¯åº¦ä¼ æ’­è‰¯å¥½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç‰©ç†æ„ä¹‰</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹å»å™ªåçš„å›¾åƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹æ·»åŠ çš„å™ªå£°</td>
                </tr>
            </table>
        </div>
        
        <h3>3.1.4 DDPM vs æ—©æœŸæ‰©æ•£æ¨¡å‹</h3>
        
        <p>è®©æˆ‘ä»¬å¯¹æ¯”DDPMä¸2015å¹´çš„åŸå§‹æ‰©æ•£æ¨¡å‹ï¼š</p>
        
        <div class="code-block">
<pre># 2015å¹´çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¤æ‚ï¼‰
# éœ€è¦ä¼°è®¡ï¼š
# 1. å‰å‘è¿‡ç¨‹çš„ç†µ
# 2. åå‘è¿‡ç¨‹çš„å®Œæ•´åˆ†å¸ƒ
# 3. å˜åˆ†å‚æ•°çš„ä¼˜åŒ–
# è®­ç»ƒæå…¶ä¸ç¨³å®šï¼Œç”Ÿæˆè´¨é‡å·®

# DDPMï¼ˆ2020å¹´ï¼‰çš„è®­ç»ƒï¼ˆæç®€ï¼‰
for x_0, _ in dataloader:
    t = torch.randint(0, num_timesteps, (batch_size,))
    noise = torch.randn_like(x_0)
    x_t = sqrt_alpha_bar[t] * x_0 + sqrt_one_minus_alpha_bar[t] * noise
    
    noise_pred = model(x_t, t)
    loss = F.mse_loss(noise_pred, noise)
    loss.backward()</pre>
        </div>
        
        <p>è¿™ç§ç®€åŒ–ä¸æ˜¯ä»¥ç‰ºç‰²æ€§èƒ½ä¸ºä»£ä»·çš„â€”â€”ç›¸åï¼ŒDDPMé¦–æ¬¡è®©æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸GANç«äº‰ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚</p>
        
        <div class="exercise">
            <div class="exercise-title">æ€è€ƒé¢˜ 3.1ï¼šç›´è§‰ç†è§£</div>
            <p>ä¸ºä»€ä¹ˆåœ¨é«˜å™ªå£°æƒ…å†µä¸‹ï¼ˆå¤§çš„tï¼‰ï¼Œé¢„æµ‹å™ªå£°æ¯”é¢„æµ‹åŸå§‹å›¾åƒæ›´å®¹æ˜“ï¼Ÿæç¤ºï¼šè€ƒè™‘ä¿¡å™ªæ¯”ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_1" class="answer">
                <p><strong>ç­”æ¡ˆï¼š</strong></p>
                <p>å½“tå¾ˆå¤§æ—¶ï¼Œ$\mathbf{x}_t \approx \mathcal{N}(0, \mathbf{I})$ï¼Œå‡ ä¹æ˜¯çº¯å™ªå£°ã€‚æ­¤æ—¶ï¼š</p>
                <ul>
                    <li>åŸå§‹å›¾åƒ $\mathbf{x}_0$ çš„ä¿¡æ¯å‡ ä¹å®Œå…¨ä¸¢å¤±ï¼Œé¢„æµ‹å®ƒéœ€è¦"å‡­ç©ºæƒ³è±¡"</li>
                    <li>ä½†æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ æ˜¯å·²çŸ¥çš„ï¼Œä¸”å ä¸»å¯¼åœ°ä½</li>
                    <li>ç½‘ç»œåªéœ€è¦è¯†åˆ«å™ªå£°æ¨¡å¼ï¼Œè€Œä¸æ˜¯é‡å»ºå¤æ‚çš„å›¾åƒç»“æ„</li>
                </ul>
                <p>ç±»æ¯”ï¼šåœ¨é›ªèŠ±å™ªå£°çš„ç”µè§†å±å¹•ä¸Šï¼Œè¯†åˆ«å™ªå£°æ¨¡å¼æ¯”é‡å»ºåŸå§‹èŠ‚ç›®å®¹æ˜“å¾—å¤šã€‚</p>
            </div>
        </div>
        
        <h2>3.2 å‰å‘è¿‡ç¨‹ï¼šæ•°å­¦æ¨å¯¼ä¸æ€§è´¨</h2>
        
        <p>å‰å‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œå®ƒå®šä¹‰äº†å¦‚ä½•å°†æ•°æ®é€æ­¥è½¬æ¢ä¸ºå™ªå£°ã€‚è™½ç„¶è¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä¸éœ€è¦å®é™…æ‰§è¡Œå®Œæ•´çš„é©¬å°”å¯å¤«é“¾ï¼Œä½†ç†è§£å…¶æ•°å­¦æ€§è´¨å¯¹æŒæ¡DDPMè‡³å…³é‡è¦ã€‚</p>
        
        <h3>3.2.1 é©¬å°”å¯å¤«é“¾çš„æ„å»º</h3>
        
        <p>å‰å‘è¿‡ç¨‹å®šä¹‰ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼š</p>
        
        <div class="math-block">
            $$\mathbf{x}_0 \to \mathbf{x}_1 \to \mathbf{x}_2 \to \cdots \to \mathbf{x}_T$$
        </div>
        
        <p>å…¶ä¸­æ¯ä¸€æ­¥çš„è½¬ç§»æ¦‚ç‡ä¸ºï¼š</p>
        
        <div class="math-block">
            $$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$$
        </div>
        
        <div class="definition">
            <div class="definition-title">å…³é”®æ€§è´¨1ï¼šæ–¹å·®è°ƒåº¦çš„çº¦æŸ</div>
            <p>ä¸ºä»€ä¹ˆæ˜¯ $\sqrt{1-\beta_t}$ è€Œä¸æ˜¯å…¶ä»–ç³»æ•°ï¼Ÿè¿™æ˜¯ä¸ºäº†ä¿æŒä¿¡å·çš„æœŸæœ›èƒ½é‡ï¼š</p>
            <div class="math-block">
                $$\mathbb{E}[\|\mathbf{x}_t\|^2 | \mathbf{x}_{t-1}] = (1-\beta_t)\|\mathbf{x}_{t-1}\|^2 + \beta_t \cdot d$$
            </div>
            <p>å…¶ä¸­ $d$ æ˜¯æ•°æ®ç»´åº¦ã€‚å½“ $\beta_t$ å¾ˆå°æ—¶ï¼Œä¿¡å·èƒ½é‡è¿‘ä¼¼ä¿æŒä¸å˜ã€‚</p>
        </div>
        
        <p>è®©æˆ‘ä»¬éªŒè¯è¿™ä¸ªæ€§è´¨ï¼š</p>
        
        <div class="code-block">
<pre>import torch
import matplotlib.pyplot as plt

# éªŒè¯èƒ½é‡ä¿æŒæ€§è´¨
x_0 = torch.randn(1000, 3, 32, 32)  # 1000ä¸ª32x32çš„RGBå›¾åƒ
beta = 0.02  # å…¸å‹çš„betaå€¼

# ä¸€æ­¥å‰å‘è¿‡ç¨‹
noise = torch.randn_like(x_0)
x_1 = torch.sqrt(1 - beta) * x_0 + torch.sqrt(beta) * noise

print(f"åŸå§‹ä¿¡å·èƒ½é‡: {x_0.pow(2).mean():.4f}")
print(f"æ‰©æ•£åä¿¡å·èƒ½é‡: {x_1.pow(2).mean():.4f}")
print(f"ç†è®ºé¢„æœŸ: {(1-beta)*x_0.pow(2).mean() + beta*3*32*32:.4f}")</pre>
        </div>
        
        <h3>3.2.2 é‡å‚æ•°åŒ–æŠ€å·§</h3>
        
        <p>DDPMçš„ä¸€ä¸ªå…³é”®æŠ€å·§æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥ä» $\mathbf{x}_0$ é‡‡æ ·ä»»æ„æ—¶åˆ»çš„ $\mathbf{x}_t$ï¼Œè€Œä¸éœ€è¦é€æ­¥æ¨¡æ‹Ÿæ•´ä¸ªé©¬å°”å¯å¤«é“¾ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">å®šç†ï¼šé—­å¼é‡‡æ ·å…¬å¼</div>
            <p>å®šä¹‰ $\alpha_t = 1 - \beta_t$ å’Œ $\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s$ï¼Œåˆ™ï¼š</p>
            <div class="math-block">
                $$q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$$
            </div>
        </div>
        
        <p><strong>è¯æ˜</strong>ï¼ˆè¿™ä¸ªè¯æ˜å¾ˆé‡è¦ï¼Œå€¼å¾—ä»”ç»†ç†è§£ï¼‰ï¼š</p>
        
        <div class="math-block">
            <p>æˆ‘ä»¬ç”¨å½’çº³æ³•è¯æ˜ã€‚</p>
            <p><strong>åŸºç¡€æƒ…å†µ</strong>ï¼ˆ$t=1$ï¼‰ï¼šæ˜¾ç„¶æˆç«‹ï¼Œå› ä¸º $\bar{\alpha}_1 = \alpha_1 = 1 - \beta_1$ã€‚</p>
            
            <p><strong>å½’çº³æ­¥éª¤</strong>ï¼šå‡è®¾å¯¹ $t-1$ æˆç«‹ï¼Œå³ï¼š</p>
            $$\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}$$
            
            <p>å…¶ä¸­ $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ã€‚æ ¹æ®å‰å‘è¿‡ç¨‹å®šä¹‰ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            <p>ä»£å…¥ $\mathbf{x}_{t-1}$ çš„è¡¨è¾¾å¼ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\alpha_t}(\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}) + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            $$= \sqrt{\alpha_t\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            <p>æ³¨æ„åˆ° $\alpha_t\bar{\alpha}_{t-1} = \bar{\alpha}_t$ï¼Œä¸”ä¸¤ä¸ªç‹¬ç«‹é«˜æ–¯å™ªå£°çš„çº¿æ€§ç»„åˆä»æ˜¯é«˜æ–¯å™ªå£°ï¼š</p>
            $$\text{Var}[\sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t] = \alpha_t(1-\bar{\alpha}_{t-1}) + (1-\alpha_t) = 1-\bar{\alpha}_t$$
            
            <p>å› æ­¤ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$$
            
            <p>å…¶ä¸­ $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ã€‚è¯æ¯•ã€‚</p>
        </div>
        
        <h3>3.2.3 å™ªå£°è°ƒåº¦çš„è®¾è®¡</h3>
        
        <p>å™ªå£°è°ƒåº¦ $\{\beta_t\}_{t=1}^T$ çš„é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚DDPMåŸæ–‡ä½¿ç”¨çº¿æ€§è°ƒåº¦ï¼Œä½†åç»­ç ”ç©¶å‘ç°å…¶ä»–è°ƒåº¦å¯èƒ½æ›´ä¼˜ã€‚</p>
        
        <div class="code-block">
<pre>import numpy as np
import matplotlib.pyplot as plt

def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """DDPMåŸå§‹çš„çº¿æ€§è°ƒåº¦"""
    return np.linspace(beta_start, beta_end, timesteps)

def cosine_beta_schedule(timesteps, s=0.008):
    """Improved DDPMçš„ä½™å¼¦è°ƒåº¦"""
    steps = timesteps + 1
    t = np.linspace(0, timesteps, steps)
    alphas_cumprod = np.cos(((t / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return np.clip(betas, 0.0001, 0.9999)

def quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """äºŒæ¬¡è°ƒåº¦ï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰"""
    t = np.linspace(0, 1, timesteps)
    return beta_start + (beta_end - beta_start) * t ** 2

# å¯è§†åŒ–ä¸åŒè°ƒåº¦
timesteps = 1000
linear_betas = linear_beta_schedule(timesteps)
cosine_betas = cosine_beta_schedule(timesteps)
quadratic_betas = quadratic_beta_schedule(timesteps)

# è®¡ç®—ä¿¡å™ªæ¯”ï¼ˆæ›´ç›´è§‚çš„æŒ‡æ ‡ï¼‰
# æ¯”è¾ƒä¸åŒå™ªå£°è°ƒåº¦
linear_betas = linear_beta_schedule(1000)
cosine_betas = cosine_beta_schedule(1000) 
quadratic_betas = quadratic_beta_schedule(1000)

# è®¡ç®—ä¿¡å™ªæ¯”
def compute_snr(betas):
    alphas = 1 - betas
    alphas_cumprod = np.cumprod(alphas)
    return alphas_cumprod / (1 - alphas_cumprod)

# å±•ç¤ºä¸åŒè°ƒåº¦ä¸‹çš„å…³é”®ç»Ÿè®¡æ•°æ®
alphas_cumprod_linear = np.cumprod(1 - linear_betas)
alphas_cumprod_cosine = np.cumprod(1 - cosine_betas)

t_vis = [0, 250, 500, 750, 999]
print("Signal preservation (âˆšá¾±_t) at key timesteps:")
print("Timestep | Linear | Cosine")
for t in t_vis:
    print(f"{t:8d} | {np.sqrt(alphas_cumprod_linear[t]):.4f} | {np.sqrt(alphas_cumprod_cosine[t]):.4f}")

print("\nSNR at key timesteps:")
snr_linear = compute_snr(linear_betas)
snr_cosine = compute_snr(cosine_betas)
for t in t_vis:
    print(f"{t:8d} | {snr_linear[t]:.4f} | {snr_cosine[t]:.4f}")</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">è°ƒåº¦ç­–ç•¥å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">è°ƒåº¦ç±»å‹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç‰¹ç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä¼˜åŠ¿</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">åŠ£åŠ¿</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">çº¿æ€§ (Linear)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">Î²çº¿æ€§å¢é•¿</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€å•ç›´è§‚</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‰æœŸç ´åè¿‡å¿«</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½™å¼¦ (Cosine)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åŸºäºSNRè®¾è®¡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æœ«æœŸå¯èƒ½è¿‡æ…¢</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">äºŒæ¬¡ (Quadratic)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">Î²äºŒæ¬¡å¢é•¿</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‰æœŸä¿ç•™æ›´å¤šä¿¡æ¯</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åæœŸå¯èƒ½å¤ªæ¿€è¿›</td>
                </tr>
            </table>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.2ï¼šå®ç°è‡ªå®šä¹‰å™ªå£°è°ƒåº¦</div>
            <p>è®¾è®¡ä¸€ä¸ª"Så½¢"å™ªå£°è°ƒåº¦ï¼Œä½¿å¾—ï¼š</p>
            <ol>
                <li>å‰æœŸï¼ˆt < 200ï¼‰ï¼šç¼“æ…¢æ·»åŠ å™ªå£°ï¼Œä¿ç•™æ›´å¤šç»“æ„ä¿¡æ¯</li>
                <li>ä¸­æœŸï¼ˆ200 â‰¤ t â‰¤ 800ï¼‰ï¼šå¿«é€Ÿæ·»åŠ å™ªå£°</li>
                <li>åæœŸï¼ˆt > 800ï¼‰ï¼šå†æ¬¡æ”¾ç¼“ï¼Œç¡®ä¿æ”¶æ•›åˆ°çº¯å™ªå£°</li>
            </ol>
            <p>å®ç°è¿™ä¸ªè°ƒåº¦å¹¶ä¸æ ‡å‡†è°ƒåº¦å¯¹æ¯”SNRæ›²çº¿ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_2')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_2" class="answer">
                <pre>def sigmoid_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Så½¢å™ªå£°è°ƒåº¦"""
    t = np.linspace(-6, 6, timesteps)
    sigmoid = 1 / (1 + np.exp(-t))
    betas = beta_start + (beta_end - beta_start) * sigmoid
    return betas

# ä¹Ÿå¯ä»¥åˆ†æ®µè®¾è®¡
def piecewise_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """åˆ†æ®µå™ªå£°è°ƒåº¦"""
    betas = np.zeros(timesteps)
    
    # å‰æœŸï¼šç¼“æ…¢å¢é•¿
    t1 = int(0.2 * timesteps)
    betas[:t1] = np.linspace(beta_start, beta_start * 5, t1)
    
    # ä¸­æœŸï¼šå¿«é€Ÿå¢é•¿
    t2 = int(0.8 * timesteps)
    betas[t1:t2] = np.linspace(beta_start * 5, beta_end * 0.8, t2 - t1)
    
    # åæœŸï¼šç¼“æ…¢å¢é•¿åˆ°beta_end
    betas[t2:] = np.linspace(beta_end * 0.8, beta_end, timesteps - t2)
    
    return betas</pre>
                <p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼šå¥½çš„å™ªå£°è°ƒåº¦åº”è¯¥åœ¨ä¿ç•™è¶³å¤Ÿä¿¡æ¯å’Œå……åˆ†æ¢ç´¢å™ªå£°ç©ºé—´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä½™å¼¦è°ƒåº¦ä¹‹æ‰€ä»¥ä¼˜äºçº¿æ€§è°ƒåº¦ï¼Œæ­£æ˜¯å› ä¸ºå®ƒæ›´å¥½åœ°å¹³è¡¡äº†è¿™ä¸¤ä¸ªéœ€æ±‚ã€‚</p>
            </div>
        </div>
        
        <h2>3.3 åå‘è¿‡ç¨‹ï¼šä»å™ªå£°åˆ°å›¾åƒ</h2>
        
        <p>åå‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒâ€”â€”å¦‚ä½•ä»çº¯å™ªå£°é€æ­¥æ¢å¤å‡ºæ¸…æ™°çš„æ•°æ®ã€‚DDPMçš„å…³é”®è´¡çŒ®ä¹‹ä¸€æ˜¯æ¨å¯¼å‡ºäº†åœ¨å·²çŸ¥ $\mathbf{x}_0$ æ—¶çš„åå‘æ¡ä»¶åˆ†å¸ƒçš„é—­å¼è§£ã€‚</p>
        
        <h3>3.3.1 åå‘æ¡ä»¶æ¦‚ç‡çš„æ¨å¯¼</h3>
        
        <p>è¿™æ˜¯DDPMä¸­æœ€é‡è¦çš„æ•°å­¦æ¨å¯¼ä¹‹ä¸€ã€‚æˆ‘ä»¬æƒ³è¦è®¡ç®— $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">å®šç†ï¼šåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒ</div>
            <p>ç»™å®š $\mathbf{x}_t$ å’Œ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒä¸ºï¼š</p>
            <div class="math-block">
                $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathbf{I})$$
            </div>
            <p>å…¶ä¸­ï¼š</p>
            <div class="math-block">
                $$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t$$
                
                $$\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$
            </div>
        </div>
        
        <p><strong>è¯æ˜</strong>ï¼šä½¿ç”¨è´å¶æ–¯å®šç†ï¼š</p>
        
        <div class="math-block">
            $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}$$
        </div>
        
        <p>ç”±äºå‰å‘è¿‡ç¨‹çš„é©¬å°”å¯å¤«æ€§è´¨ï¼Œ$q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t|\mathbf{x}_{t-1})$ã€‚ç°åœ¨æˆ‘ä»¬çŸ¥é“ï¼š</p>
        
        <ul>
            <li>$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$</li>
            <li>$q(\mathbf{x}_{t-1}|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0, (1-\bar{\alpha}_{t-1})\mathbf{I})$</li>
            <li>$q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$</li>
        </ul>
        
        <p>å°†ä¸‰ä¸ªé«˜æ–¯åˆ†å¸ƒä»£å…¥è´å¶æ–¯å…¬å¼ï¼Œç»è¿‡ç¹çä½†ç›´æ¥çš„ä»£æ•°è¿ç®—ï¼ˆä¸»è¦æ˜¯é…æ–¹ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ä¸Šè¿°ç»“æœã€‚</p>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>ğŸ’¡ å…³é”®æ´å¯Ÿ</h4>
            <p>æ³¨æ„ $\tilde{\boldsymbol{\mu}}_t$ æ˜¯ $\mathbf{x}_0$ å’Œ $\mathbf{x}_t$ çš„<strong>çº¿æ€§ç»„åˆ</strong>ï¼è¿™æ„å‘³ç€ï¼š</p>
            <ul>
                <li>å¦‚æœæˆ‘ä»¬çŸ¥é“ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹å°±æ˜¯ç¡®å®šçš„ï¼ˆé™¤äº†å°çš„é«˜æ–¯å™ªå£°ï¼‰</li>
                <li>å®è·µä¸­æˆ‘ä»¬ä¸çŸ¥é“ $\mathbf{x}_0$ï¼Œæ‰€ä»¥éœ€è¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å®ƒ</li>
                <li>è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯åœ¨å­¦ä¹ "å»å™ª"</li>
            </ul>
        </div>
        
        <h3>3.3.2 å‚æ•°åŒ–é€‰æ‹©ï¼šé¢„æµ‹å™ªå£° vs é¢„æµ‹å‡å€¼</h3>
        
        <p>æ—¢ç„¶ $\tilde{\boldsymbol{\mu}}_t$ ä¾èµ–äºæœªçŸ¥çš„ $\mathbf{x}_0$ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼å®ƒã€‚DDPMæä¾›äº†å‡ ç§å‚æ•°åŒ–æ–¹å¼ï¼š</p>
        
        <div class="code-block">
<pre># æ–¹å¼1ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆæœ€ç›´æ¥ä½†ä¸ç¨³å®šï¼‰
mu_theta = model(x_t, t)

# æ–¹å¼2ï¼šé¢„æµ‹x_0ï¼ˆéœ€è¦clipåˆ°åˆç†èŒƒå›´ï¼‰
x_0_pred = model(x_t, t)
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred + 
            sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé¢„æµ‹å™ªå£°ï¼ˆDDPMçš„é€‰æ‹©ï¼Œæœ€ç¨³å®šï¼‰
epsilon_pred = model(x_t, t)
x_0_pred = (x_t - sqrt_one_minus_alpha_bar_t * epsilon_pred) / sqrt_alpha_bar_t
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred + 
            sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)</pre>
        </div>
        
        <p>ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡é‡å‚æ•°åŒ–æ¥ç†è§£ï¼š</p>
        
        <div class="math-block">
            <p>ç”±äº $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºï¼š</p>
            $$\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}}$$
            
            <p>ä»£å…¥ $\tilde{\boldsymbol{\mu}}_t$ çš„è¡¨è¾¾å¼ï¼Œç»è¿‡åŒ–ç®€å¯å¾—ï¼š</p>
            $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$
        </div>
        
        <p>è¿™ä¸ªè¡¨è¾¾å¼æ­ç¤ºäº†ä¸€ä¸ªä¼˜é›…çš„äº‹å®ï¼š<strong>åå‘è¿‡ç¨‹çš„å‡å€¼åªéœ€è¦çŸ¥é“æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ï¼</strong></p>
        
        <div class="definition">
            <div class="definition-title">ä¸‰ç§å‚æ•°åŒ–çš„å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">å‚æ•°åŒ–</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä¼˜ç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç¼ºç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä½¿ç”¨åœºæ™¯</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\boldsymbol{\mu}_\theta$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç›´æ¥ï¼Œæ— éœ€è½¬æ¢</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä¸åŒtçš„è¾“å‡ºå°ºåº¦å·®å¼‚å¤§</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‡ ä¹ä¸ç”¨</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\mathbf{x}_0$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¯­ä¹‰æ¸…æ™°</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é«˜å™ªå£°æ—¶é¢„æµ‹å›°éš¾</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æŸäº›æ¡ä»¶ç”Ÿæˆä»»åŠ¡</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\boldsymbol{\epsilon}$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¾“å‡ºæ ‡å‡†åŒ–ï¼Œè®­ç»ƒç¨³å®š</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é—´æ¥ï¼Œéœ€è¦è½¬æ¢</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ ‡å‡†é€‰æ‹©</td>
                </tr>
            </table>
        </div>
        
        <h3>3.3.3 æ–¹å·®çš„å¤„ç†ï¼šå›ºå®š vs å¯å­¦ä¹ </h3>
        
        <p>DDPMçš„å¦ä¸€ä¸ªç®€åŒ–æ˜¯ä½¿ç”¨å›ºå®šçš„æ–¹å·® $\tilde{\beta}_t$ã€‚ä½†è¿™æ˜¯æœ€ä¼˜çš„å—ï¼Ÿ</p>
        
        <div class="code-block">
<pre># DDPMï¼šå›ºå®šæ–¹å·®ï¼ˆä¸¤ç§é€‰æ‹©ï¼‰
# é€‰æ‹©1ï¼šä½¿ç”¨åéªŒæ–¹å·®
variance = (1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t

# é€‰æ‹©2ï¼šä½¿ç”¨Î²_tï¼ˆDDPMè®ºæ–‡çš„é€‰æ‹©ï¼‰
variance = beta_t

# æ”¹è¿›çš„DDPMï¼šå­¦ä¹ æ–¹å·®
# ç½‘ç»œåŒæ—¶é¢„æµ‹å™ªå£°å’Œæ–¹å·®
epsilon_pred, v_pred = model(x_t, t).chunk(2, dim=1)

# å‚æ•°åŒ–æ–¹å·®ï¼ˆåœ¨å¯¹æ•°ç©ºé—´æ’å€¼ï¼‰
min_log = torch.log(beta_t)
max_log = torch.log((1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t)
log_variance = v_pred * max_log + (1 - v_pred) * min_log
variance = torch.exp(log_variance)</pre>
        </div>
        
        <div class="visualization" style="background-color: #fff3cd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>âš ï¸ å®è·µç»éªŒ</h4>
            <p>å°½ç®¡å­¦ä¹ æ–¹å·®ç†è®ºä¸Šæ›´ä¼˜ï¼ˆå¯ä»¥è·å¾—æ›´å¥½çš„ä¼¼ç„¶ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼š</p>
            <ul>
                <li>å›ºå®šæ–¹å·®çš„DDPMå·²ç»èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒ</li>
                <li>å­¦ä¹ æ–¹å·®å¢åŠ äº†è®­ç»ƒçš„å¤æ‚åº¦</li>
                <li>å¯¹äºå¤§å¤šæ•°åº”ç”¨ï¼Œå›ºå®šæ–¹å·®æ˜¯è¶³å¤Ÿçš„</li>
                <li>å¦‚æœè¿½æ±‚æœ€ä¼˜ä¼¼ç„¶ï¼ˆå¦‚å‹ç¼©ä»»åŠ¡ï¼‰ï¼Œæ‰è€ƒè™‘å­¦ä¹ æ–¹å·®</li>
            </ul>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.3ï¼šéªŒè¯ä¸åŒå‚æ•°åŒ–çš„ç­‰ä»·æ€§</div>
            <p>å®ç°ä¸‰ç§å‚æ•°åŒ–æ–¹å¼ï¼ŒéªŒè¯å®ƒä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ï¼š</p>
            <ol>
                <li>ç»™å®šç›¸åŒçš„ $\mathbf{x}_t$ã€$\mathbf{x}_0$ å’Œ $t$</li>
                <li>è®¡ç®—çœŸå®çš„å™ªå£° $\boldsymbol{\epsilon}$</li>
                <li>ç”¨ä¸‰ç§æ–¹å¼è®¡ç®— $\tilde{\boldsymbol{\mu}}_t$</li>
                <li>éªŒè¯ç»“æœç›¸åŒï¼ˆåœ¨æ•°å€¼ç²¾åº¦å†…ï¼‰</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_3')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_3" class="answer">
                <pre>import torch

# è®¾ç½®
batch_size = 4
channels = 3
size = 32
t = 500
T = 1000

# åˆå§‹åŒ–
x_0 = torch.randn(batch_size, channels, size, size)
epsilon = torch.randn_like(x_0)

# è®¡ç®—alphaç›¸å…³å€¼
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alphas_bar = torch.cumprod(alphas, dim=0)
alpha_t = alphas[t]
alpha_bar_t = alphas_bar[t]
alpha_bar_prev = alphas_bar[t-1]
beta_t = betas[t]

# å‰å‘è¿‡ç¨‹
x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * epsilon

# æ–¹å¼1ï¼šç›´æ¥è®¡ç®—çœŸå®çš„åéªŒå‡å€¼
mu_true = (torch.sqrt(alpha_bar_prev) * beta_t * x_0 + 
           torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼2ï¼šé€šè¿‡é¢„æµ‹x_0
x_0_pred = x_0  # å‡è®¾å®Œç¾é¢„æµ‹
mu_x0 = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_pred + 
         torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé€šè¿‡é¢„æµ‹å™ªå£°
epsilon_pred = epsilon  # å‡è®¾å®Œç¾é¢„æµ‹
x_0_from_eps = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
mu_eps = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_from_eps + 
          torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æˆ–è€…ç›´æ¥ç”¨ç®€åŒ–å…¬å¼
mu_eps_direct = (x_t - beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_t)

# éªŒè¯
print(f"æ–¹å¼1å’Œæ–¹å¼2çš„å·®å¼‚: {(mu_true - mu_x0).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3çš„å·®å¼‚: {(mu_true - mu_eps).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3(ç›´æ¥)çš„å·®å¼‚: {(mu_true - mu_eps_direct).abs().max():.6f}")

# è¾“å‡ºåº”è¯¥éƒ½æ¥è¿‘0ï¼ˆåœ¨æµ®ç‚¹ç²¾åº¦èŒƒå›´å†…ï¼‰</pre>
                <p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼šä¸‰ç§å‚æ•°åŒ–åœ¨æ•°å­¦ä¸Šç­‰ä»·ï¼Œä½†è®­ç»ƒåŠ¨æ€ä¸åŒã€‚é¢„æµ‹å™ªå£°ä¹‹æ‰€ä»¥æ›´ç¨³å®šï¼Œæ˜¯å› ä¸ºå™ªå£° $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ å§‹ç»ˆæ˜¯æ ‡å‡†åŒ–çš„ï¼Œè€Œ $\mathbf{x}_0$ çš„åˆ†å¸ƒå¯èƒ½å¾ˆå¤æ‚ã€‚</p>
            </div>
        </div>
        
        <h2>3.4 è®­ç»ƒç›®æ ‡ï¼šå˜åˆ†ä¸‹ç•Œçš„ç®€åŒ–</h2>
        
        <p>DDPMçš„å¦ä¸€ä¸ªé‡è¦è´¡çŒ®æ˜¯å°†å¤æ‚çš„å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰ç®€åŒ–ä¸ºä¸€ä¸ªç®€å•çš„å»å™ªç›®æ ‡ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†æ¨å¯¼è¿™ä¸ªè¿‡ç¨‹ã€‚</p>
        
        <h3>3.4.1 å®Œæ•´çš„å˜åˆ†ä¸‹ç•Œ</h3>
        
        <p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ $\log p_\theta(\mathbf{x}_0)$ã€‚ç”±äºç›´æ¥è®¡ç®—å›°éš¾ï¼Œæˆ‘ä»¬ä¼˜åŒ–å…¶å˜åˆ†ä¸‹ç•Œï¼š</p>
        
        <div class="math-block">
            $$\log p_\theta(\mathbf{x}_0) \geq \mathbb{E}_q\left[\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] = -L_{\text{VLB}}$$
        </div>
        
        <p>å…¶ä¸­ $L_{\text{VLB}}$ æ˜¯å˜åˆ†ä¸‹ç•ŒæŸå¤±ã€‚ç»è¿‡å±•å¼€ï¼ˆä½¿ç”¨é©¬å°”å¯å¤«æ€§è´¨ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ï¼š</p>
        
        <div class="math-block">
            $$L_{\text{VLB}} = L_T + \sum_{t=2}^{T} L_{t-1} + L_0$$
        </div>
        
        <p>å…¶ä¸­å„é¡¹å®šä¹‰ä¸ºï¼š</p>
        
        <div class="definition">
            <div class="definition-title">å˜åˆ†ä¸‹ç•Œçš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†</div>
            <div class="math-block">
                $$L_T = D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0) \| p(\mathbf{x}_T))$$
                $$L_{t-1} = \mathbb{E}_q\left[D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))\right]$$
                $$L_0 = \mathbb{E}_q\left[-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\right]$$
            </div>
            <ul>
                <li>$L_T$ï¼šå…ˆéªŒåŒ¹é…é¡¹ï¼Œé€šå¸¸å¾ˆå°å¯ä»¥å¿½ç•¥ï¼ˆå› ä¸º $q(\mathbf{x}_T|\mathbf{x}_0) \approx \mathcal{N}(0, \mathbf{I})$ï¼‰</li>
                <li>$L_{t-1}$ï¼šå»å™ªåŒ¹é…é¡¹ï¼Œè¿™æ˜¯ä¸»è¦çš„ä¼˜åŒ–ç›®æ ‡</li>
                <li>$L_0$ï¼šé‡å»ºé¡¹ï¼Œå†³å®šæœ€ç»ˆè¾“å‡ºè´¨é‡</li>
            </ul>
        </div>
        
        <p>å…³é”®åœ¨äºå¦‚ä½•å¤„ç† $L_{t-1}$ é¡¹ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“ $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ çš„é—­å¼è§£ï¼ˆè§3.3.1èŠ‚ï¼‰ï¼Œä¸”å‡è®¾ $p_\theta$ ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒKLæ•£åº¦å¯ä»¥ç®€åŒ–ä¸ºï¼š</p>
        
        <div class="math-block">
            $$L_{t-1} = \mathbb{E}_q\left[\frac{1}{2\sigma_t^2}\|\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\|^2\right] + C$$
        </div>
        
        <p>å…¶ä¸­ $C$ æ˜¯ä¸ $\theta$ æ— å…³çš„å¸¸æ•°ã€‚</p>
        
        <h3>3.4.2 ç®€åŒ–çš„å»å™ªç›®æ ‡</h3>
        
        <p>DDPMçš„å…³é”®æ´å¯Ÿæ˜¯ï¼šé€šè¿‡é€‰æ‹©å™ªå£°é¢„æµ‹å‚æ•°åŒ–ï¼Œå¯ä»¥å°†ä¸Šè¿°ç›®æ ‡è¿›ä¸€æ­¥ç®€åŒ–ã€‚å›å¿†3.3.2èŠ‚çš„ç»“æœï¼š</p>
        
        <div class="math-block">
            $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$
        </div>
        
        <p>å¦‚æœæˆ‘ä»¬å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ ä¸ºï¼š</p>
        
        <div class="math-block">
            $$\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right)$$
        </div>
        
        <p>é‚£ä¹ˆ $L_{t-1}$ å¯ä»¥ç®€åŒ–ä¸ºï¼š</p>
        
        <div class="math-block">
            $$L_{t-1} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}}\left[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$
        </div>
        
        <p>å…¶ä¸­ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ã€‚</p>
        
        <div class="visualization" style="background-color: #e8f4fd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>ğŸ¯ DDPMçš„ç®€åŒ–è®­ç»ƒç›®æ ‡</h4>
            <p>Hoç­‰äººå‘ç°ï¼Œå¿½ç•¥æƒé‡ç³»æ•°å¹¶å¯¹æ‰€æœ‰æ—¶é—´æ­¥æ±‚å’Œï¼Œå¾—åˆ°çš„ç®€åŒ–ç›®æ ‡æ•ˆæœæ›´å¥½ï¼š</p>
            <div class="math-block">
                $$L_{\text{simple}} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$
            </div>
            <p>è¿™å°±æ˜¯è‘—åçš„"ç®€å•æŸå¤±"â€”â€”åªéœ€è¦é¢„æµ‹å™ªå£°ï¼</p>
        </div>
        
        <h3>3.4.3 æŸå¤±å‡½æ•°çš„åŠ æƒç­–ç•¥</h3>
        
        <p>è™½ç„¶ç®€å•æŸå¤±æ•ˆæœå¾ˆå¥½ï¼Œä½†ä¸åŒæ—¶é—´æ­¥çš„é‡è¦æ€§ç¡®å®ä¸åŒã€‚åç»­ç ”ç©¶æå‡ºäº†å„ç§åŠ æƒç­–ç•¥ï¼š</p>
        
        <div class="code-block">
<pre>import torch
import matplotlib.pyplot as plt

# ä¸åŒçš„æŸå¤±åŠ æƒç­–ç•¥
def get_loss_weight(t, strategy='simple', snr_gamma=5.0):
    """
    è®¡ç®—æ—¶é—´æ­¥tçš„æŸå¤±æƒé‡
    
    ç­–ç•¥:
    - simple: æ‰€æœ‰æ—¶é—´æ­¥æƒé‡ç›¸åŒï¼ˆDDPMåŸå§‹ï¼‰
    - snr: åŸºäºä¿¡å™ªæ¯”çš„åŠ æƒ
    - truncated_snr: æˆªæ–­çš„SNRåŠ æƒï¼ˆé˜²æ­¢æç«¯å€¼ï¼‰
    - importance: åŸºäºé‡è¦æ€§é‡‡æ ·
    """
    if strategy == 'simple':
        return 1.0
    
    elif strategy == 'snr':
        # æƒé‡ä¸ä¿¡å™ªæ¯”æˆåæ¯”
        snr = alpha_bar[t] / (1 - alpha_bar[t])
        return 1.0 / (1.0 + snr)
    
    elif strategy == 'truncated_snr':
        # Min-SNR-Î³ åŠ æƒï¼ˆHang et al., 2023ï¼‰
        snr = alpha_bar[t] / (1 - alpha_bar[t])
        return torch.minimum(snr, torch.tensor(snr_gamma)) / snr
    
    elif strategy == 'importance':
        # åŸºäºL_tç³»æ•°çš„é‡è¦æ€§åŠ æƒ
        return beta[t]**2 / (2 * sigma[t]**2 * alpha[t] * (1 - alpha_bar[t]))

# åˆ†æä¸åŒåŠ æƒç­–ç•¥
T = 1000
t = torch.arange(T)
beta = torch.linspace(0.0001, 0.02, T)
alpha = 1 - beta
alpha_bar = torch.cumprod(alpha, dim=0)
sigma = beta  # DDPMçš„é€‰æ‹©

# è®¡ç®—ä¸åŒç­–ç•¥çš„æƒé‡
strategies = ['simple', 'snr', 'truncated_snr', 'importance']
weight_stats = {}

for strategy in strategies:
    weights = torch.tensor([get_loss_weight(i, strategy) for i in range(T)])
    weight_stats[strategy] = {
        'min': weights.min().item(),
        'max': weights.max().item(),
        'mean': weights.mean().item(),
        'std': weights.std().item()
    }

# æ‰“å°æƒé‡ç»Ÿè®¡
print("Loss weight statistics for different strategies:")
for strategy, stats in weight_stats.items():
    print(f"\n{strategy}:")
    print(f"  Min: {stats['min']:.6f}")
    print(f"  Max: {stats['max']:.6f}")
    print(f"  Mean: {stats['mean']:.6f}")
    print(f"  Std: {stats['std']:.6f}")</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">åŠ æƒç­–ç•¥å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç­–ç•¥</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">åŠ¨æœº</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ•ˆæœ</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">è®¡ç®—å¼€é”€</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€å• (Simple)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€åŒ–è®­ç»ƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åŸºå‡†ï¼Œæ•ˆæœå·²ç»ä¸é”™</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æœ€ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">SNRåŠ æƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å¹³è¡¡ä¸åŒå™ªå£°æ°´å¹³</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ”¹å–„é«˜å™ªå£°åŒºåŸŸ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">Min-SNR-Î³</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¿å…æç«¯æƒé‡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç›®å‰æœ€ä¼˜</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é‡è¦æ€§é‡‡æ ·</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç†è®ºæœ€ä¼˜</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å®è·µä¸­ä¸ç¨³å®š</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä¸­ç­‰</td>
                </tr>
            </table>
        </div>
        
        <h3>3.4.4 è®­ç»ƒç®—æ³•æ€»ç»“</h3>
        
        <p>ç»¼åˆä»¥ä¸Šæ¨å¯¼ï¼ŒDDPMçš„è®­ç»ƒç®—æ³•æå…¶ç®€æ´ï¼š</p>
        
        <div class="code-block">
<pre>def train_ddpm(model, dataloader, num_epochs, T=1000):
    """DDPMè®­ç»ƒå¾ªç¯"""
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
    
    # é¢„è®¡ç®—å™ªå£°è°ƒåº¦ç›¸å…³å€¼
    betas = linear_beta_schedule(T)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas_bar = torch.sqrt(alphas_bar)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    for epoch in range(num_epochs):
        for batch_idx, (x_0, _) in enumerate(dataloader):
            batch_size = x_0.shape[0]
            
            # éšæœºé‡‡æ ·æ—¶é—´æ­¥
            t = torch.randint(0, T, (batch_size,), device=x_0.device)
            
            # é‡‡æ ·å™ªå£°
            epsilon = torch.randn_like(x_0)
            
            # å‰å‘æ‰©æ•£ï¼šè®¡ç®—x_t
            x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 + 
                   sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)
            
            # é¢„æµ‹å™ªå£°
            epsilon_pred = model(x_t, t)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(epsilon_pred, epsilon)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.4ï¼šå®ç°åŠ æƒæŸå¤±</div>
            <p>ä¿®æ”¹ä¸Šè¿°è®­ç»ƒä»£ç ï¼Œå®ç°Min-SNR-Î³åŠ æƒç­–ç•¥ï¼š</p>
            <ol>
                <li>è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„SNR</li>
                <li>åº”ç”¨Min-SNR-Î³åŠ æƒï¼ˆå»ºè®®Î³=5ï¼‰</li>
                <li>æ¯”è¾ƒåŠ æƒå‰åçš„è®­ç»ƒæ›²çº¿</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_4')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_4" class="answer">
                <pre>def train_ddpm_weighted(model, dataloader, num_epochs, T=1000, snr_gamma=5.0):
    """å¸¦Min-SNRåŠ æƒçš„DDPMè®­ç»ƒ"""
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
    
    # é¢„è®¡ç®—
    betas = linear_beta_schedule(T)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas_bar = torch.sqrt(alphas_bar)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # é¢„è®¡ç®—SNRå’Œæƒé‡
    snr = alphas_bar / (1 - alphas_bar)
    snr_clipped = torch.minimum(snr, torch.tensor(snr_gamma))
    loss_weights = snr_clipped / snr
    
    for epoch in range(num_epochs):
        for batch_idx, (x_0, _) in enumerate(dataloader):
            batch_size = x_0.shape[0]
            
            # é‡‡æ ·æ—¶é—´æ­¥
            t = torch.randint(0, T, (batch_size,), device=x_0.device)
            
            # å‰å‘æ‰©æ•£
            epsilon = torch.randn_like(x_0)
            x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 + 
                   sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)
            
            # é¢„æµ‹å™ªå£°
            epsilon_pred = model(x_t, t)
            
            # è®¡ç®—åŠ æƒæŸå¤±
            mse_loss = (epsilon_pred - epsilon).pow(2).mean(dim=[1,2,3])
            weights = loss_weights[t]
            loss = (weights * mse_loss).mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

# å…³é”®æ”¹è¿›ï¼š
# 1. é«˜SNRï¼ˆä½å™ªå£°ï¼‰åŒºåŸŸçš„æƒé‡è¢«é™ä½ï¼Œé¿å…è¿‡æ‹Ÿåˆç»†èŠ‚
# 2. ä½SNRï¼ˆé«˜å™ªå£°ï¼‰åŒºåŸŸä¿æŒè¾ƒé«˜æƒé‡ï¼Œç¡®ä¿ç»“æ„å­¦ä¹ 
# 3. Î³å‚æ•°æ§åˆ¶æˆªæ–­ç¨‹åº¦ï¼Œé€šå¸¸5-10æ•ˆæœè¾ƒå¥½</pre>
                <p><strong>å®è·µå»ºè®®</strong>ï¼šMin-SNR-Î³åŠ æƒåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚ä½†å¯¹äºä½åˆ†è¾¨ç‡æˆ–ç®€å•æ•°æ®é›†ï¼Œç®€å•æŸå¤±å¯èƒ½å·²ç»è¶³å¤Ÿã€‚</p>
            </div>
        </div>
        
        <h2>3.5 é‡‡æ ·ç®—æ³•ï¼šä»ç†è®ºåˆ°å®è·µ</h2>
        
        <p>è®­ç»ƒå¥½DDPMåï¼Œå¦‚ä½•ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Ÿè¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»DDPMçš„é‡‡æ ·ç®—æ³•ï¼Œä»æ ‡å‡†çš„1000æ­¥é‡‡æ ·åˆ°å„ç§å®ç”¨æŠ€å·§ã€‚</p>
        
        <h3>3.5.1 æ ‡å‡†DDPMé‡‡æ ·</h3>
        
        <p>DDPMçš„é‡‡æ ·è¿‡ç¨‹æ˜¯ä»çº¯å™ªå£° $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ å¼€å§‹ï¼Œé€æ­¥å»å™ªç›´åˆ°å¾—åˆ°æ¸…æ™°çš„å›¾åƒ $\mathbf{x}_0$ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">DDPMé‡‡æ ·ç®—æ³•</div>
            <p>å¯¹äºæ¯ä¸€æ­¥ $t = T, T-1, ..., 1$ï¼š</p>
            <div class="math-block">
                $$\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right) + \sigma_t \mathbf{z}$$
            </div>
            <p>å…¶ä¸­ï¼š</p>
            <ul>
                <li>$\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ æ˜¯è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹ç½‘ç»œ</li>
                <li>$\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$ æ˜¯é‡‡æ ·å™ªå£°ï¼ˆå½“ $t > 1$ æ—¶ï¼‰</li>
                <li>$\sigma_t$ æ˜¯æ–¹å·®ï¼ŒDDPMä½¿ç”¨ $\sigma_t = \beta_t$</li>
            </ul>
        </div>
        
        <p>å®Œæ•´çš„å®ç°ä»£ç ï¼š</p>
        
        <div class="code-block">
<pre>@torch.no_grad()
def ddpm_sample(model, shape, num_timesteps=1000, device='cuda'):
    """
    DDPMæ ‡å‡†é‡‡æ ·ç®—æ³•
    
    Args:
        model: è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹æ¨¡å‹
        shape: ç”Ÿæˆå›¾åƒçš„å½¢çŠ¶ï¼Œå¦‚ (batch_size, 3, 32, 32)
        num_timesteps: æ€»æ—¶é—´æ­¥æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        ç”Ÿæˆçš„å›¾åƒ x_0
    """
    # é¢„è®¡ç®—å™ªå£°è°ƒåº¦
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas = torch.sqrt(alphas)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # ä»çº¯å™ªå£°å¼€å§‹
    x_t = torch.randn(shape, device=device)
    
    # é€æ­¥å»å™ª
    for t in reversed(range(num_timesteps)):
        # åˆ›å»ºæ—¶é—´æ­¥å¼ é‡
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t_tensor)
        
        # è®¡ç®—å‡å€¼
        mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]
        
        # æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
        if t > 0:
            noise = torch.randn_like(x_t)
            std = torch.sqrt(betas[t])  # DDPMä½¿ç”¨Î²_tä½œä¸ºæ–¹å·®
            x_t = mean + std * noise
        else:
            x_t = mean
    
    return x_t</pre>
        </div>
        
        <h4>é‡‡æ ·è¿‡ç¨‹çš„å¯è§†åŒ–</h4>
        
        <p>ä¸ºäº†æ›´å¥½åœ°ç†è§£é‡‡æ ·è¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–ä¸åŒæ—¶é—´æ­¥çš„ä¸­é—´ç»“æœï¼š</p>
        
        <div class="code-block">
<pre>def get_sampling_trajectory(model, num_steps_to_show=10):
    """è·å–DDPMé‡‡æ ·è¿‡ç¨‹çš„ä¸­é—´ç»“æœ"""
    # é‡‡æ ·å¹¶ä¿å­˜ä¸­é—´ç»“æœ
    shape = (1, 3, 32, 32)
    T = 1000
    
    # é€‰æ‹©è¦å±•ç¤ºçš„æ—¶é—´æ­¥
    steps_to_show = torch.linspace(T-1, 0, num_steps_to_show, dtype=torch.long)
    intermediate_results = []
    
    # åˆå§‹åŒ–
    x_t = torch.randn(shape, device='cuda')
    betas = linear_beta_schedule(T).to('cuda')
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    # é‡‡æ ·è¿‡ç¨‹
    for t in reversed(range(T)):
        t_tensor = torch.full((1,), t, device='cuda', dtype=torch.long)
        
        # é¢„æµ‹å¹¶æ›´æ–°
        epsilon_pred = model(x_t, t_tensor)
        # ... (é‡‡æ ·æ­¥éª¤åŒä¸Š)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if t in steps_to_show:
            # å°†x_tæ˜ å°„åˆ°[0, 1]èŒƒå›´ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
            img = (x_t.clamp(-1, 1) + 1) / 2
            intermediate_results.append({
                'timestep': t,
                'image': img.cpu()
            })
    
    return intermediate_results

# ä½¿ç”¨ç¤ºä¾‹
trajectory = get_sampling_trajectory(model, num_steps_to_show=10)
print(f"Saved {len(trajectory)} intermediate results")
for i, result in enumerate(trajectory):
    print(f"Step {i}: t={result['timestep']}, shape={result['image'].shape}")</pre>
        </div>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>é‡‡æ ·è¿‡ç¨‹çš„ç‰¹ç‚¹</h4>
            <ul>
                <li><strong>å‰æœŸï¼ˆt â‰ˆ 1000ï¼‰</strong>ï¼šä¸»è¦æ¢å¤å…¨å±€ç»“æ„å’Œå¤§è‡´å½¢çŠ¶</li>
                <li><strong>ä¸­æœŸï¼ˆt â‰ˆ 500ï¼‰</strong>ï¼šç»†åŒ–å¯¹è±¡è½®å»“å’Œä¸»è¦ç‰¹å¾</li>
                <li><strong>åæœŸï¼ˆt â‰ˆ 0ï¼‰</strong>ï¼šæ·»åŠ çº¹ç†ç»†èŠ‚å’Œé«˜é¢‘ä¿¡æ¯</li>
            </ul>
            <p>è¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºè‰ºæœ¯å®¶ä½œç”»ï¼šå…ˆå‹¾å‹’è½®å»“ï¼Œå†å¡«å……é¢œè‰²ï¼Œæœ€åæ·»åŠ ç»†èŠ‚ã€‚</p>
        </div>
        
        <h4>è®¡ç®—æ•ˆç‡åˆ†æ</h4>
        
        <p>æ ‡å‡†DDPMé‡‡æ ·çš„ä¸»è¦é—®é¢˜æ˜¯é€Ÿåº¦æ…¢ã€‚è®©æˆ‘ä»¬åˆ†æä¸€ä¸‹è®¡ç®—æˆæœ¬ï¼š</p>
        
        <div class="code-block">
<pre>def analyze_sampling_cost(model, batch_size=16, image_size=256):
    """åˆ†æDDPMé‡‡æ ·çš„è®¡ç®—æˆæœ¬"""
    import time
    
    shape = (batch_size, 3, image_size, image_size)
    T = 1000
    
    # æµ‹é‡å•æ¬¡å‰å‘ä¼ æ’­æ—¶é—´
    x = torch.randn(shape, device='cuda')
    t = torch.randint(0, T, (batch_size,), device='cuda')
    
    # é¢„çƒ­GPU
    for _ in range(10):
        _ = model(x, t)
    torch.cuda.synchronize()
    
    # è®¡æ—¶
    start = time.time()
    num_runs = 50
    for _ in range(num_runs):
        _ = model(x, t)
    torch.cuda.synchronize()
    end = time.time()
    
    time_per_forward = (end - start) / num_runs
    total_time = time_per_forward * T
    
    print(f"å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å•æ¬¡å‰å‘ä¼ æ’­: {time_per_forward*1000:.2f} ms")
    print(f"å®Œæ•´é‡‡æ · (1000æ­¥): {total_time:.2f} ç§’")
    print(f"æ¯ç§’ç”Ÿæˆå›¾åƒæ•°: {batch_size/total_time:.3f}")
    
    # å†…å­˜ä½¿ç”¨ä¼°è®¡
    model_params = sum(p.numel() for p in model.parameters()) * 4 / 1024**3  # GB
    activation_memory = batch_size * 3 * image_size**2 * 4 * 50 / 1024**3  # ç²—ç•¥ä¼°è®¡
    print(f"\nå†…å­˜ä½¿ç”¨:")
    print(f"æ¨¡å‹å‚æ•°: {model_params:.2f} GB")
    print(f"æ¿€æ´»å€¼ (ä¼°è®¡): {activation_memory:.2f} GB")</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">å…¸å‹æ€§èƒ½æ•°æ®</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">é…ç½®</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">å•æ­¥æ—¶é—´</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ€»é‡‡æ ·æ—¶é—´</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ååé‡</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">32Ã—32, batch=64</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~5ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">5ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">12.8 å›¾åƒ/ç§’</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">256Ã—256, batch=8</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~50ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">50ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">0.16 å›¾åƒ/ç§’</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">512Ã—512, batch=4</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~200ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">200ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">0.02 å›¾åƒ/ç§’</td>
                </tr>
            </table>
            <p><small>*åŸºäºRTX 3090ï¼Œå®é™…æ€§èƒ½å› æ¨¡å‹æ¶æ„è€Œå¼‚</small></p>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.1ï¼šå®ç°é‡‡æ ·è¿›åº¦æ¡</div>
            <p>ä¿®æ”¹DDPMé‡‡æ ·å‡½æ•°ï¼Œæ·»åŠ ï¼š</p>
            <ol>
                <li>tqdmè¿›åº¦æ¡æ˜¾ç¤ºé‡‡æ ·è¿›åº¦</li>
                <li>å¯é€‰çš„ä¸­é—´ç»“æœä¿å­˜</li>
                <li>EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰æ¨¡å‹æ”¯æŒ</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_1" class="answer">
                <pre>from tqdm import tqdm

@torch.no_grad()
def ddpm_sample_with_progress(
    model, 
    shape, 
    num_timesteps=1000,
    device='cuda',
    use_ema=True,
    ema_model=None,
    save_intermediate=False,
    save_steps=None
):
    """å¢å¼ºç‰ˆDDPMé‡‡æ ·"""
    # é€‰æ‹©æ¨¡å‹
    if use_ema and ema_model is not None:
        sample_model = ema_model
    else:
        sample_model = model
    
    sample_model.eval()
    
    # é¢„è®¡ç®—
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas = torch.sqrt(alphas)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # åˆå§‹åŒ–
    x_t = torch.randn(shape, device=device)
    intermediates = []
    
    # é‡‡æ ·å¾ªç¯
    for t in tqdm(reversed(range(num_timesteps)), desc='Sampling', total=num_timesteps):
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = sample_model(x_t, t_tensor)
        
        # æ›´æ–°x_t
        mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]
        
        if t > 0:
            noise = torch.randn_like(x_t)
            std = torch.sqrt(betas[t])
            x_t = mean + std * noise
        else:
            x_t = mean
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if save_intermediate and save_steps is not None and t in save_steps:
            intermediates.append({
                't': t,
                'x_t': x_t.cpu().clone(),
                'pred_x_0': self._predict_x0_from_eps(x_t, t, epsilon_pred)
            })
    
    if save_intermediate:
        return x_t, intermediates
    else:
        return x_t

def _predict_x0_from_eps(x_t, t, epsilon_pred):
    """ä»å™ªå£°é¢„æµ‹æ¢å¤x_0ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
    return (x_t - sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas_bar[t]</pre>
                <p><strong>ä½¿ç”¨æŠ€å·§</strong>ï¼š</p>
                <ul>
                    <li>EMAæ¨¡å‹é€šå¸¸ç”Ÿæˆè´¨é‡æ›´å¥½ï¼Œè®­ç»ƒæ—¶åº”åŒæ—¶ç»´æŠ¤</li>
                    <li>ä¿å­˜ä¸­é—´ç»“æœæœ‰åŠ©äºè°ƒè¯•å’Œç†è§£æ¨¡å‹è¡Œä¸º</li>
                    <li>å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œè€ƒè™‘ä½¿ç”¨DataLoaderé£æ ¼çš„ç”Ÿæˆå™¨ä»¥èŠ‚çœå†…å­˜</li>
                </ul>
            </div>
        </div>
        
        <h3>3.5.2 é‡‡æ ·çš„éšæœºæ€§æ§åˆ¶</h3>
        
        <p>DDPMé‡‡æ ·è¿‡ç¨‹ä¸­çš„éšæœºæ€§æ¥æºäºä¸¤ä¸ªåœ°æ–¹ï¼šåˆå§‹å™ªå£° $\mathbf{x}_T$ å’Œæ¯æ­¥æ·»åŠ çš„å™ªå£° $\mathbf{z}_t$ã€‚é€šè¿‡æ§åˆ¶è¿™äº›éšæœºæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å½±å“ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</p>
        
        <h4>æ¸©åº¦å‚æ•°çš„å¼•å…¥</h4>
        
        <p>ç±»ä¼¼äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥æ¸©åº¦å‚æ•°æ¥æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§ï¼š</p>
        
        <div class="code-block">
<pre>def ddpm_sample_with_temperature(
    model, 
    shape, 
    temperature=1.0,
    noise_temperature=1.0,
    num_timesteps=1000,
    device='cuda'
):
    """
    å¸¦æ¸©åº¦æ§åˆ¶çš„DDPMé‡‡æ ·
    
    Args:
        temperature: æ§åˆ¶åˆå§‹å™ªå£°çš„æ¸©åº¦
        noise_temperature: æ§åˆ¶æ¯æ­¥å™ªå£°çš„æ¸©åº¦
    """
    # é¢„è®¡ç®—ï¼ˆåŒå‰ï¼‰
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    # æ¸©åº¦è°ƒæ•´çš„åˆå§‹å™ªå£°
    x_t = torch.randn(shape, device=device) * temperature
    
    for t in reversed(range(num_timesteps)):
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t_tensor)
        
        # è®¡ç®—å‡å€¼
        mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])
        
        if t > 0:
            # æ¸©åº¦è°ƒæ•´çš„æ­¥è¿›å™ªå£°
            noise = torch.randn_like(x_t) * noise_temperature
            std = torch.sqrt(betas[t])
            x_t = mean + std * noise
        else:
            x_t = mean
    
    return x_t</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">æ¸©åº¦å‚æ•°çš„æ•ˆæœ</div>
            <ul>
                <li><strong>temperature < 1.0</strong>ï¼šå‡å°‘åˆå§‹éšæœºæ€§ï¼Œç”Ÿæˆæ›´"å…¸å‹"çš„æ ·æœ¬</li>
                <li><strong>temperature > 1.0</strong>ï¼šå¢åŠ åˆå§‹éšæœºæ€§ï¼Œç”Ÿæˆæ›´å¤šæ ·ä½†å¯èƒ½è´¨é‡è¾ƒä½çš„æ ·æœ¬</li>
                <li><strong>noise_temperature < 1.0</strong>ï¼šå‡å°‘å»å™ªè¿‡ç¨‹çš„éšæœºæ€§ï¼Œç»“æœæ›´ç¡®å®šä½†å¯èƒ½è¿‡äºå¹³æ»‘</li>
                <li><strong>noise_temperature > 1.0</strong>ï¼šå¢åŠ å»å™ªéšæœºæ€§ï¼Œå¯èƒ½äº§ç”Ÿæ›´å¤šç»†èŠ‚ä½†ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±</li>
            </ul>
        </div>
        
        <h4>ç¡®å®šæ€§é‡‡æ ·ï¼šDDIMé¢„è§ˆ</h4>
        
        <p>ä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿæ˜¯ï¼šå¦‚æœæˆ‘ä»¬å®Œå…¨å»é™¤æ­¥è¿›å™ªå£°ï¼ˆè®¾ç½® $\sigma_t = 0$ï¼‰ï¼Œé‡‡æ ·è¿‡ç¨‹å˜æˆç¡®å®šæ€§çš„ã€‚è¿™å°±æ˜¯DDIMçš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
        
        <div class="code-block">
<pre>def ddpm_deterministic_sample(model, shape, num_timesteps=1000, eta=0.0):
    """
    ç¡®å®šæ€§æˆ–éƒ¨åˆ†ç¡®å®šæ€§é‡‡æ ·
    eta=0: å®Œå…¨ç¡®å®šæ€§ï¼ˆDDIMï¼‰
    eta=1: æ ‡å‡†DDPMï¼ˆå®Œå…¨éšæœºï¼‰
    """
    x_t = torch.randn(shape, device='cuda')
    
    for t in reversed(range(num_timesteps)):
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t)
        
        # é¢„æµ‹x_0
        x_0_pred = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])
        
        if t > 0:
            # è®¡ç®—æ–¹å‘æŒ‡å‘x_{t-1}
            direction = torch.sqrt(1 - alphas_bar[t-1]) * epsilon_pred
            
            # ç¡®å®šæ€§éƒ¨åˆ†
            x_t = torch.sqrt(alphas_bar[t-1]) * x_0_pred + direction
            
            # éšæœºéƒ¨åˆ†ï¼ˆç”±etaæ§åˆ¶ï¼‰
            if eta > 0:
                noise = torch.randn_like(x_t)
                variance = eta * betas[t] * (1 - alphas_bar[t-1]) / (1 - alphas_bar[t])
                x_t = x_t + torch.sqrt(variance) * noise
        else:
            x_t = x_0_pred
    
    return x_t</pre>
        </div>
        
        <h4>é‡‡æ ·ç§å­ä¸å¯é‡å¤æ€§</h4>
        
        <p>å¯¹äºéœ€è¦å¯é‡å¤ç»“æœçš„åº”ç”¨ï¼Œæ§åˆ¶éšæœºç§å­è‡³å…³é‡è¦ï¼š</p>
        
        <div class="code-block">
<pre>class SeededSampler:
    """å¯é‡å¤çš„é‡‡æ ·å™¨"""
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def sample_with_seed(self, seed, shape, **kwargs):
        """ä½¿ç”¨æŒ‡å®šç§å­é‡‡æ ·"""
        # ä¿å­˜å½“å‰éšæœºçŠ¶æ€
        cpu_state = torch.get_rng_state()
        cuda_state = torch.cuda.get_rng_state(self.device)
        
        # è®¾ç½®ç§å­
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        # é‡‡æ ·
        result = ddpm_sample(self.model, shape, device=self.device, **kwargs)
        
        # æ¢å¤éšæœºçŠ¶æ€
        torch.set_rng_state(cpu_state)
        torch.cuda.set_rng_state(cuda_state, self.device)
        
        return result
    
    def sample_variations(self, base_seed, num_variations, shape, temperature_range=(0.8, 1.2)):
        """ç”ŸæˆåŒä¸€ç§å­çš„å¤šä¸ªå˜ä½“"""
        variations = []
        
        for i in range(num_variations):
            # ä½¿ç”¨ç›¸åŒçš„åŸºç¡€ç§å­ä½†ä¸åŒçš„æ¸©åº¦
            temp = np.linspace(temperature_range[0], temperature_range[1], num_variations)[i]
            
            torch.manual_seed(base_seed)
            torch.cuda.manual_seed(base_seed)
            
            sample = ddpm_sample_with_temperature(
                self.model, shape, 
                temperature=temp,
                device=self.device
            )
            variations.append(sample)
            
        return torch.stack(variations)</pre>
        </div>
        
        <h4>é«˜çº§æŠ€å·§ï¼šå¼•å¯¼é‡‡æ ·ï¼ˆGuided Samplingï¼‰</h4>
        
        <p>æˆ‘ä»¬å¯ä»¥åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥é¢å¤–çš„å¼•å¯¼ä¿¡å·ï¼Œè¿™æ˜¯æ¡ä»¶ç”Ÿæˆçš„åŸºç¡€ï¼š</p>
        
        <div class="code-block">
<pre>def guided_sample(model, shape, guidance_fn=None, guidance_scale=1.0):
    """
    å¸¦å¼•å¯¼çš„é‡‡æ ·
    guidance_fn: è®¡ç®—å¼•å¯¼æ¢¯åº¦çš„å‡½æ•°
    guidance_scale: å¼•å¯¼å¼ºåº¦
    """
    x_t = torch.randn(shape, device='cuda')
    x_t.requires_grad = True
    
    for t in reversed(range(num_timesteps)):
        # æ ‡å‡†DDPMæ›´æ–°
        with torch.no_grad():
            epsilon_pred = model(x_t, t)
            mean = compute_mean(x_t, epsilon_pred, t)
            std = torch.sqrt(betas[t])
        
        # è®¡ç®—å¼•å¯¼æ¢¯åº¦
        if guidance_fn is not None and t > 0:
            # è®¡ç®—å¼•å¯¼æŸå¤±
            guidance_loss = guidance_fn(x_t, t)
            
            # è®¡ç®—æ¢¯åº¦
            grad = torch.autograd.grad(guidance_loss, x_t)[0]
            
            # åº”ç”¨å¼•å¯¼ï¼ˆæ³¨æ„ç¬¦å·ï¼šæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼‰
            mean = mean - guidance_scale * std**2 * grad
        
        # æ›´æ–°x_t
        if t > 0:
            noise = torch.randn_like(x_t)
            x_t = mean + std * noise
        else:
            x_t = mean
            
        x_t = x_t.detach().requires_grad_(True)
    
    return x_t.detach()

# ç¤ºä¾‹ï¼šç±»åˆ«å¼•å¯¼
def classifier_guidance(x_t, t, classifier, target_class):
    """ä½¿ç”¨åˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆç‰¹å®šç±»åˆ«"""
    logits = classifier(x_t, t)
    log_prob = F.log_softmax(logits, dim=1)
    return -log_prob[:, target_class].sum()  # è´Ÿå¯¹æ•°æ¦‚ç‡ä½œä¸ºæŸå¤±</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.2ï¼šæ¢ç´¢æ¸©åº¦å‚æ•°çš„å½±å“</div>
            <p>å®ç°ä¸€ä¸ªå®éªŒï¼Œç³»ç»Ÿåœ°æ¢ç´¢ä¸åŒæ¸©åº¦å‚æ•°å¯¹ç”Ÿæˆç»“æœçš„å½±å“ï¼š</p>
            <ol>
                <li>å›ºå®šç§å­ï¼Œæ”¹å˜temperatureï¼ˆ0.5, 0.7, 1.0, 1.3, 1.5ï¼‰</li>
                <li>å›ºå®šç§å­ï¼Œæ”¹å˜noise_temperatureï¼ˆ0, 0.5, 1.0, 1.5ï¼‰</li>
                <li>å¯è§†åŒ–ç»“æœå¹¶è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡ï¼ˆå¦‚å¹³å‡åƒç´ æ–¹å·®ï¼‰</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_2')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_2" class="answer">
                <pre>def temperature_ablation_study(model, seed=42):
    """æ¸©åº¦å‚æ•°æ¶ˆèå®éªŒ"""
    shape = (1, 3, 32, 32)
    
    # å®éªŒ1ï¼šåˆå§‹æ¸©åº¦çš„å½±å“
    init_temps = [0.5, 0.7, 1.0, 1.3, 1.5]
    init_results = []
    
    for temp in init_temps:
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        sample = ddpm_sample_with_temperature(
            model, shape, 
            temperature=temp,
            noise_temperature=1.0
        )
        init_results.append(sample)
    
    # å®éªŒ2ï¼šå™ªå£°æ¸©åº¦çš„å½±å“
    noise_temps = [0.0, 0.5, 1.0, 1.5]
    noise_results = []
    
    for noise_temp in noise_temps:
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        sample = ddpm_sample_with_temperature(
            model, shape,
            temperature=1.0,
            noise_temperature=noise_temp
        )
        noise_results.append(sample)
    
    # åˆ†æç»“æœ
    results = {
        'init_temperature': {},
        'noise_temperature': {}
    }
    
    # è®¡ç®—åˆå§‹æ¸©åº¦çš„å½±å“
    print("åˆå§‹æ¸©åº¦å¯¹å›¾åƒç»Ÿè®¡ç‰¹æ€§çš„å½±å“:")
    for temp, img in zip(init_temps, init_results):
        stats = {
            'mean': img.mean().item(),
            'std': img.std().item(),
            'min': img.min().item(),
            'max': img.max().item()
        }
        results['init_temperature'][temp] = stats
        print(f"  T_init={temp}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")
    
    # è®¡ç®—å™ªå£°æ¸©åº¦çš„å½±å“
    print("\nå™ªå£°æ¸©åº¦å¯¹å›¾åƒç»Ÿè®¡ç‰¹æ€§çš„å½±å“:")
    for temp, img in zip(noise_temps, noise_results):
        stats = {
            'mean': img.mean().item(),
            'std': img.std().item(),
            'min': img.min().item(),
            'max': img.max().item()
        }
        results['noise_temperature'][temp] = stats
        print(f"  T_noise={temp}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")
    
    return results

# é¢å¤–åˆ†æï¼šå¤šæ¬¡é‡‡æ ·çš„å¤šæ ·æ€§
def diversity_analysis(model, num_samples=100):
    """åˆ†æä¸åŒæ¸©åº¦è®¾ç½®ä¸‹çš„æ ·æœ¬å¤šæ ·æ€§"""
    shape = (num_samples, 3, 32, 32)
    
    # æ ‡å‡†é‡‡æ ·
    samples_standard = ddpm_sample(model, shape)
    
    # ä½æ¸©é‡‡æ ·
    samples_low_temp = ddpm_sample_with_temperature(
        model, shape, temperature=0.7, noise_temperature=0.7
    )
    
    # è®¡ç®—æˆå¯¹è·ç¦»
    def pairwise_l2_distance(samples):
        # å±•å¹³æ ·æœ¬
        flat = samples.view(num_samples, -1)
        # è®¡ç®—æˆå¯¹L2è·ç¦»
        distances = torch.cdist(flat, flat, p=2)
        # å–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆé¿å…é‡å¤ï¼‰
        mask = torch.triu(torch.ones_like(distances), diagonal=1).bool()
        return distances[mask].mean().item()
    
    div_standard = pairwise_l2_distance(samples_standard)
    div_low_temp = pairwise_l2_distance(samples_low_temp)
    
    print(f"æ ‡å‡†é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_standard:.4f}")
    print(f"ä½æ¸©é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_low_temp:.4f}")
    print(f"å¤šæ ·æ€§é™ä½æ¯”ä¾‹: {(1 - div_low_temp/div_standard)*100:.1f}%")</pre>
                <p><strong>å…³é”®å‘ç°</strong>ï¼š</p>
                <ul>
                    <li>é™ä½åˆå§‹æ¸©åº¦ä¼šä½¿ç”Ÿæˆç»“æœæ›´æ¥è¿‘"å¹³å‡"å›¾åƒï¼Œå‡å°‘æç«¯æƒ…å†µ</li>
                    <li>noise_temperature=0 ä¼šäº§ç”Ÿè¿‡åº¦å¹³æ»‘çš„ç»“æœï¼Œä¸¢å¤±çº¹ç†ç»†èŠ‚</li>
                    <li>é€‚åº¦é™ä½æ¸©åº¦ï¼ˆ0.7-0.9ï¼‰é€šå¸¸èƒ½æé«˜æ„ŸçŸ¥è´¨é‡ï¼Œä½†ä¼šç‰ºç‰²å¤šæ ·æ€§</li>
                    <li>å¯¹äºç‰¹å®šåº”ç”¨ï¼Œéœ€è¦åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡</li>
                </ul>
            </div>
        </div>
        
        <h3>3.5.3 å¸¸è§é—®é¢˜ä¸è°ƒè¯•æŠ€å·§</h3>
        
        <p>DDPMé‡‡æ ·è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°å„ç§é—®é¢˜ã€‚æœ¬èŠ‚æ€»ç»“å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œä¿®å¤é—®é¢˜ã€‚</p>
        
        <h4>é—®é¢˜1ï¼šç”Ÿæˆç»“æœå…¨æ˜¯å™ªå£°</h4>
        
        <div class="definition">
            <div class="definition-title">ç—‡çŠ¶ä¸åŸå› </div>
            <ul>
                <li><strong>ç—‡çŠ¶</strong>ï¼šé‡‡æ ·ç»“æœçœ‹èµ·æ¥åƒéšæœºå™ªå£°ï¼Œæ²¡æœ‰ä»»ä½•ç»“æ„</li>
                <li><strong>å¯èƒ½åŸå› </strong>ï¼š
                    <ol>
                        <li>æ¨¡å‹æœªæ­£ç¡®åŠ è½½æˆ–æƒé‡æŸå</li>
                        <li>å™ªå£°è°ƒåº¦è®¡ç®—é”™è¯¯</li>
                        <li>æ—¶é—´æ­¥ç¼–ç é”™è¯¯</li>
                        <li>è¾“å…¥å½’ä¸€åŒ–ä¸åŒ¹é…</li>
                    </ol>
                </li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># è°ƒè¯•æ­¥éª¤1ï¼šéªŒè¯æ¨¡å‹é¢„æµ‹
def debug_model_predictions(model, device='cuda'):
    """æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ—¶é—´æ­¥çš„é¢„æµ‹"""
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(1, 3, 32, 32, device=device)
    
    # æµ‹è¯•å‡ ä¸ªå…³é”®æ—¶é—´æ­¥
    test_timesteps = [0, 250, 500, 750, 999]
    
    for t in test_timesteps:
        t_tensor = torch.tensor([t], device=device)
        with torch.no_grad():
            pred = model(x, t_tensor)
        
        print(f"t={t}:")
        print(f"  Input stats: mean={x.mean():.4f}, std={x.std():.4f}")
        print(f"  Pred stats:  mean={pred.mean():.4f}, std={pred.std():.4f}")
        
        # é¢„æµ‹åº”è¯¥æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        if abs(pred.mean()) > 0.5 or abs(pred.std() - 1.0) > 0.5:
            print("  âš ï¸ è­¦å‘Šï¼šé¢„æµ‹ç»Ÿè®¡é‡å¼‚å¸¸ï¼")

# è°ƒè¯•æ­¥éª¤2ï¼šéªŒè¯å™ªå£°è°ƒåº¦
def debug_noise_schedule(num_timesteps=1000):
    """æ£€æŸ¥å™ªå£°è°ƒåº¦çš„åˆç†æ€§"""
    betas = linear_beta_schedule(num_timesteps)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    print("å™ªå£°è°ƒåº¦æ£€æŸ¥:")
    print(f"Î²_0 = {betas[0]:.6f}, Î²_T = {betas[-1]:.6f}")
    print(f"á¾±_0 = {alphas_bar[0]:.6f}, á¾±_T = {alphas_bar[-1]:.6f}")
    
    # æ£€æŸ¥å…³é”®å±æ€§
    if alphas_bar[-1] > 0.01:
        print("âš ï¸ è­¦å‘Šï¼šá¾±_T å¤ªå¤§ï¼Œæœ€ç»ˆå™ªå£°æ°´å¹³ä¸å¤Ÿ")
    if betas[0] > 0.01:
        print("âš ï¸ è­¦å‘Šï¼šÎ²_0 å¤ªå¤§ï¼Œåˆå§‹ç ´åå¤ªä¸¥é‡")
    
    # æ£€æŸ¥å•è°ƒæ€§
    if not torch.all(alphas_bar[1:] <= alphas_bar[:-1]):
        print("âš ï¸ è­¦å‘Šï¼šá¾± ä¸æ˜¯å•è°ƒé€’å‡çš„ï¼")
    
    return betas, alphas, alphas_bar</pre>
        </div>
        
        <h4>é—®é¢˜2ï¼šç”Ÿæˆç»“æœæ¨¡ç³Šæˆ–è¿‡åº¦å¹³æ»‘</h4>
        
        <div class="visualization" style="background-color: #fff3cd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h5>å¸¸è§åŸå› åŠè§£å†³æ–¹æ¡ˆ</h5>
            <ol>
                <li><strong>æ–¹å·®è®¾ç½®è¿‡å°</strong>ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¿‡å°çš„ $\sigma_t$</li>
                <li><strong>æå‰åœæ­¢é‡‡æ ·</strong>ï¼šç¡®ä¿å®Œæˆæ‰€æœ‰1000æ­¥ï¼ˆæˆ–è®¾å®šçš„æ­¥æ•°ï¼‰</li>
                <li><strong>æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°å‡å€¼</strong>ï¼šå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒæ—¶çš„å™ªå£°è°ƒåº¦</li>
                <li><strong>æ•°å€¼ç²¾åº¦é—®é¢˜</strong>ï¼šä½¿ç”¨FP16æ—¶æŸäº›æ“ä½œå¯èƒ½æŸå¤±ç²¾åº¦</li>
            </ol>
        </div>
        
        <div class="code-block">
<pre># è¯Šæ–­è¿‡åº¦å¹³æ»‘é—®é¢˜
def diagnose_smoothness(model, num_samples=10):
    """è¯Šæ–­ç”Ÿæˆç»“æœçš„å¹³æ»‘åº¦é—®é¢˜"""
    samples = []
    
    # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
    for _ in range(num_samples):
        sample = ddpm_sample(model, (1, 3, 32, 32))
        samples.append(sample)
    
    samples = torch.cat(samples, dim=0)
    
    # è®¡ç®—é«˜é¢‘ä¿¡æ¯
    def compute_high_freq_energy(images):
        # ä½¿ç”¨Sobelæ»¤æ³¢å™¨æ£€æµ‹è¾¹ç¼˜
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                               dtype=torch.float32).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                               dtype=torch.float32).view(1, 1, 3, 3)
        
        # è½¬æ¢ä¸ºç°åº¦
        gray = images.mean(dim=1, keepdim=True)
        
        # è®¡ç®—æ¢¯åº¦
        edges_x = F.conv2d(gray, sobel_x, padding=1)
        edges_y = F.conv2d(gray, sobel_y, padding=1)
        edges = torch.sqrt(edges_x**2 + edges_y**2)
        
        return edges.mean().item()
    
    # ä¸çœŸå®æ•°æ®å¯¹æ¯”
    real_data = next(iter(train_loader))[0][:num_samples]
    
    gen_hf = compute_high_freq_energy(samples)
    real_hf = compute_high_freq_energy(real_data)
    
    print(f"ç”Ÿæˆæ ·æœ¬çš„é«˜é¢‘èƒ½é‡: {gen_hf:.4f}")
    print(f"çœŸå®æ•°æ®çš„é«˜é¢‘èƒ½é‡: {real_hf:.4f}")
    print(f"æ¯”ç‡: {gen_hf/real_hf:.2f}")
    
    if gen_hf/real_hf < 0.5:
        print("âš ï¸ ç”Ÿæˆç»“æœå¯èƒ½è¿‡åº¦å¹³æ»‘ï¼")
        print("å»ºè®®ï¼š")
        print("  1. æ£€æŸ¥å™ªå£°æ¸©åº¦è®¾ç½®")
        print("  2. éªŒè¯æœ€åå‡ æ­¥çš„æ–¹å·®")
        print("  3. è€ƒè™‘ä½¿ç”¨æ”¹è¿›çš„æ–¹å·®è°ƒåº¦")</pre>
        </div>
        
        <h4>é—®é¢˜3ï¼šç”Ÿæˆé€Ÿåº¦ææ…¢</h4>
        
        <div class="note-box">
            <div class="box-title">æ€§èƒ½åˆ†æä¸ä¼˜åŒ–</div>
            <p>DDPMçš„ä¸»è¦æ€§èƒ½ç“¶é¢ˆåœ¨äºéœ€è¦1000æ­¥è¿­ä»£ï¼Œæ¯æ­¥éƒ½éœ€è¦é€šè¿‡U-Netè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ã€‚å…¸å‹çš„æ€§èƒ½ç‰¹å¾ï¼š</p>
            <ul>
                <li><strong>é¢„è®¡ç®—é˜¶æ®µ</strong>ï¼šçº¦0.1ç§’ï¼ŒåŒ…æ‹¬å™ªå£°è°ƒåº¦çš„è®¡ç®—</li>
                <li><strong>æ¨¡å‹æ¨ç†</strong>ï¼šæ¯æ­¥15-50msï¼ˆå–å†³äºæ¨¡å‹å¤§å°å’ŒGPUï¼‰ï¼Œæ€»è®¡15-50ç§’</li>
                <li><strong>æ›´æ–°è®¡ç®—</strong>ï¼šæ¯æ­¥1-2msï¼Œç›¸å¯¹å¯å¿½ç•¥</li>
            </ul>
            
            <p><strong>ä¼˜åŒ–å»ºè®®</strong>ï¼š</p>
            <ol>
                <li>ä½¿ç”¨æ›´å°çš„æ¨¡å‹æ¶æ„ï¼ˆå‡å°‘é€šé“æ•°æˆ–å±‚æ•°ï¼‰</li>
                <li>å¯ç”¨æ··åˆç²¾åº¦æ¨ç†ï¼ˆtorch.cuda.ampï¼‰</li>
                <li>ä½¿ç”¨torch.compile()è¿›è¡Œå›¾ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰</li>
                <li>é‡‡ç”¨DDIMç­‰å¿«é€Ÿé‡‡æ ·æ–¹æ³•ï¼ˆå¯å‡å°‘åˆ°50æ­¥ä»¥ä¸‹ï¼‰</li>
                <li>æ‰¹é‡ç”Ÿæˆä»¥æé«˜GPUåˆ©ç”¨ç‡</li>
            </ol>
        </div>
        
        <h4>é—®é¢˜4ï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰</h4>
        
        <div class="code-block">
<pre># å†…å­˜å‹å¥½çš„æ‰¹é‡é‡‡æ ·
def memory_efficient_batch_sampling(model, total_samples, batch_size=16, 
                                  image_shape=(3, 32, 32)):
    """å†…å­˜é«˜æ•ˆçš„æ‰¹é‡é‡‡æ ·"""
    all_samples = []
    
    # åˆ†æ‰¹ç”Ÿæˆ
    num_batches = (total_samples + batch_size - 1) // batch_size
    
    for i in tqdm(range(num_batches), desc="Batch sampling"):
        current_batch_size = min(batch_size, total_samples - i * batch_size)
        shape = (current_batch_size,) + image_shape
        
        # ç”Ÿæˆå½“å‰æ‰¹æ¬¡
        with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
            samples = ddpm_sample(model, shape)
        
        # ç«‹å³ç§»åˆ°CPUä»¥é‡Šæ”¾GPUå†…å­˜
        all_samples.append(samples.cpu())
        
        # æ¸…ç†GPUç¼“å­˜
        if i % 10 == 0:
            torch.cuda.empty_cache()
    
    return torch.cat(all_samples, dim=0)

# è¯Šæ–­å†…å­˜ä½¿ç”¨
def diagnose_memory_usage(model, batch_sizes=[1, 2, 4, 8, 16]):
    """è¯Šæ–­ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨"""
    import gc
    
    for bs in batch_sizes:
        torch.cuda.empty_cache()
        gc.collect()
        
        try:
            # è®°å½•åˆå§‹å†…å­˜
            init_mem = torch.cuda.memory_allocated() / 1024**3
            
            # å°è¯•é‡‡æ ·
            shape = (bs, 3, 256, 256)  # ä½¿ç”¨è¾ƒå¤§å°ºå¯¸æµ‹è¯•
            _ = ddpm_sample(model, shape, num_timesteps=50)  # åªæµ‹è¯•50æ­¥
            
            # è®°å½•å³°å€¼å†…å­˜
            peak_mem = torch.cuda.max_memory_allocated() / 1024**3
            
            print(f"Batch size {bs}: å³°å€¼å†…å­˜ {peak_mem:.2f}GB "
                  f"(å¢åŠ  {peak_mem - init_mem:.2f}GB)")
            
        except torch.cuda.OutOfMemoryError:
            print(f"Batch size {bs}: OOM!")
            break
        finally:
            torch.cuda.empty_cache()</pre>
        </div>
        
        <h4>å¯è§†åŒ–è°ƒè¯•å·¥å…·</h4>
        
        <div class="code-block">
<pre>def analyze_sampling_debug(model):
    """åˆ†æé‡‡æ ·è¿‡ç¨‹ç”¨äºè°ƒè¯•"""
    # è®¾ç½®
    shape = (1, 3, 32, 32)
    checkpoints = [999, 800, 600, 400, 200, 100, 50, 20, 10, 0]
    
    # æ”¶é›†æ•°æ®
    x_t = torch.randn(shape, device='cuda')
    debug_data = {
        'x_t_history': [x_t.cpu()],
        'pred_x0_history': [],
        'noise_pred_history': [],
        'noise_stats': []
    }
    
    # é‡‡æ ·å¹¶è®°å½•
    betas = linear_beta_schedule(1000).cuda()
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    for t in reversed(range(1000)):
        t_tensor = torch.tensor([t], device='cuda')
        
        # é¢„æµ‹
        epsilon_pred = model(x_t, t_tensor)
        
        # é¢„æµ‹çš„x_0
        pred_x0 = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])
        
        # æ›´æ–°
        mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])
        if t > 0:
            noise = torch.randn_like(x_t)
            x_t = mean + torch.sqrt(betas[t]) * noise
        else:
            x_t = mean
        
        # è®°å½•æ£€æŸ¥ç‚¹
        if t in checkpoints:
            debug_data['x_t_history'].append(x_t.cpu())
            debug_data['pred_x0_history'].append(pred_x0.cpu())
            debug_data['noise_pred_history'].append(epsilon_pred.cpu())
            debug_data['noise_stats'].append({
                't': t,
                'mean': epsilon_pred.mean().item(),
                'std': epsilon_pred.std().item(),
                'min': epsilon_pred.min().item(),
                'max': epsilon_pred.max().item()
            })
    
    # æ‰“å°åˆ†æç»“æœ
    print("é‡‡æ ·è¿‡ç¨‹è°ƒè¯•åˆ†æ:")
    print("==================")
    print("\nå™ªå£°é¢„æµ‹ç»Ÿè®¡:")
    print("Timestep | Mean      | Std       | Min       | Max")
    print("-" * 55)
    for stats in debug_data['noise_stats']:
        print(f"{stats['t']:8d} | {stats['mean']:9.6f} | {stats['std']:9.6f} | {stats['min']:9.6f} | {stats['max']:9.6f}")
    
    # æ£€æŸ¥x_0é¢„æµ‹çš„ç¨³å®šæ€§
    print("\nx_0é¢„æµ‹ç¨³å®šæ€§åˆ†æ:")
    for i, (t, x0) in enumerate(zip(checkpoints[:-1], debug_data['pred_x0_history'])):
        x0_range = x0.max().item() - x0.min().item()
        x0_clipped = (x0 < -1).sum().item() + (x0 > 1).sum().item()
        total_pixels = x0.numel()
        clip_ratio = x0_clipped / total_pixels
        print(f"t={t:3d}: range={x0_range:.3f}, clipped pixels={clip_ratio:.1%}")
    
    return debug_data</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.3ï¼šå®ç°é‡‡æ ·è´¨é‡è¯Šæ–­å·¥å…·</div>
            <p>åˆ›å»ºä¸€ä¸ªç»¼åˆè¯Šæ–­å·¥å…·ï¼Œèƒ½å¤Ÿï¼š</p>
            <ol>
                <li>è‡ªåŠ¨æ£€æµ‹å¸¸è§çš„é‡‡æ ·é—®é¢˜</li>
                <li>ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š</li>
                <li>æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_3')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_3" class="answer">
                <pre>class DDPMSamplingDiagnostics:
    """DDPMé‡‡æ ·ç»¼åˆè¯Šæ–­å·¥å…·"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.diagnostics = {}
        
    def run_full_diagnostics(self, num_samples=5):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("=== DDPMé‡‡æ ·è¯Šæ–­å¼€å§‹ ===\n")
        
        # 1. æ¨¡å‹åŸºç¡€æ£€æŸ¥
        self._check_model_basics()
        
        # 2. å™ªå£°è°ƒåº¦æ£€æŸ¥
        self._check_noise_schedule()
        
        # 3. é‡‡æ ·è´¨é‡æ£€æŸ¥
        self._check_sampling_quality(num_samples)
        
        # 4. æ€§èƒ½æ£€æŸ¥
        self._check_performance()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
        
    def _check_model_basics(self):
        """æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®"""
        print("1. æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®...")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨evalæ¨¡å¼
        if self.model.training:
            self.diagnostics['model_mode'] = 'WARNING: æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼'
        else:
            self.diagnostics['model_mode'] = 'OK: æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼'
        
        # æ£€æŸ¥å‚æ•°ç»Ÿè®¡
        params = []
        for p in self.model.parameters():
            params.append(p.data.flatten())
        params = torch.cat(params)
        
        param_mean = params.mean().item()
        param_std = params.std().item()
        
        if abs(param_mean) > 1.0 or param_std > 10.0:
            self.diagnostics['param_stats'] = f'WARNING: å‚æ•°ç»Ÿè®¡å¼‚å¸¸ (mean={param_mean:.3f}, std={param_std:.3f})'
        else:
            self.diagnostics['param_stats'] = 'OK: å‚æ•°ç»Ÿè®¡æ­£å¸¸'
            
    def _check_noise_schedule(self):
        """æ£€æŸ¥å™ªå£°è°ƒåº¦"""
        print("2. æ£€æŸ¥å™ªå£°è°ƒåº¦...")
        
        betas = linear_beta_schedule(1000)
        alphas_bar = torch.cumprod(1 - betas, dim=0)
        
        # æ£€æŸ¥ç«¯ç‚¹
        if alphas_bar[0] < 0.99:
            self.diagnostics['schedule_start'] = f'WARNING: Î±Ì…_0={alphas_bar[0]:.4f} å¤ªå°'
        else:
            self.diagnostics['schedule_start'] = 'OK: èµ·å§‹ç‚¹æ­£å¸¸'
            
        if alphas_bar[-1] > 0.01:
            self.diagnostics['schedule_end'] = f'WARNING: Î±Ì…_T={alphas_bar[-1]:.4f} å¤ªå¤§'
        else:
            self.diagnostics['schedule_end'] = 'OK: ç»ˆç‚¹æ­£å¸¸'
            
    def _check_sampling_quality(self, num_samples):
        """æ£€æŸ¥é‡‡æ ·è´¨é‡"""
        print(f"3. æ£€æŸ¥é‡‡æ ·è´¨é‡ (ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬)...")
        
        samples = []
        for _ in range(num_samples):
            sample = ddpm_sample(self.model, (1, 3, 32, 32), device=self.device)
            samples.append(sample)
        samples = torch.cat(samples)
        
        # æ£€æŸ¥è¾“å‡ºèŒƒå›´
        sample_min = samples.min().item()
        sample_max = samples.max().item()
        
        if sample_min < -3 or sample_max > 3:
            self.diagnostics['output_range'] = f'WARNING: è¾“å‡ºèŒƒå›´å¼‚å¸¸ [{sample_min:.2f}, {sample_max:.2f}]'
        else:
            self.diagnostics['output_range'] = 'OK: è¾“å‡ºèŒƒå›´æ­£å¸¸'
            
        # æ£€æŸ¥å¤šæ ·æ€§
        if num_samples > 1:
            diversity = samples.std(dim=0).mean().item()
            if diversity < 0.1:
                self.diagnostics['diversity'] = f'WARNING: æ ·æœ¬å¤šæ ·æ€§è¿‡ä½ (std={diversity:.3f})'
            else:
                self.diagnostics['diversity'] = 'OK: æ ·æœ¬å¤šæ ·æ€§æ­£å¸¸'
                
    def _check_performance(self):
        """æ£€æŸ¥æ€§èƒ½"""
        print("4. æ£€æŸ¥æ€§èƒ½...")
        
        import time
        shape = (1, 3, 32, 32)
        
        # æµ‹è¯•å•æ­¥æ—¶é—´
        x = torch.randn(shape, device=self.device)
        t = torch.tensor([500], device=self.device)
        
        # é¢„çƒ­
        for _ in range(10):
            _ = self.model(x, t)
        torch.cuda.synchronize()
        
        # è®¡æ—¶
        start = time.time()
        for _ in range(100):
            _ = self.model(x, t)
        torch.cuda.synchronize()
        step_time = (time.time() - start) / 100
        
        total_time = step_time * 1000
        if total_time > 60:
            self.diagnostics['performance'] = f'WARNING: é¢„è®¡é‡‡æ ·æ—¶é—´è¿‡é•¿ ({total_time:.1f}ç§’)'
        else:
            self.diagnostics['performance'] = f'OK: é¢„è®¡é‡‡æ ·æ—¶é—´ {total_time:.1f}ç§’'
            
    def _generate_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        print("\n=== è¯Šæ–­æŠ¥å‘Š ===")
        
        warnings = 0
        for key, value in self.diagnostics.items():
            if value.startswith('WARNING'):
                print(f"âŒ {value}")
                warnings += 1
            else:
                print(f"âœ… {value}")
                
        print(f"\næ€»ç»“: {len(self.diagnostics)}é¡¹æ£€æŸ¥, {warnings}ä¸ªè­¦å‘Š")
        
        if warnings > 0:
            print("\nå»ºè®®çš„ä¿®å¤æ­¥éª¤:")
            if 'model_mode' in self.diagnostics and 'WARNING' in self.diagnostics['model_mode']:
                print("- è°ƒç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
            if 'schedule_end' in self.diagnostics and 'WARNING' in self.diagnostics['schedule_end']:
                print("- å¢åŠ æ€»æ—¶é—´æ­¥æ•°æˆ–è°ƒæ•´beta_end")
            if 'diversity' in self.diagnostics and 'WARNING' in self.diagnostics['diversity']:
                print("- æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¨¡å¼å´©å¡Œ")
            if 'performance' in self.diagnostics and 'WARNING' in self.diagnostics['performance']:
                print("- è€ƒè™‘ä½¿ç”¨DDIMæˆ–å…¶ä»–å¿«é€Ÿé‡‡æ ·æ–¹æ³•")

# ä½¿ç”¨ç¤ºä¾‹
diagnostics = DDPMSamplingDiagnostics(model)
diagnostics.run_full_diagnostics()</pre>
                <p><strong>è¯Šæ–­å·¥å…·çš„æ‰©å±•</strong>ï¼šå¯ä»¥æ·»åŠ æ›´å¤šæ£€æŸ¥é¡¹ï¼Œå¦‚ï¼š</p>
                <ul>
                    <li>æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†EMAæ¨¡å‹</li>
                    <li>éªŒè¯æ¡ä»¶ç”Ÿæˆçš„æ­£ç¡®æ€§</li>
                    <li>æ£€æµ‹ç‰¹å®šçš„è§†è§‰ä¼ªå½±ï¼ˆæ£‹ç›˜æ•ˆåº”ã€è‰²å½©åç§»ç­‰ï¼‰</li>
                    <li>ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„ç»Ÿè®¡å¯¹æ¯”</li>
                </ul>
            </div>
        </div>
        
        <h2>3.6 å®Œæ•´å®ç°ï¼šæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªDDPM</h2>
        <p>æœ¬èŠ‚å°†æŠŠå‰é¢å­¦åˆ°çš„æ‰€æœ‰æ¦‚å¿µæ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„DDPMå®ç°ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå¯ä»¥åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å®Œæ•´ç³»ç»Ÿã€‚</p>
        
        <h3>3.6.1 æ¨¡å‹æ¶æ„</h3>
        <p>é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªé€‚åˆDDPMçš„U-Netæ¶æ„ã€‚è¿™ä¸ªæ¶æ„éœ€è¦ï¼š</p>
        <ul>
            <li>æ¥å—å¸¦å™ªå£°çš„å›¾åƒ $x_t$ ä½œä¸ºè¾“å…¥</li>
            <li>æ¥å—æ—¶é—´æ­¥ $t$ ä½œä¸ºæ¡ä»¶ä¿¡æ¯</li>
            <li>è¾“å‡ºé¢„æµ‹çš„å™ªå£° $\epsilon_\theta(x_t, t)$</li>
        </ul>

        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SinusoidalPositionalEmbedding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç ï¼Œç”¨äºæ—¶é—´æ­¥åµŒå…¥"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    
    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings

class ResidualBlock(nn.Module):
    """å¸¦æ—¶é—´åµŒå…¥çš„æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.time_emb = nn.Linear(time_emb_dim, out_channels)
        self.dropout = nn.Dropout(dropout)
        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
        
    def forward(self, x, t):
        h = self.conv1(x)
        h = self.norm1(h)
        h = F.silu(h)
        
        # æ·»åŠ æ—¶é—´åµŒå…¥
        h = h + self.time_emb(F.silu(t))[:, :, None, None]
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        
        return h + self.shortcut(x)

class AttentionBlock(nn.Module):
    """è‡ªæ³¨æ„åŠ›å—"""
    def __init__(self, channels, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.norm = nn.GroupNorm(8, channels)
        self.qkv = nn.Conv2d(channels, channels * 3, 1)
        self.proj = nn.Conv2d(channels, channels, 1)
        
    def forward(self, x):
        B, C, H, W = x.shape
        h = self.norm(x)
        qkv = self.qkv(h)
        q, k, v = qkv.chunk(3, dim=1)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = q.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        k = k.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        v = v.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scale = (C // self.num_heads) ** -0.5
        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * scale, dim=-1)
        out = torch.matmul(attn, v)
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        out = out.transpose(2, 3).contiguous().view(B, C, H, W)
        return x + self.proj(out)</pre>
        </div>

        <div class="important-box">
            <div class="box-title">æ¶æ„è®¾è®¡è¦ç‚¹</div>
            <ul>
                <li><strong>æ—¶é—´åµŒå…¥</strong>ï¼šä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç å°†ç¦»æ•£æ—¶é—´æ­¥è½¬æ¢ä¸ºè¿ç»­è¡¨ç¤º</li>
                <li><strong>æ®‹å·®è¿æ¥</strong>ï¼šæ¯ä¸ªå—éƒ½åŒ…å«æ®‹å·®è¿æ¥ï¼Œæœ‰åŠ©äºæ¢¯åº¦æµåŠ¨</li>
                <li><strong>æ³¨æ„åŠ›æœºåˆ¶</strong>ï¼šåœ¨ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ä¸Šä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼Œæ•è·é•¿ç¨‹ä¾èµ–</li>
                <li><strong>GroupNorm</strong>ï¼šä½¿ç”¨ç»„å½’ä¸€åŒ–è€Œéæ‰¹å½’ä¸€åŒ–ï¼Œæ›´é€‚åˆå°æ‰¹é‡è®­ç»ƒ</li>
            </ul>
        </div>

        <h4>è½»é‡çº§DDPM U-Net</h4>
        <p>å¯¹äºç®€å•ä»»åŠ¡ï¼ˆå¦‚MNISTï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ›´è½»é‡çš„æ¶æ„ï¼š</p>

        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class SimpleDDPMUNet(nn.Module):
    """è½»é‡çº§DDPM U-Netï¼Œé€‚ç”¨äºMNISTç­‰ç®€å•æ•°æ®é›†"""
    def __init__(self, image_channels=1, n_channels=32, ch_mults=(1, 2, 2, 4),
                 n_blocks=2):
        super().__init__()
        
        # æ—¶é—´åµŒå…¥
        self.time_emb = nn.Sequential(
            SinusoidalPositionalEmbedding(n_channels),
            nn.Linear(n_channels, n_channels * 4),
            nn.GELU(),
            nn.Linear(n_channels * 4, n_channels * 4)
        )
        
        # è¾“å…¥å±‚
        self.conv_in = nn.Conv2d(image_channels, n_channels, 3, padding=1)
        
        # ä¸‹é‡‡æ ·
        self.downs = nn.ModuleList()
        chs = [n_channels]
        now_ch = n_channels
        
        for i, mult in enumerate(ch_mults):
            out_ch = n_channels * mult
            for _ in range(n_blocks):
                self.downs.append(ResidualBlock(now_ch, out_ch, n_channels * 4))
                now_ch = out_ch
                chs.append(now_ch)
            
            if i < len(ch_mults) - 1:
                self.downs.append(nn.Conv2d(now_ch, now_ch, 3, stride=2, padding=1))
                chs.append(now_ch)
        
        # ä¸­é—´å±‚
        self.middle = nn.ModuleList([
            ResidualBlock(now_ch, now_ch, n_channels * 4),
            ResidualBlock(now_ch, now_ch, n_channels * 4)
        ])
        
        # ä¸Šé‡‡æ ·
        self.ups = nn.ModuleList()
        for i, mult in reversed(list(enumerate(ch_mults))):
            out_ch = n_channels * mult
            
            for _ in range(n_blocks + 1):
                self.ups.append(ResidualBlock(chs.pop() + now_ch, out_ch, n_channels * 4))
                now_ch = out_ch
            
            if i > 0:
                self.ups.append(nn.ConvTranspose2d(now_ch, now_ch, 4, stride=2, padding=1))
        
        # è¾“å‡ºå±‚
        self.conv_out = nn.Sequential(
            nn.GroupNorm(8, now_ch),
            nn.SiLU(),
            nn.Conv2d(now_ch, image_channels, 3, padding=1)
        )
    
    def forward(self, x, t):
        # è·å–æ—¶é—´åµŒå…¥
        t = self.time_emb(t)
        
        # åˆå§‹å·ç§¯
        h = self.conv_in(x)
        
        # ä¸‹é‡‡æ ·
        hs = [h]
        for layer in self.downs:
            if isinstance(layer, ResidualBlock):
                h = layer(h, t)
            else:
                h = layer(h)
            hs.append(h)
        
        # ä¸­é—´å±‚
        for layer in self.middle:
            h = layer(h, t)
        
        # ä¸Šé‡‡æ ·
        for layer in self.ups:
            if isinstance(layer, ResidualBlock):
                h = layer(torch.cat([h, hs.pop()], dim=1), t)
            else:
                h = layer(h)
        
        # è¾“å‡º
        return self.conv_out(h)</pre>
        </div>

        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.6.1ï¼šæ¨¡å‹å‚æ•°è®¡ç®—</div>
            <p>å®ç°ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—U-Netæ¨¡å‹çš„å‚æ•°é‡ï¼Œå¹¶æ¯”è¾ƒä¸åŒé…ç½®çš„æ¨¡å‹å¤§å°ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_6_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_6_1" class="answer">
                <pre>def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def compare_model_sizes():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹é…ç½®çš„å‚æ•°é‡"""
    configs = [
        {"name": "Tiny", "n_channels": 16, "ch_mults": (1, 2, 2)},
        {"name": "Small", "n_channels": 32, "ch_mults": (1, 2, 2, 4)},
        {"name": "Base", "n_channels": 64, "ch_mults": (1, 2, 4, 8)},
        {"name": "Large", "n_channels": 128, "ch_mults": (1, 2, 4, 8)}
    ]
    
    for config in configs:
        model = SimpleDDPMUNet(
            n_channels=config["n_channels"],
            ch_mults=config["ch_mults"]
        )
        params = count_parameters(model)
        print(f"{config['name']}: {params:,} parameters ({params/1e6:.2f}M)")

# è¾“å‡ºç¤ºä¾‹ï¼š
# Tiny: 461,729 parameters (0.46M)
# Small: 3,652,481 parameters (3.65M)
# Base: 35,742,785 parameters (35.74M)
# Large: 142,836,097 parameters (142.84M)</pre>
            </div>
        </div>
        
        <h3>3.6.2 è®­ç»ƒå¾ªç¯</h3>
        <p>ç°åœ¨è®©æˆ‘ä»¬å®ç°å®Œæ•´çš„DDPMè®­ç»ƒå¾ªç¯ã€‚è¿™ä¸ªå®ç°åŒ…å«äº†å‰é¢ç« èŠ‚ä»‹ç»çš„æ‰€æœ‰å…³é”®ç»„ä»¶ã€‚</p>

        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np

class DDPMTrainer:
    """DDPMè®­ç»ƒå™¨"""
    def __init__(self, model, device='cuda', num_timesteps=1000, 
                 beta_start=1e-4, beta_end=0.02, loss_type='l2'):
        self.model = model.to(device)
        self.device = device
        self.num_timesteps = num_timesteps
        self.loss_type = loss_type
        
        # è®¾ç½®å™ªå£°è°ƒåº¦
        self.betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)
        self.alphas = 1 - self.betas
        self.alphas_bar = torch.cumprod(self.alphas, dim=0)
        self.sqrt_alphas_bar = torch.sqrt(self.alphas_bar)
        self.sqrt_one_minus_alphas_bar = torch.sqrt(1 - self.alphas_bar)
        
        # ç”¨äºé‡‡æ ·çš„é¢„è®¡ç®—å€¼
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)
        self.sqrt_alphas_bar_prev = torch.sqrt(
            torch.cat([torch.tensor([1.0]).to(device), self.alphas_bar[:-1]])
        )
        self.sqrt_one_minus_alphas_bar_prev = torch.sqrt(
            1 - torch.cat([torch.tensor([1.0]).to(device), self.alphas_bar[:-1]])
        )
        self.posterior_variance = self.betas * (1.0 - self.alphas_bar_prev) / (1.0 - self.alphas_bar)
        
    def forward_diffusion(self, x_0, t, noise=None):
        """å‰å‘æ‰©æ•£è¿‡ç¨‹"""
        if noise is None:
            noise = torch.randn_like(x_0)
        
        sqrt_alphas_bar_t = self.sqrt_alphas_bar[t].view(-1, 1, 1, 1)
        sqrt_one_minus_alphas_bar_t = self.sqrt_one_minus_alphas_bar[t].view(-1, 1, 1, 1)
        
        x_t = sqrt_alphas_bar_t * x_0 + sqrt_one_minus_alphas_bar_t * noise
        return x_t, noise
    
    def compute_loss(self, x_0, t):
        """è®¡ç®—è®­ç»ƒæŸå¤±"""
        noise = torch.randn_like(x_0)
        x_t, _ = self.forward_diffusion(x_0, t, noise)
        noise_pred = self.model(x_t, t)
        
        if self.loss_type == 'l2':
            loss = torch.nn.functional.mse_loss(noise_pred, noise)
        elif self.loss_type == 'l1':
            loss = torch.nn.functional.l1_loss(noise_pred, noise)
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")
        
        return loss
    
    def train_step(self, batch, optimizer):
        """å•æ­¥è®­ç»ƒ"""
        x_0 = batch[0].to(self.device)
        batch_size = x_0.shape[0]
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥
        t = torch.randint(0, self.num_timesteps, (batch_size,), device=self.device)
        
        # è®¡ç®—æŸå¤±
        loss = self.compute_loss(x_0, t)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        return loss.item()
    
    @torch.no_grad()
    def sample(self, num_samples, image_size=(1, 28, 28), return_trajectory=False):
        """DDPMé‡‡æ ·"""
        self.model.eval()
        
        # ä»çº¯å™ªå£°å¼€å§‹
        x_t = torch.randn(num_samples, *image_size, device=self.device)
        
        trajectory = [x_t.cpu()] if return_trajectory else None
        
        # é€æ­¥å»å™ª
        for t in tqdm(reversed(range(self.num_timesteps)), desc="Sampling"):
            t_batch = torch.full((num_samples,), t, device=self.device, dtype=torch.long)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.model(x_t, t_batch)
            
            # è®¡ç®—å‡å€¼
            beta_t = self.betas[t]
            sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alphas_bar[t]
            sqrt_recip_alpha_t = self.sqrt_recip_alphas[t]
            
            mean = sqrt_recip_alpha_t * (
                x_t - beta_t / sqrt_one_minus_alpha_bar_t * noise_pred
            )
            
            # æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
            if t > 0:
                noise = torch.randn_like(x_t)
                posterior_variance_t = self.posterior_variance[t]
                x_t = mean + torch.sqrt(posterior_variance_t) * noise
            else:
                x_t = mean
            
            if return_trajectory and t % 100 == 0:
                trajectory.append(x_t.cpu())
        
        self.model.train()
        
        if return_trajectory:
            return x_t, trajectory
        return x_t

def train_ddpm(model, train_loader, num_epochs=100, lr=2e-4, 
               device='cuda', save_interval=10):
    """å®Œæ•´çš„DDPMè®­ç»ƒæµç¨‹"""
    trainer = DDPMTrainer(model, device=device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # è®­ç»ƒå†å²
    losses = []
    
    for epoch in range(num_epochs):
        epoch_losses = []
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch in pbar:
            loss = trainer.train_step(batch, optimizer)
            epoch_losses.append(loss)
            pbar.set_postfix({'loss': f"{loss:.4f}"})
        
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")
        
        # å®šæœŸç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % save_interval == 0:
            samples = trainer.sample(16)
            save_samples(samples, epoch + 1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, f'ddpm_checkpoint_epoch_{epoch+1}.pt')
    
    return trainer, losses

def save_samples(samples, epoch, save_dir='./samples'):
    """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬"""
    import os
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºPyTorchå¼ é‡æ ¼å¼
    torch.save(samples, os.path.join(save_dir, f'samples_epoch_{epoch}.pt'))
    
    # å¯é€‰ï¼šä¿å­˜ä¸ºå•ç‹¬çš„å›¾åƒæ–‡ä»¶
    if samples.shape[1] == 1:  # å•é€šé“å›¾åƒ
        from torchvision.utils import save_image
        # å°†å€¼åŸŸä»[-1, 1]æ˜ å°„åˆ°[0, 1]
        samples_normalized = (samples + 1) / 2
        save_image(samples_normalized, 
                  os.path.join(save_dir, f'grid_epoch_{epoch}.png'),
                  nrow=4, normalize=False)
    
    print(f"å·²ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ° {save_dir}")</pre>
        </div>

        <h4>ä½¿ç”¨ç¤ºä¾‹ï¼šåœ¨MNISTä¸Šè®­ç»ƒDDPM</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre># å‡†å¤‡æ•°æ®é›†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # å½’ä¸€åŒ–åˆ°[-1, 1]
])

train_dataset = datasets.MNIST(root='./data', train=True, 
                              download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128, 
                         shuffle=True, num_workers=4)

# åˆ›å»ºæ¨¡å‹
model = SimpleDDPMUNet(
    image_channels=1,
    n_channels=32,
    ch_mults=(1, 2, 2, 4),
    n_blocks=2
)

# è®­ç»ƒæ¨¡å‹
trainer, losses = train_ddpm(
    model=model,
    train_loader=train_loader,
    num_epochs=50,
    lr=2e-4,
    device='cuda' if torch.cuda.is_available() else 'cpu',
    save_interval=10
)

# ç”Ÿæˆæ–°æ ·æœ¬
new_samples = trainer.sample(64, image_size=(1, 28, 28))

# åˆ†æè®­ç»ƒæŸå¤±
print("è®­ç»ƒæŸå¤±ç»Ÿè®¡:")
print(f"  åˆå§‹æŸå¤±: {losses[0]:.4f}")
print(f"  æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
print(f"  æœ€ä½æŸå¤±: {min(losses):.4f} (Epoch {losses.index(min(losses)) + 1})")
print(f"  æŸå¤±ä¸‹é™: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%")</pre>
        </div>

        <div class="note-box">
            <div class="box-title">è®­ç»ƒæŠ€å·§</div>
            <ul>
                <li><strong>å­¦ä¹ ç‡è°ƒåº¦</strong>ï¼šä½¿ç”¨ä½™å¼¦é€€ç«æˆ–çº¿æ€§è¡°å‡å¯ä»¥æå‡è®­ç»ƒç¨³å®šæ€§</li>
                <li><strong>EMA</strong>ï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰å¯ä»¥è·å¾—æ›´ç¨³å®šçš„ç”Ÿæˆè´¨é‡</li>
                <li><strong>æ¢¯åº¦è£å‰ª</strong>ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒåˆæœŸ</li>
                <li><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong>ï¼šä½¿ç”¨FP16å¯ä»¥åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨</li>
            </ul>
        </div>

        <h4>é«˜çº§è®­ç»ƒæŠ€æœ¯</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class EMA:
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    def __init__(self, model, decay=0.995):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()
    
    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + \
                                   (1 - self.decay) * param.data
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}

def train_ddpm_with_ema(model, train_loader, num_epochs=100):
    """å¸¦EMAçš„DDPMè®­ç»ƒ"""
    trainer = DDPMTrainer(model)
    optimizer = optim.Adam(model.parameters(), lr=2e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    ema = EMA(model)
    
    scaler = torch.cuda.amp.GradScaler()  # æ··åˆç²¾åº¦è®­ç»ƒ
    
    for epoch in range(num_epochs):
        for batch in train_loader:
            x_0 = batch[0].to(trainer.device)
            batch_size = x_0.shape[0]
            t = torch.randint(0, trainer.num_timesteps, (batch_size,), 
                            device=trainer.device)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast():
                loss = trainer.compute_loss(x_0, t)
            
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            scaler.step(optimizer)
            scaler.update()
            
            # æ›´æ–°EMA
            ema.update()
        
        scheduler.step()
        
        # ä½¿ç”¨EMAæƒé‡ç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % 10 == 0:
            ema.apply_shadow()
            samples = trainer.sample(16)
            save_samples(samples, epoch + 1)
            ema.restore()
    
    return trainer, ema</pre>
        </div>

        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.6.2ï¼šå®ç°å­¦ä¹ ç‡é¢„çƒ­</div>
            <p>ä¿®æ”¹è®­ç»ƒä»£ç ï¼Œæ·»åŠ å­¦ä¹ ç‡é¢„çƒ­ï¼ˆwarmupï¼‰åŠŸèƒ½ï¼Œåœ¨è®­ç»ƒåˆæœŸé€æ¸å¢åŠ å­¦ä¹ ç‡ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_6_2')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_6_2" class="answer">
                <pre>class WarmupCosineScheduler(optim.lr_scheduler._LRScheduler):
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
    def __init__(self, optimizer, warmup_epochs, total_epochs, 
                 warmup_lr=1e-5, base_lr=2e-4, min_lr=1e-6):
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.warmup_lr = warmup_lr
        self.base_lr = base_lr
        self.min_lr = min_lr
        super().__init__(optimizer)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # çº¿æ€§é¢„çƒ­
            lr = self.warmup_lr + (self.base_lr - self.warmup_lr) * \
                 (self.last_epoch / self.warmup_epochs)
        else:
            # ä½™å¼¦é€€ç«
            progress = (self.last_epoch - self.warmup_epochs) / \
                      (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * \
                 0.5 * (1 + np.cos(np.pi * progress))
        
        return [lr for _ in self.optimizer.param_groups]

# ä½¿ç”¨ç¤ºä¾‹
optimizer = optim.Adam(model.parameters(), lr=1e-5)  # åˆå§‹å­¦ä¹ ç‡
scheduler = WarmupCosineScheduler(
    optimizer, 
    warmup_epochs=5,
    total_epochs=100,
    warmup_lr=1e-5,
    base_lr=2e-4,
    min_lr=1e-6
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(num_epochs):
    train_one_epoch(model, train_loader, optimizer)
    scheduler.step()
    print(f"Epoch {epoch}, LR: {scheduler.get_lr()[0]:.6f}")</pre>
            </div>
        </div>
        
        <h3>3.6.3 è¯„ä¼°ä¸å¯è§†åŒ–</h3>
        <p>è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„è´¨é‡æ˜¯ä¸€ä¸ªé‡è¦ä½†å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬èŠ‚ä»‹ç»å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–æ–¹æ³•ã€‚</p>

        <h4>å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡</h4>
        <div class="definition-box">
            <div class="box-title">ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŒ‡æ ‡</div>
            <ul>
                <li><strong>FID (FrÃ©chet Inception Distance)</strong>ï¼šè¡¡é‡ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„è·ç¦»</li>
                <li><strong>IS (Inception Score)</strong>ï¼šè¯„ä¼°ç”Ÿæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§</li>
                <li><strong>LPIPS</strong>ï¼šæ„ŸçŸ¥ç›¸ä¼¼åº¦ï¼Œæ›´ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥</li>
                <li><strong>Precision/Recall</strong>ï¼šåˆ†åˆ«è¡¡é‡è´¨é‡å’Œè¦†ç›–åº¦</li>
            </ul>
        </div>

        <h4>FIDè®¡ç®—å®ç°</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>import torch
import numpy as np
from scipy import linalg
from torchvision.models import inception_v3
from torch.nn.functional import adaptive_avg_pool2d

class FIDCalculator:
    """FID (FrÃ©chet Inception Distance) è®¡ç®—å™¨"""
    def __init__(self, device='cuda'):
        self.device = device
        self.inception = inception_v3(pretrained=True, transform_input=False).to(device)
        self.inception.eval()
        # ç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚
        self.inception.fc = torch.nn.Identity()
    
    @torch.no_grad()
    def extract_features(self, images):
        """æå–Inceptionç‰¹å¾"""
        # ç¡®ä¿å›¾åƒå¤§å°è‡³å°‘ä¸º299x299ï¼ˆInception-v3è¦æ±‚ï¼‰
        if images.shape[2] < 299 or images.shape[3] < 299:
            images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)
        
        # å¦‚æœæ˜¯å•é€šé“å›¾åƒï¼Œæ‰©å±•åˆ°3é€šé“
        if images.shape[1] == 1:
            images = images.repeat(1, 3, 1, 1)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]ï¼ˆInception-v3çš„é¢„å¤„ç†è¦æ±‚ï¼‰
        images = 2 * images - 1
        
        features = self.inception(images)
        return features.cpu().numpy()
    
    def calculate_statistics(self, features):
        """è®¡ç®—å‡å€¼å’Œåæ–¹å·®"""
        mu = np.mean(features, axis=0)
        sigma = np.cov(features, rowvar=False)
        return mu, sigma
    
    def calculate_fid(self, mu1, sigma1, mu2, sigma2):
        """è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FID"""
        diff = mu1 - mu2
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µçš„å¹³æ–¹æ ¹
        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
        
        # å¤„ç†æ•°å€¼è¯¯å·®
        if not np.isfinite(covmean).all():
            msg = "FIDè®¡ç®—äº§ç”Ÿäº†æ•°å€¼è¯¯å·®"
            print(msg)
            offset = np.eye(sigma1.shape[0]) * 1e-6
            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))
        
        # ç¡®ä¿æ˜¯å®æ•°
        if np.iscomplexobj(covmean):
            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
                m = np.max(np.abs(covmean.imag))
                raise ValueError(f"Imaginary component {m}")
            covmean = covmean.real
        
        tr_covmean = np.trace(covmean)
        
        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean
    
    def compute_fid_from_samples(self, real_images, generated_images, batch_size=64):
        """ä»æ ·æœ¬è®¡ç®—FID"""
        # æå–çœŸå®å›¾åƒç‰¹å¾
        real_features = []
        for i in range(0, len(real_images), batch_size):
            batch = real_images[i:i+batch_size].to(self.device)
            features = self.extract_features(batch)
            real_features.append(features)
        real_features = np.concatenate(real_features, axis=0)
        
        # æå–ç”Ÿæˆå›¾åƒç‰¹å¾
        gen_features = []
        for i in range(0, len(generated_images), batch_size):
            batch = generated_images[i:i+batch_size].to(self.device)
            features = self.extract_features(batch)
            gen_features.append(features)
        gen_features = np.concatenate(gen_features, axis=0)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mu1, sigma1 = self.calculate_statistics(real_features)
        mu2, sigma2 = self.calculate_statistics(gen_features)
        
        # è®¡ç®—FID
        fid_score = self.calculate_fid(mu1, sigma1, mu2, sigma2)
        
        return fid_score</pre>
        </div>

        <h4>Inception Scoreå®ç°</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class InceptionScore:
    """Inception Scoreè®¡ç®—å™¨"""
    def __init__(self, device='cuda'):
        self.device = device
        self.inception = inception_v3(pretrained=True, transform_input=False).to(device)
        self.inception.eval()
    
    @torch.no_grad()
    def compute_is(self, images, batch_size=32, splits=10):
        """è®¡ç®—Inception Score
        
        Args:
            images: ç”Ÿæˆçš„å›¾åƒå¼ é‡
            batch_size: æ‰¹å¤„ç†å¤§å°
            splits: ç”¨äºè®¡ç®—ISçš„åˆ†å‰²æ•°
        
        Returns:
            is_mean: ISå‡å€¼
            is_std: ISæ ‡å‡†å·®
        """
        # è·å–é¢„æµ‹
        preds = []
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size].to(self.device)
            
            # è°ƒæ•´å¤§å°å’Œé€šé“
            if batch.shape[2] < 299:
                batch = F.interpolate(batch, size=(299, 299), mode='bilinear')
            if batch.shape[1] == 1:
                batch = batch.repeat(1, 3, 1, 1)
            
            # å½’ä¸€åŒ–
            batch = 2 * batch - 1
            
            pred = self.inception(batch)
            preds.append(F.softmax(pred, dim=1).cpu().numpy())
        
        preds = np.concatenate(preds, axis=0)
        
        # è®¡ç®—IS
        scores = []
        for i in range(splits):
            part = preds[i * len(preds) // splits: (i + 1) * len(preds) // splits]
            kl = part * (np.log(part) - np.log(np.mean(part, axis=0, keepdims=True)))
            kl = np.mean(np.sum(kl, axis=1))
            scores.append(np.exp(kl))
        
        return np.mean(scores), np.std(scores)</pre>
        </div>

        <h4>ç»¼åˆè¯„ä¼°å·¥å…·</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class DDPMEvaluator:
    """DDPMæ¨¡å‹ç»¼åˆè¯„ä¼°å·¥å…·"""
    def __init__(self, trainer, test_loader, device='cuda'):
        self.trainer = trainer
        self.test_loader = test_loader
        self.device = device
        self.fid_calculator = FIDCalculator(device)
        self.is_calculator = InceptionScore(device)
    
    def evaluate(self, num_samples=10000, batch_size=64):
        """å…¨é¢è¯„ä¼°æ¨¡å‹"""
        print("ç”Ÿæˆæ ·æœ¬è¿›è¡Œè¯„ä¼°...")
        
        # ç”Ÿæˆæ ·æœ¬
        generated_samples = []
        num_batches = (num_samples + batch_size - 1) // batch_size
        
        for i in tqdm(range(num_batches), desc="ç”Ÿæˆæ ·æœ¬"):
            samples_in_batch = min(batch_size, num_samples - i * batch_size)
            samples = self.trainer.sample(samples_in_batch, image_size=(1, 28, 28))
            generated_samples.append(samples.cpu())
        
        generated_samples = torch.cat(generated_samples, dim=0)
        
        # æ”¶é›†çœŸå®æ ·æœ¬
        real_samples = []
        for batch, _ in self.test_loader:
            real_samples.append(batch)
            if len(real_samples) * batch.shape[0] >= num_samples:
                break
        real_samples = torch.cat(real_samples, dim=0)[:num_samples]
        
        # è®¡ç®—FID
        print("è®¡ç®—FID...")
        fid_score = self.fid_calculator.compute_fid_from_samples(
            real_samples, generated_samples, batch_size=batch_size
        )
        
        # è®¡ç®—IS
        print("è®¡ç®—Inception Score...")
        is_mean, is_std = self.is_calculator.compute_is(
            generated_samples, batch_size=batch_size
        )
        
        # è®¡ç®—æ ·æœ¬å¤šæ ·æ€§
        diversity = self.compute_diversity(generated_samples)
        
        results = {
            'fid': fid_score,
            'is_mean': is_mean,
            'is_std': is_std,
            'diversity': diversity
        }
        
        return results, generated_samples
    
    def compute_diversity(self, samples):
        """è®¡ç®—æ ·æœ¬å¤šæ ·æ€§ï¼ˆä½¿ç”¨LPIPSæˆ–ç®€å•çš„L2è·ç¦»ï¼‰"""
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨L2è·ç¦»
        n_samples = min(1000, len(samples))
        indices = torch.randperm(len(samples))[:n_samples]
        subset = samples[indices]
        
        # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„L2è·ç¦»
        distances = []
        for i in range(n_samples):
            for j in range(i+1, n_samples):
                dist = torch.norm(subset[i] - subset[j], p=2)
                distances.append(dist.item())
        
        return np.mean(distances)
    
    def save_results(self, results, samples, save_path='evaluation_results.pt'):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜æ ·æœ¬å’Œè¯„ä¼°æŒ‡æ ‡
        torch.save({
            'samples': samples,
            'metrics': results,
            'timestamp': np.datetime64('now')
        }, save_path)
        
        # æ‰“å°è¯„ä¼°æŠ¥å‘Š
        print("\n" + "="*50)
        print("è¯„ä¼°ç»“æœæŠ¥å‘Š")
        print("="*50)
        print(f"FID Score: {results['fid']:.2f} (è¶Šä½è¶Šå¥½)")
        print(f"Inception Score: {results['is_mean']:.2f} Â± {results['is_std']:.2f} (è¶Šé«˜è¶Šå¥½)")
        print(f"Diversity Score: {results['diversity']:.4f} (è¶Šé«˜è¶Šå¥½)")
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        
        return results</pre>
        </div>

        <h4>ä½¿ç”¨ç¤ºä¾‹</h4>
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre># åˆ›å»ºè¯„ä¼°å™¨
evaluator = DDPMEvaluator(trainer, test_loader)

# è¿è¡Œå®Œæ•´è¯„ä¼°
results, generated_samples = evaluator.evaluate(num_samples=5000)

# æ‰“å°ç»“æœ
print(f"FID Score: {results['fid']:.2f}")
print(f"Inception Score: {results['is_mean']:.2f} Â± {results['is_std']:.2f}")
print(f"Diversity Score: {results['diversity']:.4f}")

# ä¿å­˜ç»“æœ
evaluator.save_results(results, generated_samples)

# åˆ†æé‡‡æ ·è½¨è¿¹
def analyze_sampling_trajectory(trainer, num_steps_show=10):
    """åˆ†æé‡‡æ ·è½¨è¿¹"""
    # ç”Ÿæˆå¸¦è½¨è¿¹çš„æ ·æœ¬
    samples, trajectory = trainer.sample(4, return_trajectory=True)
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ­¥éª¤
    total_steps = len(trajectory)
    step_indices = np.linspace(0, total_steps-1, num_steps_show, dtype=int)
    
    print("\né‡‡æ ·è½¨è¿¹åˆ†æ:")
    print("="*50)
    print(f"æ€»æ­¥æ•°: {trainer.num_timesteps}")
    print(f"è½¨è¿¹é‡‡æ ·ç‚¹: {len(step_indices)}")
    
    # åˆ†ææ¯ä¸ªé˜¶æ®µçš„ç»Ÿè®¡ç‰¹æ€§
    for i, step_idx in enumerate(step_indices):
        t = trainer.num_timesteps - step_idx * 100 if step_idx > 0 else trainer.num_timesteps
        img_batch = trajectory[step_idx]
        
        stats = {
            'mean': img_batch.mean().item(),
            'std': img_batch.std().item(),
            'min': img_batch.min().item(),
            'max': img_batch.max().item()
        }
        
        print(f"\næ­¥éª¤ {i+1}/{num_steps_show} (t={t}):")
        print(f"  å‡å€¼: {stats['mean']:6.3f}, æ ‡å‡†å·®: {stats['std']:6.3f}")
        print(f"  èŒƒå›´: [{stats['min']:6.3f}, {stats['max']:6.3f}]")
    
    return samples, trajectory

# åˆ†æé‡‡æ ·è½¨è¿¹
final_samples, full_trajectory = analyze_sampling_trajectory(trainer)</pre>
        </div>

        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.6.3ï¼šå®ç°Precisionå’ŒRecallæŒ‡æ ‡</div>
            <p>å®ç°æ”¹è¿›çš„Precisionå’ŒRecallæŒ‡æ ‡ï¼Œåˆ†åˆ«è¡¡é‡ç”Ÿæˆè´¨é‡å’Œæ¨¡å¼è¦†ç›–åº¦ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_6_3')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_6_3" class="answer">
                <pre>def compute_precision_recall(real_features, gen_features, k=3):
    """è®¡ç®—æ”¹è¿›çš„Precisionå’ŒRecall
    
    åŸºäºk-æœ€è¿‘é‚»çš„æ–¹æ³•ï¼š
    - Precision: ç”Ÿæˆæ ·æœ¬ä¸­æœ‰å¤šå°‘è½åœ¨çœŸå®æ•°æ®çš„æ”¯æ’‘é›†å†…
    - Recall: çœŸå®æ•°æ®çš„æ”¯æ’‘é›†æœ‰å¤šå°‘è¢«ç”Ÿæˆæ ·æœ¬è¦†ç›–
    """
    from sklearn.neighbors import NearestNeighbors
    
    # æ„å»ºk-NNæ¨¡å‹
    nbrs_real = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(real_features)
    nbrs_gen = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(gen_features)
    
    # è®¡ç®—çœŸå®æ ·æœ¬çš„k-NNè·ç¦»
    distances_real, _ = nbrs_real.kneighbors(real_features)
    distances_real = distances_real[:, -1]  # ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»
    
    # è®¡ç®—ç”Ÿæˆæ ·æœ¬çš„k-NNè·ç¦»
    distances_gen, _ = nbrs_gen.kneighbors(gen_features)
    distances_gen = distances_gen[:, -1]
    
    # è®¡ç®—Precisionï¼šç”Ÿæˆæ ·æœ¬åˆ°çœŸå®æµå½¢çš„è·ç¦»
    distances_gen_to_real, _ = nbrs_real.kneighbors(gen_features, n_neighbors=1)
    distances_gen_to_real = distances_gen_to_real[:, 0]
    precision = np.mean(distances_gen_to_real <= np.median(distances_real))
    
    # è®¡ç®—Recallï¼šçœŸå®æ ·æœ¬åˆ°ç”Ÿæˆæµå½¢çš„è·ç¦»
    distances_real_to_gen, _ = nbrs_gen.kneighbors(real_features, n_neighbors=1)
    distances_real_to_gen = distances_real_to_gen[:, 0]
    recall = np.mean(distances_real_to_gen <= np.median(distances_gen))
    
    return precision, recall

# ä½¿ç”¨ç¤ºä¾‹
real_features = fid_calculator.extract_features(real_samples)
gen_features = fid_calculator.extract_features(generated_samples)
precision, recall = compute_precision_recall(real_features, gen_features)
print(f"Precision: {precision:.3f}, Recall: {recall:.3f}")</pre>
            </div>
        </div>
        
        <h2>3.7 DDPMçš„å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘</h2>
        <p>è™½ç„¶DDPMåœ¨ç”Ÿæˆè´¨é‡ä¸Šå–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å®ƒä»å­˜åœ¨ä¸€äº›é‡è¦çš„å±€é™æ€§ã€‚ç†è§£è¿™äº›å±€é™æ€§æœ‰åŠ©äºæˆ‘ä»¬ç†è§£åç»­çš„æ”¹è¿›æ–¹æ³•ã€‚</p>

        <h3>3.7.1 ä¸»è¦å±€é™æ€§</h3>
        
        <div class="important-box">
            <div class="box-title">DDPMçš„æ ¸å¿ƒé—®é¢˜</div>
            <ol>
                <li><strong>é‡‡æ ·é€Ÿåº¦æ…¢</strong>
                    <ul>
                        <li>éœ€è¦1000æ­¥è¿­ä»£æ‰èƒ½ç”Ÿæˆä¸€å¼ å›¾åƒ</li>
                        <li>ç›¸æ¯”GANçš„å•æ¬¡å‰å‘ä¼ æ’­ï¼Œæ•ˆç‡å·®è·å·¨å¤§</li>
                        <li>é™åˆ¶äº†å®æ—¶åº”ç”¨çš„å¯èƒ½æ€§</li>
                    </ul>
                </li>
                <li><strong>å›ºå®šçš„å™ªå£°è°ƒåº¦</strong>
                    <ul>
                        <li>çº¿æ€§Î²è°ƒåº¦å¹¶éæœ€ä¼˜</li>
                        <li>ä¸åŒæ•°æ®é›†å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒåº¦ç­–ç•¥</li>
                        <li>è®­ç»ƒå’Œé‡‡æ ·å¿…é¡»ä½¿ç”¨ç›¸åŒçš„è°ƒåº¦</li>
                    </ul>
                </li>
                <li><strong>å›ºå®šçš„åéªŒæ–¹å·®</strong>
                    <ul>
                        <li>DDPMä½¿ç”¨å›ºå®šçš„åéªŒæ–¹å·® $\sigma_t^2 = \beta_t$</li>
                        <li>è¿™å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©</li>
                        <li>é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›</li>
                    </ul>
                </li>
                <li><strong>è®¡ç®—èµ„æºéœ€æ±‚é«˜</strong>
                    <ul>
                        <li>è®­ç»ƒéœ€è¦å¤§é‡GPUæ—¶é—´</li>
                        <li>æ¨ç†æ—¶çš„å†…å­˜å ç”¨è¾ƒå¤§</li>
                        <li>éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²</li>
                    </ul>
                </li>
            </ol>
        </div>

        <h3>3.7.2 æ€§èƒ½åˆ†æ</h3>
        
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>def analyze_ddpm_performance(trainer, num_samples=100):
    """åˆ†æDDPMçš„æ€§èƒ½ç“¶é¢ˆ"""
    import time
    
    results = {
        'sampling_times': [],
        'memory_usage': [],
        'step_times': []
    }
    
    # æµ‹è¯•ä¸åŒæ­¥æ•°çš„é‡‡æ ·æ—¶é—´
    for num_steps in [10, 50, 100, 500, 1000]:
        # ä¿®æ”¹é‡‡æ ·æ­¥æ•°
        original_steps = trainer.num_timesteps
        trainer.num_timesteps = num_steps
        
        # è®¡æ—¶
        start_time = time.time()
        samples = trainer.sample(num_samples, image_size=(1, 28, 28))
        end_time = time.time()
        
        sampling_time = end_time - start_time
        results['sampling_times'].append({
            'steps': num_steps,
            'total_time': sampling_time,
            'time_per_sample': sampling_time / num_samples,
            'time_per_step': sampling_time / (num_samples * num_steps)
        })
        
        trainer.num_timesteps = original_steps
    
    # åˆ†ææ¯æ­¥çš„æ—¶é—´åˆ†å¸ƒ
    with torch.profiler.profile(
        activities=[torch.profiler.ProfilerActivity.CPU, 
                   torch.profiler.ProfilerActivity.CUDA],
        record_shapes=True
    ) as prof:
        trainer.sample(1, image_size=(1, 28, 28))
    
    # æ‰“å°åˆ†æç»“æœ
    print("=== DDPM Performance Analysis ===")
    print(f"\nSampling Time vs Steps:")
    for result in results['sampling_times']:
        print(f"Steps: {result['steps']:4d} | "
              f"Total: {result['total_time']:6.2f}s | "
              f"Per Sample: {result['time_per_sample']:6.4f}s | "
              f"Per Step: {result['time_per_step']*1000:6.2f}ms")
    
    print(f"\nTop operations by time:")
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    
    return results

# åˆ†ææ€§èƒ½ç»“æœ
def analyze_performance_results(results):
    """åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ"""
    steps = [r['steps'] for r in results['sampling_times']]
    times = [r['time_per_sample'] for r in results['sampling_times']]
    
    print("\næ€§èƒ½åˆ†ææŠ¥å‘Š:")
    print("="*60)
    print("æ­¥æ•°    | æ¯æ ·æœ¬æ—¶é—´(s) | ç›¸å¯¹1000æ­¥åŠ é€Ÿæ¯” | è´¨é‡å½±å“")
    print("-"*60)
    
    baseline_time = times[-1]  # 1000æ­¥çš„æ—¶é—´
    for i, (step, time) in enumerate(zip(steps, times)):
        speedup = baseline_time / time
        quality_impact = "é«˜" if step >= 500 else ("ä¸­" if step >= 100 else "ä½")
        print(f"{step:8d} | {time:13.4f} | {speedup:15.1f}x | {quality_impact}")
    
    print("\nå…³é”®å‘ç°:")
    print(f"- ä»1000æ­¥å‡å°‘åˆ°50æ­¥å¯è·å¾— {baseline_time/times[1]:.1f}x åŠ é€Ÿ")
    print(f"- æ¯æ­¥å¹³å‡è€—æ—¶: {times[-1]/1000*1000:.2f}ms")
    print(f"- ä¸»è¦ç“¶é¢ˆ: U-Netå‰å‘ä¼ æ’­")
    
    return results</pre>
        </div>

        <h3>3.7.3 æ”¹è¿›æ–¹å‘æ¦‚è§ˆ</h3>
        
        <div class="note-box">
            <div class="box-title">ä¸»è¦æ”¹è¿›æ–¹å‘</div>
            <table style="width: 100%; border-collapse: collapse;">
                <thead>
                    <tr style="background-color: #f0f0f0;">
                        <th style="padding: 10px; text-align: left;">é—®é¢˜</th>
                        <th style="padding: 10px; text-align: left;">æ”¹è¿›æ–¹æ³•</th>
                        <th style="padding: 10px; text-align: left;">å…³é”®æ€æƒ³</th>
                        <th style="padding: 10px; text-align: left;">ç›¸å…³ç« èŠ‚</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">é‡‡æ ·é€Ÿåº¦æ…¢</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">DDIM</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¡®å®šæ€§é‡‡æ ·ï¼Œè·³æ­¥</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬8ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">é‡‡æ ·é€Ÿåº¦æ…¢</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">DPM-Solver</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">é«˜é˜¶ODEæ±‚è§£å™¨</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬8ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">å›ºå®šå™ªå£°è°ƒåº¦</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">Improved DDPM</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ä½™å¼¦è°ƒåº¦ï¼Œå­¦ä¹ æ–¹å·®</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬8ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç†è®ºæ¡†æ¶</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">Score-based Models</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">åˆ†æ•°åŒ¹é…è§†è§’</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬4ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">è¿ç»­æ—¶é—´</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">SDE/ODE</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">è¿ç»­æ—¶é—´æ¡†æ¶</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬5ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">è®¡ç®—æ•ˆç‡</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">Latent Diffusion</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">æ½œåœ¨ç©ºé—´æ‰©æ•£</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬10ç« </td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ä¸€æ­¥ç”Ÿæˆ</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">Consistency Models</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">è‡ªä¸€è‡´æ€§æ˜ å°„</td>
                        <td style="padding: 10px; border-top: 1px solid #ddd;">ç¬¬14ç« </td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>3.7.4 å®éªŒï¼šä¸åŒæ”¹è¿›çš„æ•ˆæœ</h3>
        
        <div class="code-block">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class ImprovedDDPMExperiments:
    """å®éªŒä¸åŒçš„DDPMæ”¹è¿›æ–¹æ³•"""
    
    @staticmethod
    def cosine_beta_schedule(num_timesteps, s=0.008):
        """ä½™å¼¦å™ªå£°è°ƒåº¦ï¼ˆImproved DDPMï¼‰"""
        steps = num_timesteps + 1
        x = torch.linspace(0, num_timesteps, steps)
        alphas_bar = torch.cos(((x / num_timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_bar = alphas_bar / alphas_bar[0]
        betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])
        return torch.clip(betas, 0.0001, 0.9999)
    
    @staticmethod
    def learned_variance_output(model_output, num_channels):
        """å­¦ä¹ æ–¹å·®çš„æ¨¡å‹è¾“å‡ºï¼ˆImproved DDPMï¼‰"""
        # æ¨¡å‹è¾“å‡ºä¸¤å€é€šé“ï¼šå‰åŠéƒ¨åˆ†æ˜¯å‡å€¼ï¼ŒååŠéƒ¨åˆ†æ˜¯å¯¹æ•°æ–¹å·®
        mean, log_variance = torch.split(model_output, num_channels, dim=1)
        
        # å‚æ•°åŒ–å¯¹æ•°æ–¹å·®åœ¨[beta_t, beta_tilde_t]ä¹‹é—´
        # log_variance = log(beta_t) + v * log(beta_tilde_t / beta_t)
        # å…¶ä¸­væ˜¯æ¨¡å‹é¢„æµ‹çš„æ’å€¼å‚æ•°
        return mean, log_variance
    
    @staticmethod
    def ddim_sampling_step(x_t, epsilon_pred, t, t_prev, alphas_bar, eta=0):
        """DDIMé‡‡æ ·æ­¥éª¤ï¼ˆå¯è°ƒèŠ‚éšæœºæ€§ï¼‰"""
        alpha_bar_t = alphas_bar[t]
        alpha_bar_t_prev = alphas_bar[t_prev] if t_prev >= 0 else 1.0
        
        # è®¡ç®—x_0çš„é¢„æµ‹
        x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
        
        # è®¡ç®—æ–¹å·®
        sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t)) * \
                  torch.sqrt(1 - alpha_bar_t / alpha_bar_t_prev)
        
        # é¢„æµ‹x_{t-1}
        mean = torch.sqrt(alpha_bar_t_prev) * x_0_pred + \
               torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * epsilon_pred
        
        if t_prev > 0:
            noise = torch.randn_like(x_t)
            x_t_prev = mean + sigma_t * noise
        else:
            x_t_prev = mean
        
        return x_t_prev
    
    @staticmethod
    def compare_sampling_methods(model, device='cuda'):
        """æ¯”è¾ƒä¸åŒé‡‡æ ·æ–¹æ³•çš„æ•ˆæœ"""
        results = {}
        
        # æ ‡å‡†DDPMé‡‡æ ·
        print("Testing standard DDPM sampling...")
        start_time = time.time()
        ddpm_samples = standard_ddpm_sample(model, num_samples=16, num_steps=1000)
        ddpm_time = time.time() - start_time
        results['ddpm'] = {'samples': ddpm_samples, 'time': ddpm_time}
        
        # DDIMé‡‡æ ·ï¼ˆ50æ­¥ï¼‰
        print("Testing DDIM sampling (50 steps)...")
        start_time = time.time()
        ddim_samples = ddim_sample(model, num_samples=16, num_steps=50, eta=0)
        ddim_time = time.time() - start_time
        results['ddim'] = {'samples': ddim_samples, 'time': ddim_time}
        
        # å¸¦éšæœºæ€§çš„DDIMé‡‡æ ·
        print("Testing stochastic DDIM (eta=0.5)...")
        start_time = time.time()
        stochastic_samples = ddim_sample(model, num_samples=16, num_steps=50, eta=0.5)
        stochastic_time = time.time() - start_time
        results['stochastic'] = {'samples': stochastic_samples, 'time': stochastic_time}
        
        return results</pre>
        </div>

        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.7ï¼šå®ç°ç®€åŒ–ç‰ˆDDIM</div>
            <p>åŸºäºæœ¬ç« å­¦åˆ°çš„DDPMçŸ¥è¯†ï¼Œå®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„DDIMé‡‡æ ·å™¨ï¼Œæ”¯æŒå¯å˜æ­¥æ•°é‡‡æ ·ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_7')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_7" class="answer">
                <pre>@torch.no_grad()
def simplified_ddim_sample(model, shape, num_inference_steps=50, 
                          num_train_steps=1000, eta=0.0, device='cuda'):
    """ç®€åŒ–ç‰ˆDDIMé‡‡æ ·å®ç°
    
    Args:
        model: è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹æ¨¡å‹
        shape: ç”Ÿæˆå›¾åƒçš„å½¢çŠ¶
        num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆ<= num_train_stepsï¼‰
        num_train_steps: è®­ç»ƒæ—¶ä½¿ç”¨çš„æ€»æ­¥æ•°
        eta: æ§åˆ¶éšæœºæ€§ï¼ˆ0=ç¡®å®šæ€§ï¼Œ1=DDPMï¼‰
    """
    # è®¾ç½®å™ªå£°è°ƒåº¦
    betas = linear_beta_schedule(num_train_steps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    # é€‰æ‹©æ¨ç†æ—¶é—´æ­¥
    step_ratio = num_train_steps // num_inference_steps
    timesteps = torch.arange(0, num_train_steps, step_ratio).flip(0).to(device)
    
    # ä»çº¯å™ªå£°å¼€å§‹
    x_t = torch.randn(shape, device=device)
    
    for i, t in enumerate(timesteps):
        # è·å–å‰ä¸€ä¸ªæ—¶é—´æ­¥
        t_prev = timesteps[i + 1] if i < len(timesteps) - 1 else -1
        
        # é¢„æµ‹å™ªå£°
        t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)
        epsilon_pred = model(x_t, t_batch)
        
        # DDIMæ›´æ–°
        alpha_bar_t = alphas_bar[t]
        alpha_bar_t_prev = alphas_bar[t_prev] if t_prev >= 0 else 1.0
        
        # é¢„æµ‹x_0
        x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
        x_0_pred = torch.clamp(x_0_pred, -1, 1)  # æ•°å€¼ç¨³å®šæ€§
        
        # è®¡ç®—æ–¹å·®
        sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t)) * \
                  torch.sqrt(1 - alpha_bar_t / alpha_bar_t_prev) if t_prev >= 0 else 0
        
        # è®¡ç®—å‡å€¼
        mean = torch.sqrt(alpha_bar_t_prev) * x_0_pred + \
               torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * epsilon_pred
        
        # æ·»åŠ å™ªå£°
        if t_prev >= 0:
            noise = torch.randn_like(x_t) if eta > 0 else 0
            x_t = mean + sigma_t * noise
        else:
            x_t = mean
    
    return x_t

# æµ‹è¯•ä¸åŒæ­¥æ•°å’Œetaå€¼
test_configs = [
    {'steps': 10, 'etas': [0.0, 0.3, 0.7, 1.0]},
    {'steps': 25, 'etas': [0.0, 0.3, 0.7, 1.0]},
    {'steps': 50, 'etas': [0.0, 0.3, 0.7, 1.0]}
]

print("DDIMé‡‡æ ·æµ‹è¯•ç»“æœ:")
print("="*60)
print("æ­¥æ•° | Î·å€¼  | é‡‡æ ·æ—¶é—´(s) | ç›¸å¯¹è´¨é‡è¯„ä¼°")
print("-"*60)

for config in test_configs:
    num_steps = config['steps']
    for eta in config['etas']:
        import time
        start = time.time()
        samples = simplified_ddim_sample(
            model, shape=(1, 1, 28, 28), 
            num_inference_steps=num_steps, 
            eta=eta
        )
        elapsed = time.time() - start
        
        # ç®€å•çš„è´¨é‡è¯„ä¼°ï¼ˆåŸºäºæ ·æœ¬ç»Ÿè®¡ï¼‰
        quality = "é«˜" if samples.std() > 0.3 else ("ä¸­" if samples.std() > 0.2 else "ä½")
        
        print(f"{num_steps:4d} | {eta:4.1f} | {elapsed:11.4f} | {quality}")

print("\nå…³é”®è§‚å¯Ÿ:")
print("- Î·=0 (ç¡®å®šæ€§é‡‡æ ·) é€Ÿåº¦æœ€å¿«ï¼Œè´¨é‡ç¨³å®š")
print("- Î·=1 (å®Œå…¨éšæœºï¼Œç­‰åŒäºDDPM) è´¨é‡æœ€é«˜ä½†é€Ÿåº¦æ…¢")
print("- æ­¥æ•°å‡å°‘æ˜¾è‘—æå‡é€Ÿåº¦ï¼Œä½†å¯èƒ½å½±å“è´¨é‡")</pre>
            </div>
        </div>

        <div class="chapter-summary">
            <h2>æœ¬ç« å°ç»“</h2>
            <p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†DDPMï¼ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼‰çš„æ ¸å¿ƒåŸç†å’Œå®ç°ç»†èŠ‚ï¼š</p>
            
            <h4>ä¸»è¦æ”¶è·</h4>
            <ul>
                <li><strong>ç†è®ºåŸºç¡€</strong>ï¼šç†è§£äº†å‰å‘æ‰©æ•£è¿‡ç¨‹ã€åå‘å»å™ªè¿‡ç¨‹å’Œå˜åˆ†ä¸‹ç•Œçš„æ¨å¯¼</li>
                <li><strong>å®è·µå®ç°</strong>ï¼šæ„å»ºäº†å®Œæ•´çš„DDPMç³»ç»Ÿï¼ŒåŒ…æ‹¬U-Netæ¶æ„ã€è®­ç»ƒå¾ªç¯å’Œé‡‡æ ·ç®—æ³•</li>
                <li><strong>è¯„ä¼°æ–¹æ³•</strong>ï¼šå­¦ä¹ äº†FIDã€ISç­‰ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—å’Œä½¿ç”¨</li>
                <li><strong>å±€é™è®¤è¯†</strong>ï¼šäº†è§£äº†DDPMçš„ä¸»è¦é—®é¢˜ï¼Œä¸ºå­¦ä¹ åç»­æ”¹è¿›æ–¹æ³•æ‰“ä¸‹åŸºç¡€</li>
            </ul>
            
            <h4>å…³é”®è¦ç‚¹</h4>
            <ol>
                <li>DDPMé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å’Œå­¦ä¹ é€†è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®</li>
                <li>è®­ç»ƒç›®æ ‡ç®€åŒ–ä¸ºé¢„æµ‹æ¯ä¸€æ­¥æ·»åŠ çš„å™ªå£°</li>
                <li>é‡‡æ ·è¿‡ç¨‹éœ€è¦å¤šæ­¥è¿­ä»£ï¼Œè¿™æ˜¯ä¸»è¦çš„æ•ˆç‡ç“¶é¢ˆ</li>
                <li>æ¨¡å‹è´¨é‡é«˜ä½†æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ¨åŠ¨äº†åç»­çš„ä¼—å¤šæ”¹è¿›</li>
            </ol>
            
            <h4>å±•æœ›</h4>
            <p>åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ï¼š</p>
            <ul>
                <li><strong>ç¬¬4ç« </strong>ï¼šä»åˆ†æ•°åŒ¹é…çš„è§’åº¦é‡æ–°ç†è§£æ‰©æ•£æ¨¡å‹</li>
                <li><strong>ç¬¬5ç« </strong>ï¼šè¿ç»­æ—¶é—´æ¡†æ¶ä¸‹çš„SDE/ODEè¡¨è¿°</li>
                <li><strong>ç¬¬8ç« </strong>ï¼šDDIMç­‰å¿«é€Ÿé‡‡æ ·æ–¹æ³•çš„åŸç†ä¸å®ç°</li>
            </ul>
            
            <p>DDPMå¥ å®šäº†ç°ä»£æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œç†è§£å®ƒçš„åŸç†å¯¹äºæŒæ¡åç»­çš„é«˜çº§æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç»§ç»­å‰è¿›ï¼Œè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€ç« æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„å¦ä¸€ç§è§†è§’â€”â€”åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹ï¼</p>
        </div>
    </div>
</body>
</html>