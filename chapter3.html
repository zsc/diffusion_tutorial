<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬3ç« ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) - Diffusion Models Tutorial</title>
    <link rel="stylesheet" href="common.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script src="common.js"></script>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="chapter2.html">â† ä¸Šä¸€ç« </a>
            <span>ç¬¬3ç«  / å…±14ç« </span>
            <a href="chapter4.html">ä¸‹ä¸€ç«  â†’</a>
        </div>
        
        <h1>ç¬¬3ç« ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM)</h1>
        
        <div class="chapter-intro">
            2020å¹´ï¼ŒHoç­‰äººçš„è®ºæ–‡"Denoising Diffusion Probabilistic Models"è®©æ‰©æ•£æ¨¡å‹çœŸæ­£è¿›å…¥äº†å®ç”¨é˜¶æ®µã€‚DDPMä¸ä»…ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜è¾¾åˆ°äº†ä¸GANç›¸åª²ç¾çš„ç”Ÿæˆè´¨é‡ã€‚æœ¬ç« å°†æ·±å…¥å‰–æDDPMçš„æ•°å­¦åŸç†ã€è®­ç»ƒç®—æ³•å’Œå®ç°ç»†èŠ‚ã€‚é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†æŒæ¡å¦‚ä½•ä»é›¶å®ç°ä¸€ä¸ªå®Œæ•´çš„DDPMï¼Œå¹¶ç†è§£å…¶èƒŒåçš„æ¦‚ç‡è®ºåŸºç¡€ã€‚
        </div>
        
        <h2>3.1 DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼šç®€åŒ–ä¸ç»Ÿä¸€</h2>
        
        <p>åœ¨DDPMä¹‹å‰ï¼Œæ‰©æ•£æ¨¡å‹è™½ç„¶ç†è®ºä¼˜é›…ï¼Œä½†å®è·µå›°éš¾ã€‚2015å¹´Sohl-Dicksteinç­‰äººçš„å¼€åˆ›æ€§å·¥ä½œéœ€è¦ä¼°è®¡æ•´ä¸ªåå‘è¿‡ç¨‹çš„ç†µï¼Œè®­ç»ƒæå…¶å¤æ‚ã€‚DDPMçš„é©å‘½æ€§è´¡çŒ®åœ¨äºï¼š<strong>å°†å¤æ‚çš„å˜åˆ†æ¨æ–­ç®€åŒ–ä¸ºç®€å•çš„å»å™ªä»»åŠ¡</strong>ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">DDPMçš„ä¸‰ä¸ªå…³é”®ç®€åŒ–</div>
            <ol>
                <li><strong>å›ºå®šæ–¹å·®è°ƒåº¦</strong>ï¼šå‰å‘è¿‡ç¨‹ä½¿ç”¨é¢„å®šä¹‰çš„ $\beta_t$ åºåˆ—ï¼Œæ— éœ€å­¦ä¹ </li>
                <li><strong>ç®€åŒ–åå‘è¿‡ç¨‹</strong>ï¼šå‡è®¾åå‘è¿‡ç¨‹ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œåªéœ€å­¦ä¹ å‡å€¼ï¼ˆå®é™…ä¸Šæ˜¯å­¦ä¹ å™ªå£°ï¼‰</li>
                <li><strong>é‡å‚æ•°åŒ–ç›®æ ‡</strong>ï¼šå°†é¢„æµ‹å‡å€¼è½¬æ¢ä¸ºé¢„æµ‹å™ªå£°ï¼Œå¤§å¹…æå‡è®­ç»ƒç¨³å®šæ€§</li>
            </ol>
        </div>
        
        <h3>3.1.1 ä»å¤æ‚åˆ°ç®€å•ï¼šDDPMçš„æ´å¯Ÿ</h3>
        
        <p>è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç±»æ¯”æ¥ç†è§£DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>å¢¨æ°´æ‰©æ•£çš„ç±»æ¯”</h4>
            <p>æƒ³è±¡ä¸€æ»´å¢¨æ°´åœ¨æ°´ä¸­æ‰©æ•£ï¼š</p>
            <ul>
                <li><strong>å‰å‘è¿‡ç¨‹</strong>ï¼šå¢¨æ°´é€æ¸æ‰©æ•£ï¼Œæœ€ç»ˆå‡åŒ€åˆ†å¸ƒï¼ˆç‰©ç†è¿‡ç¨‹ï¼Œç¡®å®šçš„ï¼‰</li>
                <li><strong>åå‘è¿‡ç¨‹</strong>ï¼šå¦‚ä½•è®©æ‰©æ•£çš„å¢¨æ°´é‡æ–°èšé›†ï¼Ÿï¼ˆéœ€è¦å­¦ä¹ çš„ï¼‰</li>
            </ul>
            <p>DDPMçš„å…³é”®æ´å¯Ÿï¼š<strong>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“"å¢¨æ°´åº”è¯¥å‘å“ªä¸ªæ–¹å‘èšé›†"</strong>ï¼Œè€Œè¿™ä¸ªæ–¹å‘æ°å¥½ä¸æ·»åŠ çš„å™ªå£°æ–¹å‘ç›¸åï¼</p>
        </div>
        
        <h3>3.1.2 æ•°å­¦æ¡†æ¶æ¦‚è§ˆ</h3>
        
        <p>DDPMå®šä¹‰äº†ä¸¤ä¸ªè¿‡ç¨‹ï¼š</p>
        
        <div class="math-block">
            <strong>å‰å‘è¿‡ç¨‹ï¼ˆå›ºå®šï¼‰</strong>ï¼š<br>
            $q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$<br><br>
            
            <strong>åå‘è¿‡ç¨‹ï¼ˆå­¦ä¹ ï¼‰</strong>ï¼š<br>
            $p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2\mathbf{I})$
        </div>
        
        <p>å…³é”®åˆ›æ–°åœ¨äºå¦‚ä½•å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ï¼š</p>
        
        <div class="code-block">
<pre># æ—©æœŸæ–¹æ³•ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆä¸ç¨³å®šï¼‰
mean = model(x_t, t)

# DDPMåˆ›æ–°ï¼šé¢„æµ‹å™ªå£°ï¼ˆç¨³å®šä¸”æœ‰æ•ˆï¼‰
noise_pred = model(x_t, t)
mean = (x_t - beta_t / sqrt(1 - alpha_bar_t) * noise_pred) / sqrt(alpha_t)</pre>
        </div>
        
        <h3>3.1.3 ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿ</h3>
        
        <p>è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„æ”¹å˜å¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼š</p>
        
        <div class="definition">
            <div class="definition-title">é¢„æµ‹å™ªå£°çš„ä¼˜åŠ¿</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ–¹é¢</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">é¢„æµ‹å‡å€¼</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">é¢„æµ‹å™ªå£°</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¾“å‡ºèŒƒå›´</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">éœ€è¦åŒ¹é…æ•°æ®åˆ†å¸ƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ ‡å‡†é«˜æ–¯ï¼ˆå·²å½’ä¸€åŒ–ï¼‰</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">è®­ç»ƒä¿¡å·</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">éštå˜åŒ–å‰§çƒˆ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å„æ—¶é—´æ­¥ç›¸å¯¹ä¸€è‡´</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ¢¯åº¦æµ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å¯èƒ½æ¢¯åº¦æ¶ˆå¤±</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ¢¯åº¦ä¼ æ’­è‰¯å¥½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç‰©ç†æ„ä¹‰</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹å»å™ªåçš„å›¾åƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹æ·»åŠ çš„å™ªå£°</td>
                </tr>
            </table>
        </div>
        
        <h3>3.1.4 DDPM vs æ—©æœŸæ‰©æ•£æ¨¡å‹</h3>
        
        <p>è®©æˆ‘ä»¬å¯¹æ¯”DDPMä¸2015å¹´çš„åŸå§‹æ‰©æ•£æ¨¡å‹ï¼š</p>
        
        <div class="code-block">
<pre># 2015å¹´çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¤æ‚ï¼‰
# éœ€è¦ä¼°è®¡ï¼š
# 1. å‰å‘è¿‡ç¨‹çš„ç†µ
# 2. åå‘è¿‡ç¨‹çš„å®Œæ•´åˆ†å¸ƒ
# 3. å˜åˆ†å‚æ•°çš„ä¼˜åŒ–
# è®­ç»ƒæå…¶ä¸ç¨³å®šï¼Œç”Ÿæˆè´¨é‡å·®

# DDPMï¼ˆ2020å¹´ï¼‰çš„è®­ç»ƒï¼ˆæç®€ï¼‰
for x_0, _ in dataloader:
    t = torch.randint(0, num_timesteps, (batch_size,))
    noise = torch.randn_like(x_0)
    x_t = sqrt_alpha_bar[t] * x_0 + sqrt_one_minus_alpha_bar[t] * noise
    
    noise_pred = model(x_t, t)
    loss = F.mse_loss(noise_pred, noise)
    loss.backward()</pre>
        </div>
        
        <p>è¿™ç§ç®€åŒ–ä¸æ˜¯ä»¥ç‰ºç‰²æ€§èƒ½ä¸ºä»£ä»·çš„â€”â€”ç›¸åï¼ŒDDPMé¦–æ¬¡è®©æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸GANç«äº‰ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚</p>
        
        <div class="exercise">
            <div class="exercise-title">æ€è€ƒé¢˜ 3.1ï¼šç›´è§‰ç†è§£</div>
            <p>ä¸ºä»€ä¹ˆåœ¨é«˜å™ªå£°æƒ…å†µä¸‹ï¼ˆå¤§çš„tï¼‰ï¼Œé¢„æµ‹å™ªå£°æ¯”é¢„æµ‹åŸå§‹å›¾åƒæ›´å®¹æ˜“ï¼Ÿæç¤ºï¼šè€ƒè™‘ä¿¡å™ªæ¯”ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_1" class="answer">
                <p><strong>ç­”æ¡ˆï¼š</strong></p>
                <p>å½“tå¾ˆå¤§æ—¶ï¼Œ$\mathbf{x}_t \approx \mathcal{N}(0, \mathbf{I})$ï¼Œå‡ ä¹æ˜¯çº¯å™ªå£°ã€‚æ­¤æ—¶ï¼š</p>
                <ul>
                    <li>åŸå§‹å›¾åƒ $\mathbf{x}_0$ çš„ä¿¡æ¯å‡ ä¹å®Œå…¨ä¸¢å¤±ï¼Œé¢„æµ‹å®ƒéœ€è¦"å‡­ç©ºæƒ³è±¡"</li>
                    <li>ä½†æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ æ˜¯å·²çŸ¥çš„ï¼Œä¸”å ä¸»å¯¼åœ°ä½</li>
                    <li>ç½‘ç»œåªéœ€è¦è¯†åˆ«å™ªå£°æ¨¡å¼ï¼Œè€Œä¸æ˜¯é‡å»ºå¤æ‚çš„å›¾åƒç»“æ„</li>
                </ul>
                <p>ç±»æ¯”ï¼šåœ¨é›ªèŠ±å™ªå£°çš„ç”µè§†å±å¹•ä¸Šï¼Œè¯†åˆ«å™ªå£°æ¨¡å¼æ¯”é‡å»ºåŸå§‹èŠ‚ç›®å®¹æ˜“å¾—å¤šã€‚</p>
            </div>
        </div>
        
        <h2>3.2 å‰å‘è¿‡ç¨‹ï¼šæ•°å­¦æ¨å¯¼ä¸æ€§è´¨</h2>
        
        <p>å‰å‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œå®ƒå®šä¹‰äº†å¦‚ä½•å°†æ•°æ®é€æ­¥è½¬æ¢ä¸ºå™ªå£°ã€‚è™½ç„¶è¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä¸éœ€è¦å®é™…æ‰§è¡Œå®Œæ•´çš„é©¬å°”å¯å¤«é“¾ï¼Œä½†ç†è§£å…¶æ•°å­¦æ€§è´¨å¯¹æŒæ¡DDPMè‡³å…³é‡è¦ã€‚</p>
        
        <h3>3.2.1 é©¬å°”å¯å¤«é“¾çš„æ„å»º</h3>
        
        <p>å‰å‘è¿‡ç¨‹å®šä¹‰ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼š</p>
        
        <div class="math-block">
            $$\mathbf{x}_0 \to \mathbf{x}_1 \to \mathbf{x}_2 \to \cdots \to \mathbf{x}_T$$
        </div>
        
        <p>å…¶ä¸­æ¯ä¸€æ­¥çš„è½¬ç§»æ¦‚ç‡ä¸ºï¼š</p>
        
        <div class="math-block">
            $$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$$
        </div>
        
        <div class="definition">
            <div class="definition-title">å…³é”®æ€§è´¨1ï¼šæ–¹å·®è°ƒåº¦çš„çº¦æŸ</div>
            <p>ä¸ºä»€ä¹ˆæ˜¯ $\sqrt{1-\beta_t}$ è€Œä¸æ˜¯å…¶ä»–ç³»æ•°ï¼Ÿè¿™æ˜¯ä¸ºäº†ä¿æŒä¿¡å·çš„æœŸæœ›èƒ½é‡ï¼š</p>
            <div class="math-block">
                $$\mathbb{E}[\|\mathbf{x}_t\|^2 | \mathbf{x}_{t-1}] = (1-\beta_t)\|\mathbf{x}_{t-1}\|^2 + \beta_t \cdot d$$
            </div>
            <p>å…¶ä¸­ $d$ æ˜¯æ•°æ®ç»´åº¦ã€‚å½“ $\beta_t$ å¾ˆå°æ—¶ï¼Œä¿¡å·èƒ½é‡è¿‘ä¼¼ä¿æŒä¸å˜ã€‚</p>
        </div>
        
        <p>è®©æˆ‘ä»¬éªŒè¯è¿™ä¸ªæ€§è´¨ï¼š</p>
        
        <div class="code-block">
<pre>import torch
import matplotlib.pyplot as plt

# éªŒè¯èƒ½é‡ä¿æŒæ€§è´¨
x_0 = torch.randn(1000, 3, 32, 32)  # 1000ä¸ª32x32çš„RGBå›¾åƒ
beta = 0.02  # å…¸å‹çš„betaå€¼

# ä¸€æ­¥å‰å‘è¿‡ç¨‹
noise = torch.randn_like(x_0)
x_1 = torch.sqrt(1 - beta) * x_0 + torch.sqrt(beta) * noise

print(f"åŸå§‹ä¿¡å·èƒ½é‡: {x_0.pow(2).mean():.4f}")
print(f"æ‰©æ•£åä¿¡å·èƒ½é‡: {x_1.pow(2).mean():.4f}")
print(f"ç†è®ºé¢„æœŸ: {(1-beta)*x_0.pow(2).mean() + beta*3*32*32:.4f}")</pre>
        </div>
        
        <h3>3.2.2 é‡å‚æ•°åŒ–æŠ€å·§</h3>
        
        <p>DDPMçš„ä¸€ä¸ªå…³é”®æŠ€å·§æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥ä» $\mathbf{x}_0$ é‡‡æ ·ä»»æ„æ—¶åˆ»çš„ $\mathbf{x}_t$ï¼Œè€Œä¸éœ€è¦é€æ­¥æ¨¡æ‹Ÿæ•´ä¸ªé©¬å°”å¯å¤«é“¾ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">å®šç†ï¼šé—­å¼é‡‡æ ·å…¬å¼</div>
            <p>å®šä¹‰ $\alpha_t = 1 - \beta_t$ å’Œ $\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s$ï¼Œåˆ™ï¼š</p>
            <div class="math-block">
                $$q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$$
            </div>
        </div>
        
        <p><strong>è¯æ˜</strong>ï¼ˆè¿™ä¸ªè¯æ˜å¾ˆé‡è¦ï¼Œå€¼å¾—ä»”ç»†ç†è§£ï¼‰ï¼š</p>
        
        <div class="math-block">
            <p>æˆ‘ä»¬ç”¨å½’çº³æ³•è¯æ˜ã€‚</p>
            <p><strong>åŸºç¡€æƒ…å†µ</strong>ï¼ˆ$t=1$ï¼‰ï¼šæ˜¾ç„¶æˆç«‹ï¼Œå› ä¸º $\bar{\alpha}_1 = \alpha_1 = 1 - \beta_1$ã€‚</p>
            
            <p><strong>å½’çº³æ­¥éª¤</strong>ï¼šå‡è®¾å¯¹ $t-1$ æˆç«‹ï¼Œå³ï¼š</p>
            $$\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}$$
            
            <p>å…¶ä¸­ $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ã€‚æ ¹æ®å‰å‘è¿‡ç¨‹å®šä¹‰ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            <p>ä»£å…¥ $\mathbf{x}_{t-1}$ çš„è¡¨è¾¾å¼ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\alpha_t}(\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}) + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            $$= \sqrt{\alpha_t\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$
            
            <p>æ³¨æ„åˆ° $\alpha_t\bar{\alpha}_{t-1} = \bar{\alpha}_t$ï¼Œä¸”ä¸¤ä¸ªç‹¬ç«‹é«˜æ–¯å™ªå£°çš„çº¿æ€§ç»„åˆä»æ˜¯é«˜æ–¯å™ªå£°ï¼š</p>
            $$\text{Var}[\sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t] = \alpha_t(1-\bar{\alpha}_{t-1}) + (1-\alpha_t) = 1-\bar{\alpha}_t$$
            
            <p>å› æ­¤ï¼š</p>
            $$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$$
            
            <p>å…¶ä¸­ $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ã€‚è¯æ¯•ã€‚</p>
        </div>
        
        <h3>3.2.3 å™ªå£°è°ƒåº¦çš„è®¾è®¡</h3>
        
        <p>å™ªå£°è°ƒåº¦ $\{\beta_t\}_{t=1}^T$ çš„é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚DDPMåŸæ–‡ä½¿ç”¨çº¿æ€§è°ƒåº¦ï¼Œä½†åç»­ç ”ç©¶å‘ç°å…¶ä»–è°ƒåº¦å¯èƒ½æ›´ä¼˜ã€‚</p>
        
        <div class="code-block">
<pre>import numpy as np
import matplotlib.pyplot as plt

def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """DDPMåŸå§‹çš„çº¿æ€§è°ƒåº¦"""
    return np.linspace(beta_start, beta_end, timesteps)

def cosine_beta_schedule(timesteps, s=0.008):
    """Improved DDPMçš„ä½™å¼¦è°ƒåº¦"""
    steps = timesteps + 1
    t = np.linspace(0, timesteps, steps)
    alphas_cumprod = np.cos(((t / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return np.clip(betas, 0.0001, 0.9999)

def quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """äºŒæ¬¡è°ƒåº¦ï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰"""
    t = np.linspace(0, 1, timesteps)
    return beta_start + (beta_end - beta_start) * t ** 2

# å¯è§†åŒ–ä¸åŒè°ƒåº¦
timesteps = 1000
linear_betas = linear_beta_schedule(timesteps)
cosine_betas = cosine_beta_schedule(timesteps)
quadratic_betas = quadratic_beta_schedule(timesteps)

# è®¡ç®—ä¿¡å™ªæ¯”ï¼ˆæ›´ç›´è§‚çš„æŒ‡æ ‡ï¼‰
def compute_snr(betas):
    alphas = 1 - betas
    alphas_cumprod = np.cumprod(alphas)
    return alphas_cumprod / (1 - alphas_cumprod)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(linear_betas, label='Linear')
plt.plot(cosine_betas, label='Cosine')
plt.plot(quadratic_betas, label='Quadratic')
plt.xlabel('Timestep')
plt.ylabel('Î²_t')
plt.title('Beta Schedules')
plt.legend()

plt.subplot(1, 3, 2)
plt.semilogy(compute_snr(linear_betas), label='Linear')
plt.semilogy(compute_snr(cosine_betas), label='Cosine')
plt.semilogy(compute_snr(quadratic_betas), label='Quadratic')
plt.xlabel('Timestep')
plt.ylabel('SNR (log scale)')
plt.title('Signal-to-Noise Ratio')
plt.legend()

plt.subplot(1, 3, 3)
# å±•ç¤ºä¸åŒè°ƒåº¦ä¸‹çš„æ ·æœ¬
alphas_cumprod_linear = np.cumprod(1 - linear_betas)
alphas_cumprod_cosine = np.cumprod(1 - cosine_betas)

t_vis = [0, 250, 500, 750, 999]
for i, t in enumerate(t_vis):
    plt.scatter(i, alphas_cumprod_linear[t], color='blue', s=100)
    plt.scatter(i, alphas_cumprod_cosine[t], color='red', s=100)
    
plt.xlabel('Visualization Step')
plt.ylabel('âˆš(á¾±_t)')
plt.title('Signal Preservation at Key Steps')
plt.legend(['Linear', 'Cosine'])
plt.tight_layout()
plt.show()</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">è°ƒåº¦ç­–ç•¥å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">è°ƒåº¦ç±»å‹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç‰¹ç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä¼˜åŠ¿</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">åŠ£åŠ¿</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">çº¿æ€§ (Linear)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">Î²çº¿æ€§å¢é•¿</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€å•ç›´è§‚</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‰æœŸç ´åè¿‡å¿«</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½™å¼¦ (Cosine)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åŸºäºSNRè®¾è®¡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æœ«æœŸå¯èƒ½è¿‡æ…¢</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">äºŒæ¬¡ (Quadratic)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">Î²äºŒæ¬¡å¢é•¿</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‰æœŸä¿ç•™æ›´å¤šä¿¡æ¯</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åæœŸå¯èƒ½å¤ªæ¿€è¿›</td>
                </tr>
            </table>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.2ï¼šå®ç°è‡ªå®šä¹‰å™ªå£°è°ƒåº¦</div>
            <p>è®¾è®¡ä¸€ä¸ª"Så½¢"å™ªå£°è°ƒåº¦ï¼Œä½¿å¾—ï¼š</p>
            <ol>
                <li>å‰æœŸï¼ˆt < 200ï¼‰ï¼šç¼“æ…¢æ·»åŠ å™ªå£°ï¼Œä¿ç•™æ›´å¤šç»“æ„ä¿¡æ¯</li>
                <li>ä¸­æœŸï¼ˆ200 â‰¤ t â‰¤ 800ï¼‰ï¼šå¿«é€Ÿæ·»åŠ å™ªå£°</li>
                <li>åæœŸï¼ˆt > 800ï¼‰ï¼šå†æ¬¡æ”¾ç¼“ï¼Œç¡®ä¿æ”¶æ•›åˆ°çº¯å™ªå£°</li>
            </ol>
            <p>å®ç°è¿™ä¸ªè°ƒåº¦å¹¶ä¸æ ‡å‡†è°ƒåº¦å¯¹æ¯”SNRæ›²çº¿ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_2')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_2" class="answer">
                <pre>def sigmoid_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """Så½¢å™ªå£°è°ƒåº¦"""
    t = np.linspace(-6, 6, timesteps)
    sigmoid = 1 / (1 + np.exp(-t))
    betas = beta_start + (beta_end - beta_start) * sigmoid
    return betas

# ä¹Ÿå¯ä»¥åˆ†æ®µè®¾è®¡
def piecewise_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
    """åˆ†æ®µå™ªå£°è°ƒåº¦"""
    betas = np.zeros(timesteps)
    
    # å‰æœŸï¼šç¼“æ…¢å¢é•¿
    t1 = int(0.2 * timesteps)
    betas[:t1] = np.linspace(beta_start, beta_start * 5, t1)
    
    # ä¸­æœŸï¼šå¿«é€Ÿå¢é•¿
    t2 = int(0.8 * timesteps)
    betas[t1:t2] = np.linspace(beta_start * 5, beta_end * 0.8, t2 - t1)
    
    # åæœŸï¼šç¼“æ…¢å¢é•¿åˆ°beta_end
    betas[t2:] = np.linspace(beta_end * 0.8, beta_end, timesteps - t2)
    
    return betas</pre>
                <p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼šå¥½çš„å™ªå£°è°ƒåº¦åº”è¯¥åœ¨ä¿ç•™è¶³å¤Ÿä¿¡æ¯å’Œå……åˆ†æ¢ç´¢å™ªå£°ç©ºé—´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä½™å¼¦è°ƒåº¦ä¹‹æ‰€ä»¥ä¼˜äºçº¿æ€§è°ƒåº¦ï¼Œæ­£æ˜¯å› ä¸ºå®ƒæ›´å¥½åœ°å¹³è¡¡äº†è¿™ä¸¤ä¸ªéœ€æ±‚ã€‚</p>
            </div>
        </div>
        
        <h2>3.3 åå‘è¿‡ç¨‹ï¼šä»å™ªå£°åˆ°å›¾åƒ</h2>
        
        <p>åå‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒâ€”â€”å¦‚ä½•ä»çº¯å™ªå£°é€æ­¥æ¢å¤å‡ºæ¸…æ™°çš„æ•°æ®ã€‚DDPMçš„å…³é”®è´¡çŒ®ä¹‹ä¸€æ˜¯æ¨å¯¼å‡ºäº†åœ¨å·²çŸ¥ $\mathbf{x}_0$ æ—¶çš„åå‘æ¡ä»¶åˆ†å¸ƒçš„é—­å¼è§£ã€‚</p>
        
        <h3>3.3.1 åå‘æ¡ä»¶æ¦‚ç‡çš„æ¨å¯¼</h3>
        
        <p>è¿™æ˜¯DDPMä¸­æœ€é‡è¦çš„æ•°å­¦æ¨å¯¼ä¹‹ä¸€ã€‚æˆ‘ä»¬æƒ³è¦è®¡ç®— $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">å®šç†ï¼šåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒ</div>
            <p>ç»™å®š $\mathbf{x}_t$ å’Œ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒä¸ºï¼š</p>
            <div class="math-block">
                $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathbf{I})$$
            </div>
            <p>å…¶ä¸­ï¼š</p>
            <div class="math-block">
                $$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t$$
                
                $$\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$
            </div>
        </div>
        
        <p><strong>è¯æ˜</strong>ï¼šä½¿ç”¨è´å¶æ–¯å®šç†ï¼š</p>
        
        <div class="math-block">
            $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}$$
        </div>
        
        <p>ç”±äºå‰å‘è¿‡ç¨‹çš„é©¬å°”å¯å¤«æ€§è´¨ï¼Œ$q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t|\mathbf{x}_{t-1})$ã€‚ç°åœ¨æˆ‘ä»¬çŸ¥é“ï¼š</p>
        
        <ul>
            <li>$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$</li>
            <li>$q(\mathbf{x}_{t-1}|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0, (1-\bar{\alpha}_{t-1})\mathbf{I})$</li>
            <li>$q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$</li>
        </ul>
        
        <p>å°†ä¸‰ä¸ªé«˜æ–¯åˆ†å¸ƒä»£å…¥è´å¶æ–¯å…¬å¼ï¼Œç»è¿‡ç¹çä½†ç›´æ¥çš„ä»£æ•°è¿ç®—ï¼ˆä¸»è¦æ˜¯é…æ–¹ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ä¸Šè¿°ç»“æœã€‚</p>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>ğŸ’¡ å…³é”®æ´å¯Ÿ</h4>
            <p>æ³¨æ„ $\tilde{\boldsymbol{\mu}}_t$ æ˜¯ $\mathbf{x}_0$ å’Œ $\mathbf{x}_t$ çš„<strong>çº¿æ€§ç»„åˆ</strong>ï¼è¿™æ„å‘³ç€ï¼š</p>
            <ul>
                <li>å¦‚æœæˆ‘ä»¬çŸ¥é“ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹å°±æ˜¯ç¡®å®šçš„ï¼ˆé™¤äº†å°çš„é«˜æ–¯å™ªå£°ï¼‰</li>
                <li>å®è·µä¸­æˆ‘ä»¬ä¸çŸ¥é“ $\mathbf{x}_0$ï¼Œæ‰€ä»¥éœ€è¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å®ƒ</li>
                <li>è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯åœ¨å­¦ä¹ "å»å™ª"</li>
            </ul>
        </div>
        
        <h3>3.3.2 å‚æ•°åŒ–é€‰æ‹©ï¼šé¢„æµ‹å™ªå£° vs é¢„æµ‹å‡å€¼</h3>
        
        <p>æ—¢ç„¶ $\tilde{\boldsymbol{\mu}}_t$ ä¾èµ–äºæœªçŸ¥çš„ $\mathbf{x}_0$ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼å®ƒã€‚DDPMæä¾›äº†å‡ ç§å‚æ•°åŒ–æ–¹å¼ï¼š</p>
        
        <div class="code-block">
<pre># æ–¹å¼1ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆæœ€ç›´æ¥ä½†ä¸ç¨³å®šï¼‰
mu_theta = model(x_t, t)

# æ–¹å¼2ï¼šé¢„æµ‹x_0ï¼ˆéœ€è¦clipåˆ°åˆç†èŒƒå›´ï¼‰
x_0_pred = model(x_t, t)
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred + 
            sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé¢„æµ‹å™ªå£°ï¼ˆDDPMçš„é€‰æ‹©ï¼Œæœ€ç¨³å®šï¼‰
epsilon_pred = model(x_t, t)
x_0_pred = (x_t - sqrt_one_minus_alpha_bar_t * epsilon_pred) / sqrt_alpha_bar_t
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred + 
            sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)</pre>
        </div>
        
        <p>ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡é‡å‚æ•°åŒ–æ¥ç†è§£ï¼š</p>
        
        <div class="math-block">
            <p>ç”±äº $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºï¼š</p>
            $$\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}}$$
            
            <p>ä»£å…¥ $\tilde{\boldsymbol{\mu}}_t$ çš„è¡¨è¾¾å¼ï¼Œç»è¿‡åŒ–ç®€å¯å¾—ï¼š</p>
            $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$
        </div>
        
        <p>è¿™ä¸ªè¡¨è¾¾å¼æ­ç¤ºäº†ä¸€ä¸ªä¼˜é›…çš„äº‹å®ï¼š<strong>åå‘è¿‡ç¨‹çš„å‡å€¼åªéœ€è¦çŸ¥é“æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ï¼</strong></p>
        
        <div class="definition">
            <div class="definition-title">ä¸‰ç§å‚æ•°åŒ–çš„å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">å‚æ•°åŒ–</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä¼˜ç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç¼ºç‚¹</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ä½¿ç”¨åœºæ™¯</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\boldsymbol{\mu}_\theta$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç›´æ¥ï¼Œæ— éœ€è½¬æ¢</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä¸åŒtçš„è¾“å‡ºå°ºåº¦å·®å¼‚å¤§</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å‡ ä¹ä¸ç”¨</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\mathbf{x}_0$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¯­ä¹‰æ¸…æ™°</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é«˜å™ªå£°æ—¶é¢„æµ‹å›°éš¾</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æŸäº›æ¡ä»¶ç”Ÿæˆä»»åŠ¡</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¢„æµ‹ $\boldsymbol{\epsilon}$</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">è¾“å‡ºæ ‡å‡†åŒ–ï¼Œè®­ç»ƒç¨³å®š</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é—´æ¥ï¼Œéœ€è¦è½¬æ¢</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ ‡å‡†é€‰æ‹©</td>
                </tr>
            </table>
        </div>
        
        <h3>3.3.3 æ–¹å·®çš„å¤„ç†ï¼šå›ºå®š vs å¯å­¦ä¹ </h3>
        
        <p>DDPMçš„å¦ä¸€ä¸ªç®€åŒ–æ˜¯ä½¿ç”¨å›ºå®šçš„æ–¹å·® $\tilde{\beta}_t$ã€‚ä½†è¿™æ˜¯æœ€ä¼˜çš„å—ï¼Ÿ</p>
        
        <div class="code-block">
<pre># DDPMï¼šå›ºå®šæ–¹å·®ï¼ˆä¸¤ç§é€‰æ‹©ï¼‰
# é€‰æ‹©1ï¼šä½¿ç”¨åéªŒæ–¹å·®
variance = (1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t

# é€‰æ‹©2ï¼šä½¿ç”¨Î²_tï¼ˆDDPMè®ºæ–‡çš„é€‰æ‹©ï¼‰
variance = beta_t

# æ”¹è¿›çš„DDPMï¼šå­¦ä¹ æ–¹å·®
# ç½‘ç»œåŒæ—¶é¢„æµ‹å™ªå£°å’Œæ–¹å·®
epsilon_pred, v_pred = model(x_t, t).chunk(2, dim=1)

# å‚æ•°åŒ–æ–¹å·®ï¼ˆåœ¨å¯¹æ•°ç©ºé—´æ’å€¼ï¼‰
min_log = torch.log(beta_t)
max_log = torch.log((1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t)
log_variance = v_pred * max_log + (1 - v_pred) * min_log
variance = torch.exp(log_variance)</pre>
        </div>
        
        <div class="visualization" style="background-color: #fff3cd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>âš ï¸ å®è·µç»éªŒ</h4>
            <p>å°½ç®¡å­¦ä¹ æ–¹å·®ç†è®ºä¸Šæ›´ä¼˜ï¼ˆå¯ä»¥è·å¾—æ›´å¥½çš„ä¼¼ç„¶ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼š</p>
            <ul>
                <li>å›ºå®šæ–¹å·®çš„DDPMå·²ç»èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒ</li>
                <li>å­¦ä¹ æ–¹å·®å¢åŠ äº†è®­ç»ƒçš„å¤æ‚åº¦</li>
                <li>å¯¹äºå¤§å¤šæ•°åº”ç”¨ï¼Œå›ºå®šæ–¹å·®æ˜¯è¶³å¤Ÿçš„</li>
                <li>å¦‚æœè¿½æ±‚æœ€ä¼˜ä¼¼ç„¶ï¼ˆå¦‚å‹ç¼©ä»»åŠ¡ï¼‰ï¼Œæ‰è€ƒè™‘å­¦ä¹ æ–¹å·®</li>
            </ul>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.3ï¼šéªŒè¯ä¸åŒå‚æ•°åŒ–çš„ç­‰ä»·æ€§</div>
            <p>å®ç°ä¸‰ç§å‚æ•°åŒ–æ–¹å¼ï¼ŒéªŒè¯å®ƒä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ï¼š</p>
            <ol>
                <li>ç»™å®šç›¸åŒçš„ $\mathbf{x}_t$ã€$\mathbf{x}_0$ å’Œ $t$</li>
                <li>è®¡ç®—çœŸå®çš„å™ªå£° $\boldsymbol{\epsilon}$</li>
                <li>ç”¨ä¸‰ç§æ–¹å¼è®¡ç®— $\tilde{\boldsymbol{\mu}}_t$</li>
                <li>éªŒè¯ç»“æœç›¸åŒï¼ˆåœ¨æ•°å€¼ç²¾åº¦å†…ï¼‰</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_3')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_3" class="answer">
                <pre>import torch

# è®¾ç½®
batch_size = 4
channels = 3
size = 32
t = 500
T = 1000

# åˆå§‹åŒ–
x_0 = torch.randn(batch_size, channels, size, size)
epsilon = torch.randn_like(x_0)

# è®¡ç®—alphaç›¸å…³å€¼
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alphas_bar = torch.cumprod(alphas, dim=0)
alpha_t = alphas[t]
alpha_bar_t = alphas_bar[t]
alpha_bar_prev = alphas_bar[t-1]
beta_t = betas[t]

# å‰å‘è¿‡ç¨‹
x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * epsilon

# æ–¹å¼1ï¼šç›´æ¥è®¡ç®—çœŸå®çš„åéªŒå‡å€¼
mu_true = (torch.sqrt(alpha_bar_prev) * beta_t * x_0 + 
           torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼2ï¼šé€šè¿‡é¢„æµ‹x_0
x_0_pred = x_0  # å‡è®¾å®Œç¾é¢„æµ‹
mu_x0 = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_pred + 
         torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé€šè¿‡é¢„æµ‹å™ªå£°
epsilon_pred = epsilon  # å‡è®¾å®Œç¾é¢„æµ‹
x_0_from_eps = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
mu_eps = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_from_eps + 
          torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æˆ–è€…ç›´æ¥ç”¨ç®€åŒ–å…¬å¼
mu_eps_direct = (x_t - beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_t)

# éªŒè¯
print(f"æ–¹å¼1å’Œæ–¹å¼2çš„å·®å¼‚: {(mu_true - mu_x0).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3çš„å·®å¼‚: {(mu_true - mu_eps).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3(ç›´æ¥)çš„å·®å¼‚: {(mu_true - mu_eps_direct).abs().max():.6f}")

# è¾“å‡ºåº”è¯¥éƒ½æ¥è¿‘0ï¼ˆåœ¨æµ®ç‚¹ç²¾åº¦èŒƒå›´å†…ï¼‰</pre>
                <p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼šä¸‰ç§å‚æ•°åŒ–åœ¨æ•°å­¦ä¸Šç­‰ä»·ï¼Œä½†è®­ç»ƒåŠ¨æ€ä¸åŒã€‚é¢„æµ‹å™ªå£°ä¹‹æ‰€ä»¥æ›´ç¨³å®šï¼Œæ˜¯å› ä¸ºå™ªå£° $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ å§‹ç»ˆæ˜¯æ ‡å‡†åŒ–çš„ï¼Œè€Œ $\mathbf{x}_0$ çš„åˆ†å¸ƒå¯èƒ½å¾ˆå¤æ‚ã€‚</p>
            </div>
        </div>
        
        <h2>3.4 è®­ç»ƒç›®æ ‡ï¼šå˜åˆ†ä¸‹ç•Œçš„ç®€åŒ–</h2>
        
        <p>DDPMçš„å¦ä¸€ä¸ªé‡è¦è´¡çŒ®æ˜¯å°†å¤æ‚çš„å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰ç®€åŒ–ä¸ºä¸€ä¸ªç®€å•çš„å»å™ªç›®æ ‡ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†æ¨å¯¼è¿™ä¸ªè¿‡ç¨‹ã€‚</p>
        
        <h3>3.4.1 å®Œæ•´çš„å˜åˆ†ä¸‹ç•Œ</h3>
        
        <p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ $\log p_\theta(\mathbf{x}_0)$ã€‚ç”±äºç›´æ¥è®¡ç®—å›°éš¾ï¼Œæˆ‘ä»¬ä¼˜åŒ–å…¶å˜åˆ†ä¸‹ç•Œï¼š</p>
        
        <div class="math-block">
            $$\log p_\theta(\mathbf{x}_0) \geq \mathbb{E}_q\left[\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] = -L_{\text{VLB}}$$
        </div>
        
        <p>å…¶ä¸­ $L_{\text{VLB}}$ æ˜¯å˜åˆ†ä¸‹ç•ŒæŸå¤±ã€‚ç»è¿‡å±•å¼€ï¼ˆä½¿ç”¨é©¬å°”å¯å¤«æ€§è´¨ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ï¼š</p>
        
        <div class="math-block">
            $$L_{\text{VLB}} = L_T + \sum_{t=2}^{T} L_{t-1} + L_0$$
        </div>
        
        <p>å…¶ä¸­å„é¡¹å®šä¹‰ä¸ºï¼š</p>
        
        <div class="definition">
            <div class="definition-title">å˜åˆ†ä¸‹ç•Œçš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†</div>
            <div class="math-block">
                $$L_T = D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0) \| p(\mathbf{x}_T))$$
                $$L_{t-1} = \mathbb{E}_q\left[D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))\right]$$
                $$L_0 = \mathbb{E}_q\left[-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\right]$$
            </div>
            <ul>
                <li>$L_T$ï¼šå…ˆéªŒåŒ¹é…é¡¹ï¼Œé€šå¸¸å¾ˆå°å¯ä»¥å¿½ç•¥ï¼ˆå› ä¸º $q(\mathbf{x}_T|\mathbf{x}_0) \approx \mathcal{N}(0, \mathbf{I})$ï¼‰</li>
                <li>$L_{t-1}$ï¼šå»å™ªåŒ¹é…é¡¹ï¼Œè¿™æ˜¯ä¸»è¦çš„ä¼˜åŒ–ç›®æ ‡</li>
                <li>$L_0$ï¼šé‡å»ºé¡¹ï¼Œå†³å®šæœ€ç»ˆè¾“å‡ºè´¨é‡</li>
            </ul>
        </div>
        
        <p>å…³é”®åœ¨äºå¦‚ä½•å¤„ç† $L_{t-1}$ é¡¹ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“ $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ çš„é—­å¼è§£ï¼ˆè§3.3.1èŠ‚ï¼‰ï¼Œä¸”å‡è®¾ $p_\theta$ ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒKLæ•£åº¦å¯ä»¥ç®€åŒ–ä¸ºï¼š</p>
        
        <div class="math-block">
            $$L_{t-1} = \mathbb{E}_q\left[\frac{1}{2\sigma_t^2}\|\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\|^2\right] + C$$
        </div>
        
        <p>å…¶ä¸­ $C$ æ˜¯ä¸ $\theta$ æ— å…³çš„å¸¸æ•°ã€‚</p>
        
        <h3>3.4.2 ç®€åŒ–çš„å»å™ªç›®æ ‡</h3>
        
        <p>DDPMçš„å…³é”®æ´å¯Ÿæ˜¯ï¼šé€šè¿‡é€‰æ‹©å™ªå£°é¢„æµ‹å‚æ•°åŒ–ï¼Œå¯ä»¥å°†ä¸Šè¿°ç›®æ ‡è¿›ä¸€æ­¥ç®€åŒ–ã€‚å›å¿†3.3.2èŠ‚çš„ç»“æœï¼š</p>
        
        <div class="math-block">
            $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$
        </div>
        
        <p>å¦‚æœæˆ‘ä»¬å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ ä¸ºï¼š</p>
        
        <div class="math-block">
            $$\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right)$$
        </div>
        
        <p>é‚£ä¹ˆ $L_{t-1}$ å¯ä»¥ç®€åŒ–ä¸ºï¼š</p>
        
        <div class="math-block">
            $$L_{t-1} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}}\left[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$
        </div>
        
        <p>å…¶ä¸­ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ã€‚</p>
        
        <div class="visualization" style="background-color: #e8f4fd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>ğŸ¯ DDPMçš„ç®€åŒ–è®­ç»ƒç›®æ ‡</h4>
            <p>Hoç­‰äººå‘ç°ï¼Œå¿½ç•¥æƒé‡ç³»æ•°å¹¶å¯¹æ‰€æœ‰æ—¶é—´æ­¥æ±‚å’Œï¼Œå¾—åˆ°çš„ç®€åŒ–ç›®æ ‡æ•ˆæœæ›´å¥½ï¼š</p>
            <div class="math-block">
                $$L_{\text{simple}} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$
            </div>
            <p>è¿™å°±æ˜¯è‘—åçš„"ç®€å•æŸå¤±"â€”â€”åªéœ€è¦é¢„æµ‹å™ªå£°ï¼</p>
        </div>
        
        <h3>3.4.3 æŸå¤±å‡½æ•°çš„åŠ æƒç­–ç•¥</h3>
        
        <p>è™½ç„¶ç®€å•æŸå¤±æ•ˆæœå¾ˆå¥½ï¼Œä½†ä¸åŒæ—¶é—´æ­¥çš„é‡è¦æ€§ç¡®å®ä¸åŒã€‚åç»­ç ”ç©¶æå‡ºäº†å„ç§åŠ æƒç­–ç•¥ï¼š</p>
        
        <div class="code-block">
<pre>import torch
import matplotlib.pyplot as plt

# ä¸åŒçš„æŸå¤±åŠ æƒç­–ç•¥
def get_loss_weight(t, strategy='simple', snr_gamma=5.0):
    """
    è®¡ç®—æ—¶é—´æ­¥tçš„æŸå¤±æƒé‡
    
    ç­–ç•¥:
    - simple: æ‰€æœ‰æ—¶é—´æ­¥æƒé‡ç›¸åŒï¼ˆDDPMåŸå§‹ï¼‰
    - snr: åŸºäºä¿¡å™ªæ¯”çš„åŠ æƒ
    - truncated_snr: æˆªæ–­çš„SNRåŠ æƒï¼ˆé˜²æ­¢æç«¯å€¼ï¼‰
    - importance: åŸºäºé‡è¦æ€§é‡‡æ ·
    """
    if strategy == 'simple':
        return 1.0
    
    elif strategy == 'snr':
        # æƒé‡ä¸ä¿¡å™ªæ¯”æˆåæ¯”
        snr = alpha_bar[t] / (1 - alpha_bar[t])
        return 1.0 / (1.0 + snr)
    
    elif strategy == 'truncated_snr':
        # Min-SNR-Î³ åŠ æƒï¼ˆHang et al., 2023ï¼‰
        snr = alpha_bar[t] / (1 - alpha_bar[t])
        return torch.minimum(snr, torch.tensor(snr_gamma)) / snr
    
    elif strategy == 'importance':
        # åŸºäºL_tç³»æ•°çš„é‡è¦æ€§åŠ æƒ
        return beta[t]**2 / (2 * sigma[t]**2 * alpha[t] * (1 - alpha_bar[t]))

# å¯è§†åŒ–ä¸åŒåŠ æƒç­–ç•¥
T = 1000
t = torch.arange(T)
beta = torch.linspace(0.0001, 0.02, T)
alpha = 1 - beta
alpha_bar = torch.cumprod(alpha, dim=0)
sigma = beta  # DDPMçš„é€‰æ‹©

plt.figure(figsize=(12, 6))

strategies = ['simple', 'snr', 'truncated_snr', 'importance']
for strategy in strategies:
    weights = torch.tensor([get_loss_weight(i, strategy) for i in range(T)])
    plt.plot(weights, label=strategy)

plt.xlabel('Time Step t')
plt.ylabel('Loss Weight')
plt.title('Different Loss Weighting Strategies')
plt.legend()
plt.yscale('log')
plt.grid(True, alpha=0.3)
plt.show()</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">åŠ æƒç­–ç•¥å¯¹æ¯”</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">ç­–ç•¥</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">åŠ¨æœº</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ•ˆæœ</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">è®¡ç®—å¼€é”€</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€å• (Simple)</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç®€åŒ–è®­ç»ƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">åŸºå‡†ï¼Œæ•ˆæœå·²ç»ä¸é”™</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æœ€ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">SNRåŠ æƒ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å¹³è¡¡ä¸åŒå™ªå£°æ°´å¹³</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">æ”¹å–„é«˜å™ªå£°åŒºåŸŸ</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">Min-SNR-Î³</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">é¿å…æç«¯æƒé‡</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç›®å‰æœ€ä¼˜</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä½</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">é‡è¦æ€§é‡‡æ ·</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ç†è®ºæœ€ä¼˜</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">å®è·µä¸­ä¸ç¨³å®š</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">ä¸­ç­‰</td>
                </tr>
            </table>
        </div>
        
        <h3>3.4.4 è®­ç»ƒç®—æ³•æ€»ç»“</h3>
        
        <p>ç»¼åˆä»¥ä¸Šæ¨å¯¼ï¼ŒDDPMçš„è®­ç»ƒç®—æ³•æå…¶ç®€æ´ï¼š</p>
        
        <div class="code-block">
<pre>def train_ddpm(model, dataloader, num_epochs, T=1000):
    """DDPMè®­ç»ƒå¾ªç¯"""
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
    
    # é¢„è®¡ç®—å™ªå£°è°ƒåº¦ç›¸å…³å€¼
    betas = linear_beta_schedule(T)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas_bar = torch.sqrt(alphas_bar)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    for epoch in range(num_epochs):
        for batch_idx, (x_0, _) in enumerate(dataloader):
            batch_size = x_0.shape[0]
            
            # éšæœºé‡‡æ ·æ—¶é—´æ­¥
            t = torch.randint(0, T, (batch_size,), device=x_0.device)
            
            # é‡‡æ ·å™ªå£°
            epsilon = torch.randn_like(x_0)
            
            # å‰å‘æ‰©æ•£ï¼šè®¡ç®—x_t
            x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 + 
                   sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)
            
            # é¢„æµ‹å™ªå£°
            epsilon_pred = model(x_t, t)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(epsilon_pred, epsilon)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.4ï¼šå®ç°åŠ æƒæŸå¤±</div>
            <p>ä¿®æ”¹ä¸Šè¿°è®­ç»ƒä»£ç ï¼Œå®ç°Min-SNR-Î³åŠ æƒç­–ç•¥ï¼š</p>
            <ol>
                <li>è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„SNR</li>
                <li>åº”ç”¨Min-SNR-Î³åŠ æƒï¼ˆå»ºè®®Î³=5ï¼‰</li>
                <li>æ¯”è¾ƒåŠ æƒå‰åçš„è®­ç»ƒæ›²çº¿</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_4')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_4" class="answer">
                <pre>def train_ddpm_weighted(model, dataloader, num_epochs, T=1000, snr_gamma=5.0):
    """å¸¦Min-SNRåŠ æƒçš„DDPMè®­ç»ƒ"""
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
    
    # é¢„è®¡ç®—
    betas = linear_beta_schedule(T)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas_bar = torch.sqrt(alphas_bar)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # é¢„è®¡ç®—SNRå’Œæƒé‡
    snr = alphas_bar / (1 - alphas_bar)
    snr_clipped = torch.minimum(snr, torch.tensor(snr_gamma))
    loss_weights = snr_clipped / snr
    
    for epoch in range(num_epochs):
        for batch_idx, (x_0, _) in enumerate(dataloader):
            batch_size = x_0.shape[0]
            
            # é‡‡æ ·æ—¶é—´æ­¥
            t = torch.randint(0, T, (batch_size,), device=x_0.device)
            
            # å‰å‘æ‰©æ•£
            epsilon = torch.randn_like(x_0)
            x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 + 
                   sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)
            
            # é¢„æµ‹å™ªå£°
            epsilon_pred = model(x_t, t)
            
            # è®¡ç®—åŠ æƒæŸå¤±
            mse_loss = (epsilon_pred - epsilon).pow(2).mean(dim=[1,2,3])
            weights = loss_weights[t]
            loss = (weights * mse_loss).mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

# å…³é”®æ”¹è¿›ï¼š
# 1. é«˜SNRï¼ˆä½å™ªå£°ï¼‰åŒºåŸŸçš„æƒé‡è¢«é™ä½ï¼Œé¿å…è¿‡æ‹Ÿåˆç»†èŠ‚
# 2. ä½SNRï¼ˆé«˜å™ªå£°ï¼‰åŒºåŸŸä¿æŒè¾ƒé«˜æƒé‡ï¼Œç¡®ä¿ç»“æ„å­¦ä¹ 
# 3. Î³å‚æ•°æ§åˆ¶æˆªæ–­ç¨‹åº¦ï¼Œé€šå¸¸5-10æ•ˆæœè¾ƒå¥½</pre>
                <p><strong>å®è·µå»ºè®®</strong>ï¼šMin-SNR-Î³åŠ æƒåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚ä½†å¯¹äºä½åˆ†è¾¨ç‡æˆ–ç®€å•æ•°æ®é›†ï¼Œç®€å•æŸå¤±å¯èƒ½å·²ç»è¶³å¤Ÿã€‚</p>
            </div>
        </div>
        
        <h2>3.5 é‡‡æ ·ç®—æ³•ï¼šä»ç†è®ºåˆ°å®è·µ</h2>
        
        <p>è®­ç»ƒå¥½DDPMåï¼Œå¦‚ä½•ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Ÿè¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»DDPMçš„é‡‡æ ·ç®—æ³•ï¼Œä»æ ‡å‡†çš„1000æ­¥é‡‡æ ·åˆ°å„ç§å®ç”¨æŠ€å·§ã€‚</p>
        
        <h3>3.5.1 æ ‡å‡†DDPMé‡‡æ ·</h3>
        
        <p>DDPMçš„é‡‡æ ·è¿‡ç¨‹æ˜¯ä»çº¯å™ªå£° $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ å¼€å§‹ï¼Œé€æ­¥å»å™ªç›´åˆ°å¾—åˆ°æ¸…æ™°çš„å›¾åƒ $\mathbf{x}_0$ã€‚</p>
        
        <div class="definition">
            <div class="definition-title">DDPMé‡‡æ ·ç®—æ³•</div>
            <p>å¯¹äºæ¯ä¸€æ­¥ $t = T, T-1, ..., 1$ï¼š</p>
            <div class="math-block">
                $$\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right) + \sigma_t \mathbf{z}$$
            </div>
            <p>å…¶ä¸­ï¼š</p>
            <ul>
                <li>$\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ æ˜¯è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹ç½‘ç»œ</li>
                <li>$\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$ æ˜¯é‡‡æ ·å™ªå£°ï¼ˆå½“ $t > 1$ æ—¶ï¼‰</li>
                <li>$\sigma_t$ æ˜¯æ–¹å·®ï¼ŒDDPMä½¿ç”¨ $\sigma_t = \beta_t$</li>
            </ul>
        </div>
        
        <p>å®Œæ•´çš„å®ç°ä»£ç ï¼š</p>
        
        <div class="code-block">
<pre>@torch.no_grad()
def ddpm_sample(model, shape, num_timesteps=1000, device='cuda'):
    """
    DDPMæ ‡å‡†é‡‡æ ·ç®—æ³•
    
    Args:
        model: è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹æ¨¡å‹
        shape: ç”Ÿæˆå›¾åƒçš„å½¢çŠ¶ï¼Œå¦‚ (batch_size, 3, 32, 32)
        num_timesteps: æ€»æ—¶é—´æ­¥æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        ç”Ÿæˆçš„å›¾åƒ x_0
    """
    # é¢„è®¡ç®—å™ªå£°è°ƒåº¦
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas = torch.sqrt(alphas)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # ä»çº¯å™ªå£°å¼€å§‹
    x_t = torch.randn(shape, device=device)
    
    # é€æ­¥å»å™ª
    for t in reversed(range(num_timesteps)):
        # åˆ›å»ºæ—¶é—´æ­¥å¼ é‡
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t_tensor)
        
        # è®¡ç®—å‡å€¼
        mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]
        
        # æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
        if t > 0:
            noise = torch.randn_like(x_t)
            std = torch.sqrt(betas[t])  # DDPMä½¿ç”¨Î²_tä½œä¸ºæ–¹å·®
            x_t = mean + std * noise
        else:
            x_t = mean
    
    return x_t</pre>
        </div>
        
        <h4>é‡‡æ ·è¿‡ç¨‹çš„å¯è§†åŒ–</h4>
        
        <p>ä¸ºäº†æ›´å¥½åœ°ç†è§£é‡‡æ ·è¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–ä¸åŒæ—¶é—´æ­¥çš„ä¸­é—´ç»“æœï¼š</p>
        
        <div class="code-block">
<pre>def visualize_sampling_process(model, num_steps_to_show=10):
    """å¯è§†åŒ–DDPMé‡‡æ ·è¿‡ç¨‹"""
    import matplotlib.pyplot as plt
    
    # é‡‡æ ·å¹¶ä¿å­˜ä¸­é—´ç»“æœ
    shape = (1, 3, 32, 32)
    T = 1000
    
    # é€‰æ‹©è¦å±•ç¤ºçš„æ—¶é—´æ­¥
    steps_to_show = torch.linspace(T-1, 0, num_steps_to_show, dtype=torch.long)
    intermediate_images = []
    
    # åˆå§‹åŒ–
    x_t = torch.randn(shape, device='cuda')
    betas = linear_beta_schedule(T).to('cuda')
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    # é‡‡æ ·è¿‡ç¨‹
    for t in reversed(range(T)):
        t_tensor = torch.full((1,), t, device='cuda', dtype=torch.long)
        
        # é¢„æµ‹å¹¶æ›´æ–°
        epsilon_pred = model(x_t, t_tensor)
        # ... (é‡‡æ ·æ­¥éª¤åŒä¸Š)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if t in steps_to_show:
            # å°†x_tæ˜ å°„åˆ°[0, 1]èŒƒå›´ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
            img = (x_t.clamp(-1, 1) + 1) / 2
            intermediate_images.append(img.cpu())
    
    # ç»˜åˆ¶ç»“æœ
    fig, axes = plt.subplots(1, len(intermediate_images), figsize=(20, 4))
    for i, (img, t) in enumerate(zip(intermediate_images, steps_to_show)):
        axes[i].imshow(img[0].permute(1, 2, 0))
        axes[i].set_title(f't = {t.item()}')
        axes[i].axis('off')
    
    plt.suptitle('DDPM Sampling Process: From Noise to Image')
    plt.tight_layout()
    plt.show()</pre>
        </div>
        
        <div class="visualization" style="background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h4>é‡‡æ ·è¿‡ç¨‹çš„ç‰¹ç‚¹</h4>
            <ul>
                <li><strong>å‰æœŸï¼ˆt â‰ˆ 1000ï¼‰</strong>ï¼šä¸»è¦æ¢å¤å…¨å±€ç»“æ„å’Œå¤§è‡´å½¢çŠ¶</li>
                <li><strong>ä¸­æœŸï¼ˆt â‰ˆ 500ï¼‰</strong>ï¼šç»†åŒ–å¯¹è±¡è½®å»“å’Œä¸»è¦ç‰¹å¾</li>
                <li><strong>åæœŸï¼ˆt â‰ˆ 0ï¼‰</strong>ï¼šæ·»åŠ çº¹ç†ç»†èŠ‚å’Œé«˜é¢‘ä¿¡æ¯</li>
            </ul>
            <p>è¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºè‰ºæœ¯å®¶ä½œç”»ï¼šå…ˆå‹¾å‹’è½®å»“ï¼Œå†å¡«å……é¢œè‰²ï¼Œæœ€åæ·»åŠ ç»†èŠ‚ã€‚</p>
        </div>
        
        <h4>è®¡ç®—æ•ˆç‡åˆ†æ</h4>
        
        <p>æ ‡å‡†DDPMé‡‡æ ·çš„ä¸»è¦é—®é¢˜æ˜¯é€Ÿåº¦æ…¢ã€‚è®©æˆ‘ä»¬åˆ†æä¸€ä¸‹è®¡ç®—æˆæœ¬ï¼š</p>
        
        <div class="code-block">
<pre>def analyze_sampling_cost(model, batch_size=16, image_size=256):
    """åˆ†æDDPMé‡‡æ ·çš„è®¡ç®—æˆæœ¬"""
    import time
    
    shape = (batch_size, 3, image_size, image_size)
    T = 1000
    
    # æµ‹é‡å•æ¬¡å‰å‘ä¼ æ’­æ—¶é—´
    x = torch.randn(shape, device='cuda')
    t = torch.randint(0, T, (batch_size,), device='cuda')
    
    # é¢„çƒ­GPU
    for _ in range(10):
        _ = model(x, t)
    torch.cuda.synchronize()
    
    # è®¡æ—¶
    start = time.time()
    num_runs = 50
    for _ in range(num_runs):
        _ = model(x, t)
    torch.cuda.synchronize()
    end = time.time()
    
    time_per_forward = (end - start) / num_runs
    total_time = time_per_forward * T
    
    print(f"å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å•æ¬¡å‰å‘ä¼ æ’­: {time_per_forward*1000:.2f} ms")
    print(f"å®Œæ•´é‡‡æ · (1000æ­¥): {total_time:.2f} ç§’")
    print(f"æ¯ç§’ç”Ÿæˆå›¾åƒæ•°: {batch_size/total_time:.3f}")
    
    # å†…å­˜ä½¿ç”¨ä¼°è®¡
    model_params = sum(p.numel() for p in model.parameters()) * 4 / 1024**3  # GB
    activation_memory = batch_size * 3 * image_size**2 * 4 * 50 / 1024**3  # ç²—ç•¥ä¼°è®¡
    print(f"\nå†…å­˜ä½¿ç”¨:")
    print(f"æ¨¡å‹å‚æ•°: {model_params:.2f} GB")
    print(f"æ¿€æ´»å€¼ (ä¼°è®¡): {activation_memory:.2f} GB")</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">å…¸å‹æ€§èƒ½æ•°æ®</div>
            <table style="width: 100%; margin-top: 10px;">
                <tr>
                    <th style="padding: 10px; background-color: #f0f0f0;">é…ç½®</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">å•æ­¥æ—¶é—´</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">æ€»é‡‡æ ·æ—¶é—´</th>
                    <th style="padding: 10px; background-color: #f0f0f0;">ååé‡</th>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">32Ã—32, batch=64</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~5ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">5ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">12.8 å›¾åƒ/ç§’</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">256Ã—256, batch=8</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~50ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">50ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">0.16 å›¾åƒ/ç§’</td>
                </tr>
                <tr>
                    <td style="padding: 10px; border: 1px solid #ddd;">512Ã—512, batch=4</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">~200ms</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">200ç§’</td>
                    <td style="padding: 10px; border: 1px solid #ddd;">0.02 å›¾åƒ/ç§’</td>
                </tr>
            </table>
            <p><small>*åŸºäºRTX 3090ï¼Œå®é™…æ€§èƒ½å› æ¨¡å‹æ¶æ„è€Œå¼‚</small></p>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.1ï¼šå®ç°é‡‡æ ·è¿›åº¦æ¡</div>
            <p>ä¿®æ”¹DDPMé‡‡æ ·å‡½æ•°ï¼Œæ·»åŠ ï¼š</p>
            <ol>
                <li>tqdmè¿›åº¦æ¡æ˜¾ç¤ºé‡‡æ ·è¿›åº¦</li>
                <li>å¯é€‰çš„ä¸­é—´ç»“æœä¿å­˜</li>
                <li>EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰æ¨¡å‹æ”¯æŒ</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_1" class="answer">
                <pre>from tqdm import tqdm

@torch.no_grad()
def ddpm_sample_with_progress(
    model, 
    shape, 
    num_timesteps=1000,
    device='cuda',
    use_ema=True,
    ema_model=None,
    save_intermediate=False,
    save_steps=None
):
    """å¢å¼ºç‰ˆDDPMé‡‡æ ·"""
    # é€‰æ‹©æ¨¡å‹
    if use_ema and ema_model is not None:
        sample_model = ema_model
    else:
        sample_model = model
    
    sample_model.eval()
    
    # é¢„è®¡ç®—
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    sqrt_alphas = torch.sqrt(alphas)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    
    # åˆå§‹åŒ–
    x_t = torch.randn(shape, device=device)
    intermediates = []
    
    # é‡‡æ ·å¾ªç¯
    for t in tqdm(reversed(range(num_timesteps)), desc='Sampling', total=num_timesteps):
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = sample_model(x_t, t_tensor)
        
        # æ›´æ–°x_t
        mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]
        
        if t > 0:
            noise = torch.randn_like(x_t)
            std = torch.sqrt(betas[t])
            x_t = mean + std * noise
        else:
            x_t = mean
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if save_intermediate and save_steps is not None and t in save_steps:
            intermediates.append({
                't': t,
                'x_t': x_t.cpu().clone(),
                'pred_x_0': self._predict_x0_from_eps(x_t, t, epsilon_pred)
            })
    
    if save_intermediate:
        return x_t, intermediates
    else:
        return x_t

def _predict_x0_from_eps(x_t, t, epsilon_pred):
    """ä»å™ªå£°é¢„æµ‹æ¢å¤x_0ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
    return (x_t - sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas_bar[t]</pre>
                <p><strong>ä½¿ç”¨æŠ€å·§</strong>ï¼š</p>
                <ul>
                    <li>EMAæ¨¡å‹é€šå¸¸ç”Ÿæˆè´¨é‡æ›´å¥½ï¼Œè®­ç»ƒæ—¶åº”åŒæ—¶ç»´æŠ¤</li>
                    <li>ä¿å­˜ä¸­é—´ç»“æœæœ‰åŠ©äºè°ƒè¯•å’Œç†è§£æ¨¡å‹è¡Œä¸º</li>
                    <li>å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œè€ƒè™‘ä½¿ç”¨DataLoaderé£æ ¼çš„ç”Ÿæˆå™¨ä»¥èŠ‚çœå†…å­˜</li>
                </ul>
            </div>
        </div>
        
        <h3>3.5.2 é‡‡æ ·çš„éšæœºæ€§æ§åˆ¶</h3>
        
        <p>DDPMé‡‡æ ·è¿‡ç¨‹ä¸­çš„éšæœºæ€§æ¥æºäºä¸¤ä¸ªåœ°æ–¹ï¼šåˆå§‹å™ªå£° $\mathbf{x}_T$ å’Œæ¯æ­¥æ·»åŠ çš„å™ªå£° $\mathbf{z}_t$ã€‚é€šè¿‡æ§åˆ¶è¿™äº›éšæœºæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å½±å“ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</p>
        
        <h4>æ¸©åº¦å‚æ•°çš„å¼•å…¥</h4>
        
        <p>ç±»ä¼¼äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥æ¸©åº¦å‚æ•°æ¥æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§ï¼š</p>
        
        <div class="code-block">
<pre>def ddpm_sample_with_temperature(
    model, 
    shape, 
    temperature=1.0,
    noise_temperature=1.0,
    num_timesteps=1000,
    device='cuda'
):
    """
    å¸¦æ¸©åº¦æ§åˆ¶çš„DDPMé‡‡æ ·
    
    Args:
        temperature: æ§åˆ¶åˆå§‹å™ªå£°çš„æ¸©åº¦
        noise_temperature: æ§åˆ¶æ¯æ­¥å™ªå£°çš„æ¸©åº¦
    """
    # é¢„è®¡ç®—ï¼ˆåŒå‰ï¼‰
    betas = linear_beta_schedule(num_timesteps).to(device)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    # æ¸©åº¦è°ƒæ•´çš„åˆå§‹å™ªå£°
    x_t = torch.randn(shape, device=device) * temperature
    
    for t in reversed(range(num_timesteps)):
        t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)
        
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t_tensor)
        
        # è®¡ç®—å‡å€¼
        mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])
        
        if t > 0:
            # æ¸©åº¦è°ƒæ•´çš„æ­¥è¿›å™ªå£°
            noise = torch.randn_like(x_t) * noise_temperature
            std = torch.sqrt(betas[t])
            x_t = mean + std * noise
        else:
            x_t = mean
    
    return x_t</pre>
        </div>
        
        <div class="definition">
            <div class="definition-title">æ¸©åº¦å‚æ•°çš„æ•ˆæœ</div>
            <ul>
                <li><strong>temperature < 1.0</strong>ï¼šå‡å°‘åˆå§‹éšæœºæ€§ï¼Œç”Ÿæˆæ›´"å…¸å‹"çš„æ ·æœ¬</li>
                <li><strong>temperature > 1.0</strong>ï¼šå¢åŠ åˆå§‹éšæœºæ€§ï¼Œç”Ÿæˆæ›´å¤šæ ·ä½†å¯èƒ½è´¨é‡è¾ƒä½çš„æ ·æœ¬</li>
                <li><strong>noise_temperature < 1.0</strong>ï¼šå‡å°‘å»å™ªè¿‡ç¨‹çš„éšæœºæ€§ï¼Œç»“æœæ›´ç¡®å®šä½†å¯èƒ½è¿‡äºå¹³æ»‘</li>
                <li><strong>noise_temperature > 1.0</strong>ï¼šå¢åŠ å»å™ªéšæœºæ€§ï¼Œå¯èƒ½äº§ç”Ÿæ›´å¤šç»†èŠ‚ä½†ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±</li>
            </ul>
        </div>
        
        <h4>ç¡®å®šæ€§é‡‡æ ·ï¼šDDIMé¢„è§ˆ</h4>
        
        <p>ä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿæ˜¯ï¼šå¦‚æœæˆ‘ä»¬å®Œå…¨å»é™¤æ­¥è¿›å™ªå£°ï¼ˆè®¾ç½® $\sigma_t = 0$ï¼‰ï¼Œé‡‡æ ·è¿‡ç¨‹å˜æˆç¡®å®šæ€§çš„ã€‚è¿™å°±æ˜¯DDIMçš„æ ¸å¿ƒæ€æƒ³ï¼š</p>
        
        <div class="code-block">
<pre>def ddpm_deterministic_sample(model, shape, num_timesteps=1000, eta=0.0):
    """
    ç¡®å®šæ€§æˆ–éƒ¨åˆ†ç¡®å®šæ€§é‡‡æ ·
    eta=0: å®Œå…¨ç¡®å®šæ€§ï¼ˆDDIMï¼‰
    eta=1: æ ‡å‡†DDPMï¼ˆå®Œå…¨éšæœºï¼‰
    """
    x_t = torch.randn(shape, device='cuda')
    
    for t in reversed(range(num_timesteps)):
        # é¢„æµ‹å™ªå£°
        epsilon_pred = model(x_t, t)
        
        # é¢„æµ‹x_0
        x_0_pred = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])
        
        if t > 0:
            # è®¡ç®—æ–¹å‘æŒ‡å‘x_{t-1}
            direction = torch.sqrt(1 - alphas_bar[t-1]) * epsilon_pred
            
            # ç¡®å®šæ€§éƒ¨åˆ†
            x_t = torch.sqrt(alphas_bar[t-1]) * x_0_pred + direction
            
            # éšæœºéƒ¨åˆ†ï¼ˆç”±etaæ§åˆ¶ï¼‰
            if eta > 0:
                noise = torch.randn_like(x_t)
                variance = eta * betas[t] * (1 - alphas_bar[t-1]) / (1 - alphas_bar[t])
                x_t = x_t + torch.sqrt(variance) * noise
        else:
            x_t = x_0_pred
    
    return x_t</pre>
        </div>
        
        <h4>é‡‡æ ·ç§å­ä¸å¯é‡å¤æ€§</h4>
        
        <p>å¯¹äºéœ€è¦å¯é‡å¤ç»“æœçš„åº”ç”¨ï¼Œæ§åˆ¶éšæœºç§å­è‡³å…³é‡è¦ï¼š</p>
        
        <div class="code-block">
<pre>class SeededSampler:
    """å¯é‡å¤çš„é‡‡æ ·å™¨"""
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def sample_with_seed(self, seed, shape, **kwargs):
        """ä½¿ç”¨æŒ‡å®šç§å­é‡‡æ ·"""
        # ä¿å­˜å½“å‰éšæœºçŠ¶æ€
        cpu_state = torch.get_rng_state()
        cuda_state = torch.cuda.get_rng_state(self.device)
        
        # è®¾ç½®ç§å­
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        # é‡‡æ ·
        result = ddpm_sample(self.model, shape, device=self.device, **kwargs)
        
        # æ¢å¤éšæœºçŠ¶æ€
        torch.set_rng_state(cpu_state)
        torch.cuda.set_rng_state(cuda_state, self.device)
        
        return result
    
    def sample_variations(self, base_seed, num_variations, shape, temperature_range=(0.8, 1.2)):
        """ç”ŸæˆåŒä¸€ç§å­çš„å¤šä¸ªå˜ä½“"""
        variations = []
        
        for i in range(num_variations):
            # ä½¿ç”¨ç›¸åŒçš„åŸºç¡€ç§å­ä½†ä¸åŒçš„æ¸©åº¦
            temp = np.linspace(temperature_range[0], temperature_range[1], num_variations)[i]
            
            torch.manual_seed(base_seed)
            torch.cuda.manual_seed(base_seed)
            
            sample = ddpm_sample_with_temperature(
                self.model, shape, 
                temperature=temp,
                device=self.device
            )
            variations.append(sample)
            
        return torch.stack(variations)</pre>
        </div>
        
        <h4>é«˜çº§æŠ€å·§ï¼šå¼•å¯¼é‡‡æ ·ï¼ˆGuided Samplingï¼‰</h4>
        
        <p>æˆ‘ä»¬å¯ä»¥åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥é¢å¤–çš„å¼•å¯¼ä¿¡å·ï¼Œè¿™æ˜¯æ¡ä»¶ç”Ÿæˆçš„åŸºç¡€ï¼š</p>
        
        <div class="code-block">
<pre>def guided_sample(model, shape, guidance_fn=None, guidance_scale=1.0):
    """
    å¸¦å¼•å¯¼çš„é‡‡æ ·
    guidance_fn: è®¡ç®—å¼•å¯¼æ¢¯åº¦çš„å‡½æ•°
    guidance_scale: å¼•å¯¼å¼ºåº¦
    """
    x_t = torch.randn(shape, device='cuda')
    x_t.requires_grad = True
    
    for t in reversed(range(num_timesteps)):
        # æ ‡å‡†DDPMæ›´æ–°
        with torch.no_grad():
            epsilon_pred = model(x_t, t)
            mean = compute_mean(x_t, epsilon_pred, t)
            std = torch.sqrt(betas[t])
        
        # è®¡ç®—å¼•å¯¼æ¢¯åº¦
        if guidance_fn is not None and t > 0:
            # è®¡ç®—å¼•å¯¼æŸå¤±
            guidance_loss = guidance_fn(x_t, t)
            
            # è®¡ç®—æ¢¯åº¦
            grad = torch.autograd.grad(guidance_loss, x_t)[0]
            
            # åº”ç”¨å¼•å¯¼ï¼ˆæ³¨æ„ç¬¦å·ï¼šæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼‰
            mean = mean - guidance_scale * std**2 * grad
        
        # æ›´æ–°x_t
        if t > 0:
            noise = torch.randn_like(x_t)
            x_t = mean + std * noise
        else:
            x_t = mean
            
        x_t = x_t.detach().requires_grad_(True)
    
    return x_t.detach()

# ç¤ºä¾‹ï¼šç±»åˆ«å¼•å¯¼
def classifier_guidance(x_t, t, classifier, target_class):
    """ä½¿ç”¨åˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆç‰¹å®šç±»åˆ«"""
    logits = classifier(x_t, t)
    log_prob = F.log_softmax(logits, dim=1)
    return -log_prob[:, target_class].sum()  # è´Ÿå¯¹æ•°æ¦‚ç‡ä½œä¸ºæŸå¤±</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.2ï¼šæ¢ç´¢æ¸©åº¦å‚æ•°çš„å½±å“</div>
            <p>å®ç°ä¸€ä¸ªå®éªŒï¼Œç³»ç»Ÿåœ°æ¢ç´¢ä¸åŒæ¸©åº¦å‚æ•°å¯¹ç”Ÿæˆç»“æœçš„å½±å“ï¼š</p>
            <ol>
                <li>å›ºå®šç§å­ï¼Œæ”¹å˜temperatureï¼ˆ0.5, 0.7, 1.0, 1.3, 1.5ï¼‰</li>
                <li>å›ºå®šç§å­ï¼Œæ”¹å˜noise_temperatureï¼ˆ0, 0.5, 1.0, 1.5ï¼‰</li>
                <li>å¯è§†åŒ–ç»“æœå¹¶è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡ï¼ˆå¦‚å¹³å‡åƒç´ æ–¹å·®ï¼‰</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_2')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_2" class="answer">
                <pre>def temperature_ablation_study(model, seed=42):
    """æ¸©åº¦å‚æ•°æ¶ˆèå®éªŒ"""
    import matplotlib.pyplot as plt
    
    shape = (1, 3, 32, 32)
    
    # å®éªŒ1ï¼šåˆå§‹æ¸©åº¦çš„å½±å“
    init_temps = [0.5, 0.7, 1.0, 1.3, 1.5]
    init_results = []
    
    for temp in init_temps:
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        sample = ddpm_sample_with_temperature(
            model, shape, 
            temperature=temp,
            noise_temperature=1.0
        )
        init_results.append(sample)
    
    # å®éªŒ2ï¼šå™ªå£°æ¸©åº¦çš„å½±å“
    noise_temps = [0.0, 0.5, 1.0, 1.5]
    noise_results = []
    
    for noise_temp in noise_temps:
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        sample = ddpm_sample_with_temperature(
            model, shape,
            temperature=1.0,
            noise_temperature=noise_temp
        )
        noise_results.append(sample)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, max(len(init_temps), len(noise_temps)), figsize=(15, 6))
    
    # ç»˜åˆ¶åˆå§‹æ¸©åº¦ç»“æœ
    for i, (temp, img) in enumerate(zip(init_temps, init_results)):
        img_np = (img[0].clamp(-1, 1) + 1) / 2
        axes[0, i].imshow(img_np.permute(1, 2, 0).cpu())
        axes[0, i].set_title(f'T_init={temp}')
        axes[0, i].axis('off')
    
    # ç»˜åˆ¶å™ªå£°æ¸©åº¦ç»“æœ
    for i, (temp, img) in enumerate(zip(noise_temps, noise_results)):
        img_np = (img[0].clamp(-1, 1) + 1) / 2
        axes[1, i].imshow(img_np.permute(1, 2, 0).cpu())
        axes[1, i].set_title(f'T_noise={temp}')
        axes[1, i].axis('off')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(noise_temps), len(init_temps)):
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
    print("åˆå§‹æ¸©åº¦å¯¹å›¾åƒæ ‡å‡†å·®çš„å½±å“:")
    for temp, img in zip(init_temps, init_results):
        std = img.std().item()
        print(f"  T_init={temp}: std={std:.4f}")
    
    print("\nå™ªå£°æ¸©åº¦å¯¹å›¾åƒæ ‡å‡†å·®çš„å½±å“:")
    for temp, img in zip(noise_temps, noise_results):
        std = img.std().item()
        print(f"  T_noise={temp}: std={std:.4f}")

# é¢å¤–åˆ†æï¼šå¤šæ¬¡é‡‡æ ·çš„å¤šæ ·æ€§
def diversity_analysis(model, num_samples=100):
    """åˆ†æä¸åŒæ¸©åº¦è®¾ç½®ä¸‹çš„æ ·æœ¬å¤šæ ·æ€§"""
    shape = (num_samples, 3, 32, 32)
    
    # æ ‡å‡†é‡‡æ ·
    samples_standard = ddpm_sample(model, shape)
    
    # ä½æ¸©é‡‡æ ·
    samples_low_temp = ddpm_sample_with_temperature(
        model, shape, temperature=0.7, noise_temperature=0.7
    )
    
    # è®¡ç®—æˆå¯¹è·ç¦»
    def pairwise_l2_distance(samples):
        # å±•å¹³æ ·æœ¬
        flat = samples.view(num_samples, -1)
        # è®¡ç®—æˆå¯¹L2è·ç¦»
        distances = torch.cdist(flat, flat, p=2)
        # å–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆé¿å…é‡å¤ï¼‰
        mask = torch.triu(torch.ones_like(distances), diagonal=1).bool()
        return distances[mask].mean().item()
    
    div_standard = pairwise_l2_distance(samples_standard)
    div_low_temp = pairwise_l2_distance(samples_low_temp)
    
    print(f"æ ‡å‡†é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_standard:.4f}")
    print(f"ä½æ¸©é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_low_temp:.4f}")
    print(f"å¤šæ ·æ€§é™ä½æ¯”ä¾‹: {(1 - div_low_temp/div_standard)*100:.1f}%")</pre>
                <p><strong>å…³é”®å‘ç°</strong>ï¼š</p>
                <ul>
                    <li>é™ä½åˆå§‹æ¸©åº¦ä¼šä½¿ç”Ÿæˆç»“æœæ›´æ¥è¿‘"å¹³å‡"å›¾åƒï¼Œå‡å°‘æç«¯æƒ…å†µ</li>
                    <li>noise_temperature=0 ä¼šäº§ç”Ÿè¿‡åº¦å¹³æ»‘çš„ç»“æœï¼Œä¸¢å¤±çº¹ç†ç»†èŠ‚</li>
                    <li>é€‚åº¦é™ä½æ¸©åº¦ï¼ˆ0.7-0.9ï¼‰é€šå¸¸èƒ½æé«˜æ„ŸçŸ¥è´¨é‡ï¼Œä½†ä¼šç‰ºç‰²å¤šæ ·æ€§</li>
                    <li>å¯¹äºç‰¹å®šåº”ç”¨ï¼Œéœ€è¦åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡</li>
                </ul>
            </div>
        </div>
        
        <h3>3.5.3 å¸¸è§é—®é¢˜ä¸è°ƒè¯•æŠ€å·§</h3>
        
        <p>DDPMé‡‡æ ·è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°å„ç§é—®é¢˜ã€‚æœ¬èŠ‚æ€»ç»“å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œä¿®å¤é—®é¢˜ã€‚</p>
        
        <h4>é—®é¢˜1ï¼šç”Ÿæˆç»“æœå…¨æ˜¯å™ªå£°</h4>
        
        <div class="definition">
            <div class="definition-title">ç—‡çŠ¶ä¸åŸå› </div>
            <ul>
                <li><strong>ç—‡çŠ¶</strong>ï¼šé‡‡æ ·ç»“æœçœ‹èµ·æ¥åƒéšæœºå™ªå£°ï¼Œæ²¡æœ‰ä»»ä½•ç»“æ„</li>
                <li><strong>å¯èƒ½åŸå› </strong>ï¼š
                    <ol>
                        <li>æ¨¡å‹æœªæ­£ç¡®åŠ è½½æˆ–æƒé‡æŸå</li>
                        <li>å™ªå£°è°ƒåº¦è®¡ç®—é”™è¯¯</li>
                        <li>æ—¶é—´æ­¥ç¼–ç é”™è¯¯</li>
                        <li>è¾“å…¥å½’ä¸€åŒ–ä¸åŒ¹é…</li>
                    </ol>
                </li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># è°ƒè¯•æ­¥éª¤1ï¼šéªŒè¯æ¨¡å‹é¢„æµ‹
def debug_model_predictions(model, device='cuda'):
    """æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ—¶é—´æ­¥çš„é¢„æµ‹"""
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(1, 3, 32, 32, device=device)
    
    # æµ‹è¯•å‡ ä¸ªå…³é”®æ—¶é—´æ­¥
    test_timesteps = [0, 250, 500, 750, 999]
    
    for t in test_timesteps:
        t_tensor = torch.tensor([t], device=device)
        with torch.no_grad():
            pred = model(x, t_tensor)
        
        print(f"t={t}:")
        print(f"  Input stats: mean={x.mean():.4f}, std={x.std():.4f}")
        print(f"  Pred stats:  mean={pred.mean():.4f}, std={pred.std():.4f}")
        
        # é¢„æµ‹åº”è¯¥æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
        if abs(pred.mean()) > 0.5 or abs(pred.std() - 1.0) > 0.5:
            print("  âš ï¸ è­¦å‘Šï¼šé¢„æµ‹ç»Ÿè®¡é‡å¼‚å¸¸ï¼")

# è°ƒè¯•æ­¥éª¤2ï¼šéªŒè¯å™ªå£°è°ƒåº¦
def debug_noise_schedule(num_timesteps=1000):
    """æ£€æŸ¥å™ªå£°è°ƒåº¦çš„åˆç†æ€§"""
    betas = linear_beta_schedule(num_timesteps)
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    print("å™ªå£°è°ƒåº¦æ£€æŸ¥:")
    print(f"Î²_0 = {betas[0]:.6f}, Î²_T = {betas[-1]:.6f}")
    print(f"á¾±_0 = {alphas_bar[0]:.6f}, á¾±_T = {alphas_bar[-1]:.6f}")
    
    # æ£€æŸ¥å…³é”®å±æ€§
    if alphas_bar[-1] > 0.01:
        print("âš ï¸ è­¦å‘Šï¼šá¾±_T å¤ªå¤§ï¼Œæœ€ç»ˆå™ªå£°æ°´å¹³ä¸å¤Ÿ")
    if betas[0] > 0.01:
        print("âš ï¸ è­¦å‘Šï¼šÎ²_0 å¤ªå¤§ï¼Œåˆå§‹ç ´åå¤ªä¸¥é‡")
    
    # æ£€æŸ¥å•è°ƒæ€§
    if not torch.all(alphas_bar[1:] <= alphas_bar[:-1]):
        print("âš ï¸ è­¦å‘Šï¼šá¾± ä¸æ˜¯å•è°ƒé€’å‡çš„ï¼")
    
    return betas, alphas, alphas_bar</pre>
        </div>
        
        <h4>é—®é¢˜2ï¼šç”Ÿæˆç»“æœæ¨¡ç³Šæˆ–è¿‡åº¦å¹³æ»‘</h4>
        
        <div class="visualization" style="background-color: #fff3cd; padding: 20px; margin: 20px 0; border-radius: 8px;">
            <h5>å¸¸è§åŸå› åŠè§£å†³æ–¹æ¡ˆ</h5>
            <ol>
                <li><strong>æ–¹å·®è®¾ç½®è¿‡å°</strong>ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¿‡å°çš„ $\sigma_t$</li>
                <li><strong>æå‰åœæ­¢é‡‡æ ·</strong>ï¼šç¡®ä¿å®Œæˆæ‰€æœ‰1000æ­¥ï¼ˆæˆ–è®¾å®šçš„æ­¥æ•°ï¼‰</li>
                <li><strong>æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°å‡å€¼</strong>ï¼šå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒæ—¶çš„å™ªå£°è°ƒåº¦</li>
                <li><strong>æ•°å€¼ç²¾åº¦é—®é¢˜</strong>ï¼šä½¿ç”¨FP16æ—¶æŸäº›æ“ä½œå¯èƒ½æŸå¤±ç²¾åº¦</li>
            </ol>
        </div>
        
        <div class="code-block">
<pre># è¯Šæ–­è¿‡åº¦å¹³æ»‘é—®é¢˜
def diagnose_smoothness(model, num_samples=10):
    """è¯Šæ–­ç”Ÿæˆç»“æœçš„å¹³æ»‘åº¦é—®é¢˜"""
    samples = []
    
    # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
    for _ in range(num_samples):
        sample = ddpm_sample(model, (1, 3, 32, 32))
        samples.append(sample)
    
    samples = torch.cat(samples, dim=0)
    
    # è®¡ç®—é«˜é¢‘ä¿¡æ¯
    def compute_high_freq_energy(images):
        # ä½¿ç”¨Sobelæ»¤æ³¢å™¨æ£€æµ‹è¾¹ç¼˜
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                               dtype=torch.float32).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                               dtype=torch.float32).view(1, 1, 3, 3)
        
        # è½¬æ¢ä¸ºç°åº¦
        gray = images.mean(dim=1, keepdim=True)
        
        # è®¡ç®—æ¢¯åº¦
        edges_x = F.conv2d(gray, sobel_x, padding=1)
        edges_y = F.conv2d(gray, sobel_y, padding=1)
        edges = torch.sqrt(edges_x**2 + edges_y**2)
        
        return edges.mean().item()
    
    # ä¸çœŸå®æ•°æ®å¯¹æ¯”
    real_data = next(iter(train_loader))[0][:num_samples]
    
    gen_hf = compute_high_freq_energy(samples)
    real_hf = compute_high_freq_energy(real_data)
    
    print(f"ç”Ÿæˆæ ·æœ¬çš„é«˜é¢‘èƒ½é‡: {gen_hf:.4f}")
    print(f"çœŸå®æ•°æ®çš„é«˜é¢‘èƒ½é‡: {real_hf:.4f}")
    print(f"æ¯”ç‡: {gen_hf/real_hf:.2f}")
    
    if gen_hf/real_hf < 0.5:
        print("âš ï¸ ç”Ÿæˆç»“æœå¯èƒ½è¿‡åº¦å¹³æ»‘ï¼")
        print("å»ºè®®ï¼š")
        print("  1. æ£€æŸ¥å™ªå£°æ¸©åº¦è®¾ç½®")
        print("  2. éªŒè¯æœ€åå‡ æ­¥çš„æ–¹å·®")
        print("  3. è€ƒè™‘ä½¿ç”¨æ”¹è¿›çš„æ–¹å·®è°ƒåº¦")</pre>
        </div>
        
        <h4>é—®é¢˜3ï¼šç”Ÿæˆé€Ÿåº¦ææ…¢</h4>
        
        <div class="code-block">
<pre># æ€§èƒ½åˆ†æå·¥å…·
class SamplingProfiler:
    """é‡‡æ ·æ€§èƒ½åˆ†æå™¨"""
    def __init__(self):
        self.timings = defaultdict(list)
    
    def profile_sampling(self, model, shape=(1, 3, 32, 32), num_steps=1000):
        """è¯¦ç»†åˆ†æé‡‡æ ·å„é˜¶æ®µè€—æ—¶"""
        import time
        
        # åˆå§‹åŒ–
        start_total = time.time()
        x_t = torch.randn(shape, device='cuda')
        
        # é¢„è®¡ç®—
        start_precompute = time.time()
        betas = linear_beta_schedule(num_steps).cuda()
        alphas = 1 - betas
        alphas_bar = torch.cumprod(alphas, dim=0)
        sqrt_alphas = torch.sqrt(alphas)
        sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
        self.timings['precompute'].append(time.time() - start_precompute)
        
        # é‡‡æ ·å¾ªç¯
        for t in reversed(range(num_steps)):
            # æ¨¡å‹æ¨ç†
            start_inference = time.time()
            t_tensor = torch.full((shape[0],), t, device='cuda', dtype=torch.long)
            with torch.no_grad():
                epsilon_pred = model(x_t, t_tensor)
            torch.cuda.synchronize()
            self.timings['inference'].append(time.time() - start_inference)
            
            # æ›´æ–°è®¡ç®—
            start_update = time.time()
            mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]
            if t > 0:
                noise = torch.randn_like(x_t)
                std = torch.sqrt(betas[t])
                x_t = mean + std * noise
            else:
                x_t = mean
            torch.cuda.synchronize()
            self.timings['update'].append(time.time() - start_update)
        
        total_time = time.time() - start_total
        
        # åˆ†æç»“æœ
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"é¢„è®¡ç®—: {sum(self.timings['precompute']):.3f}ç§’")
        print(f"æ¨¡å‹æ¨ç†: {sum(self.timings['inference']):.2f}ç§’ "
              f"({np.mean(self.timings['inference'])*1000:.1f}ms/æ­¥)")
        print(f"æ›´æ–°è®¡ç®—: {sum(self.timings['update']):.2f}ç§’ "
              f"({np.mean(self.timings['update'])*1000:.1f}ms/æ­¥)")
        
        # ä¼˜åŒ–å»ºè®®
        if np.mean(self.timings['inference']) > 0.05:  # 50ms
            print("\nä¼˜åŒ–å»ºè®®:")
            print("  1. è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            print("  2. å¯ç”¨æ··åˆç²¾åº¦æ¨ç†")
            print("  3. ä½¿ç”¨torch.compile()ä¼˜åŒ–")
            print("  4. è€ƒè™‘DDIMç­‰å¿«é€Ÿé‡‡æ ·æ–¹æ³•")</pre>
        </div>
        
        <h4>é—®é¢˜4ï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰</h4>
        
        <div class="code-block">
<pre># å†…å­˜å‹å¥½çš„æ‰¹é‡é‡‡æ ·
def memory_efficient_batch_sampling(model, total_samples, batch_size=16, 
                                  image_shape=(3, 32, 32)):
    """å†…å­˜é«˜æ•ˆçš„æ‰¹é‡é‡‡æ ·"""
    all_samples = []
    
    # åˆ†æ‰¹ç”Ÿæˆ
    num_batches = (total_samples + batch_size - 1) // batch_size
    
    for i in tqdm(range(num_batches), desc="Batch sampling"):
        current_batch_size = min(batch_size, total_samples - i * batch_size)
        shape = (current_batch_size,) + image_shape
        
        # ç”Ÿæˆå½“å‰æ‰¹æ¬¡
        with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
            samples = ddpm_sample(model, shape)
        
        # ç«‹å³ç§»åˆ°CPUä»¥é‡Šæ”¾GPUå†…å­˜
        all_samples.append(samples.cpu())
        
        # æ¸…ç†GPUç¼“å­˜
        if i % 10 == 0:
            torch.cuda.empty_cache()
    
    return torch.cat(all_samples, dim=0)

# è¯Šæ–­å†…å­˜ä½¿ç”¨
def diagnose_memory_usage(model, batch_sizes=[1, 2, 4, 8, 16]):
    """è¯Šæ–­ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨"""
    import gc
    
    for bs in batch_sizes:
        torch.cuda.empty_cache()
        gc.collect()
        
        try:
            # è®°å½•åˆå§‹å†…å­˜
            init_mem = torch.cuda.memory_allocated() / 1024**3
            
            # å°è¯•é‡‡æ ·
            shape = (bs, 3, 256, 256)  # ä½¿ç”¨è¾ƒå¤§å°ºå¯¸æµ‹è¯•
            _ = ddpm_sample(model, shape, num_timesteps=50)  # åªæµ‹è¯•50æ­¥
            
            # è®°å½•å³°å€¼å†…å­˜
            peak_mem = torch.cuda.max_memory_allocated() / 1024**3
            
            print(f"Batch size {bs}: å³°å€¼å†…å­˜ {peak_mem:.2f}GB "
                  f"(å¢åŠ  {peak_mem - init_mem:.2f}GB)")
            
        except torch.cuda.OutOfMemoryError:
            print(f"Batch size {bs}: OOM!")
            break
        finally:
            torch.cuda.empty_cache()</pre>
        </div>
        
        <h4>å¯è§†åŒ–è°ƒè¯•å·¥å…·</h4>
        
        <div class="code-block">
<pre>def visualize_sampling_debug(model, save_path="debug_sampling.png"):
    """å¯è§†åŒ–é‡‡æ ·è¿‡ç¨‹ç”¨äºè°ƒè¯•"""
    import matplotlib.pyplot as plt
    from matplotlib.gridspec import GridSpec
    
    # è®¾ç½®
    shape = (1, 3, 32, 32)
    checkpoints = [999, 800, 600, 400, 200, 100, 50, 20, 10, 0]
    
    # æ”¶é›†æ•°æ®
    x_t = torch.randn(shape, device='cuda')
    x_t_history = [x_t.cpu()]
    pred_x0_history = []
    noise_pred_history = []
    
    # é‡‡æ ·å¹¶è®°å½•
    betas = linear_beta_schedule(1000).cuda()
    alphas = 1 - betas
    alphas_bar = torch.cumprod(alphas, dim=0)
    
    for t in reversed(range(1000)):
        t_tensor = torch.tensor([t], device='cuda')
        
        # é¢„æµ‹
        epsilon_pred = model(x_t, t_tensor)
        
        # é¢„æµ‹çš„x_0
        pred_x0 = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])
        
        # æ›´æ–°
        mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])
        if t > 0:
            noise = torch.randn_like(x_t)
            x_t = mean + torch.sqrt(betas[t]) * noise
        else:
            x_t = mean
        
        # è®°å½•æ£€æŸ¥ç‚¹
        if t in checkpoints:
            x_t_history.append(x_t.cpu())
            pred_x0_history.append(pred_x0.cpu())
            noise_pred_history.append(epsilon_pred.cpu())
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = plt.figure(figsize=(15, 10))
    gs = GridSpec(4, len(checkpoints), figure=fig)
    
    # ç»˜åˆ¶x_t
    for i, (t, x) in enumerate(zip(checkpoints, x_t_history)):
        ax = fig.add_subplot(gs[0, i])
        img = (x[0].clamp(-1, 1) + 1) / 2
        ax.imshow(img.permute(1, 2, 0))
        ax.set_title(f't={t}')
        ax.axis('off')
    
    # ç»˜åˆ¶é¢„æµ‹çš„x_0
    for i, (t, x0) in enumerate(zip(checkpoints[:-1], pred_x0_history)):
        ax = fig.add_subplot(gs[1, i])
        img = (x0[0].clamp(-1, 1) + 1) / 2
        ax.imshow(img.permute(1, 2, 0))
        ax.set_title(f'pred x_0')
        ax.axis('off')
    
    # ç»˜åˆ¶å™ªå£°é¢„æµ‹çš„ç»Ÿè®¡
    ax = fig.add_subplot(gs[2:, :])
    t_values = checkpoints[:-1]
    means = [eps.mean().item() for eps in noise_pred_history]
    stds = [eps.std().item() for eps in noise_pred_history]
    
    ax.plot(t_values, means, 'b-', label='Mean')
    ax.plot(t_values, stds, 'r-', label='Std')
    ax.axhline(y=0, color='b', linestyle='--', alpha=0.5)
    ax.axhline(y=1, color='r', linestyle='--', alpha=0.5)
    ax.set_xlabel('Time step')
    ax.set_ylabel('Statistics')
    ax.set_title('Noise Prediction Statistics')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.invert_xaxis()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"è°ƒè¯•å¯è§†åŒ–å·²ä¿å­˜åˆ° {save_path}")</pre>
        </div>
        
        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.5.3ï¼šå®ç°é‡‡æ ·è´¨é‡è¯Šæ–­å·¥å…·</div>
            <p>åˆ›å»ºä¸€ä¸ªç»¼åˆè¯Šæ–­å·¥å…·ï¼Œèƒ½å¤Ÿï¼š</p>
            <ol>
                <li>è‡ªåŠ¨æ£€æµ‹å¸¸è§çš„é‡‡æ ·é—®é¢˜</li>
                <li>ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š</li>
                <li>æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®</li>
            </ol>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_5_3')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_5_3" class="answer">
                <pre>class DDPMSamplingDiagnostics:
    """DDPMé‡‡æ ·ç»¼åˆè¯Šæ–­å·¥å…·"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.diagnostics = {}
        
    def run_full_diagnostics(self, num_samples=5):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("=== DDPMé‡‡æ ·è¯Šæ–­å¼€å§‹ ===\n")
        
        # 1. æ¨¡å‹åŸºç¡€æ£€æŸ¥
        self._check_model_basics()
        
        # 2. å™ªå£°è°ƒåº¦æ£€æŸ¥
        self._check_noise_schedule()
        
        # 3. é‡‡æ ·è´¨é‡æ£€æŸ¥
        self._check_sampling_quality(num_samples)
        
        # 4. æ€§èƒ½æ£€æŸ¥
        self._check_performance()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
        
    def _check_model_basics(self):
        """æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®"""
        print("1. æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®...")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨evalæ¨¡å¼
        if self.model.training:
            self.diagnostics['model_mode'] = 'WARNING: æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼'
        else:
            self.diagnostics['model_mode'] = 'OK: æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼'
        
        # æ£€æŸ¥å‚æ•°ç»Ÿè®¡
        params = []
        for p in self.model.parameters():
            params.append(p.data.flatten())
        params = torch.cat(params)
        
        param_mean = params.mean().item()
        param_std = params.std().item()
        
        if abs(param_mean) > 1.0 or param_std > 10.0:
            self.diagnostics['param_stats'] = f'WARNING: å‚æ•°ç»Ÿè®¡å¼‚å¸¸ (mean={param_mean:.3f}, std={param_std:.3f})'
        else:
            self.diagnostics['param_stats'] = 'OK: å‚æ•°ç»Ÿè®¡æ­£å¸¸'
            
    def _check_noise_schedule(self):
        """æ£€æŸ¥å™ªå£°è°ƒåº¦"""
        print("2. æ£€æŸ¥å™ªå£°è°ƒåº¦...")
        
        betas = linear_beta_schedule(1000)
        alphas_bar = torch.cumprod(1 - betas, dim=0)
        
        # æ£€æŸ¥ç«¯ç‚¹
        if alphas_bar[0] < 0.99:
            self.diagnostics['schedule_start'] = f'WARNING: Î±Ì…_0={alphas_bar[0]:.4f} å¤ªå°'
        else:
            self.diagnostics['schedule_start'] = 'OK: èµ·å§‹ç‚¹æ­£å¸¸'
            
        if alphas_bar[-1] > 0.01:
            self.diagnostics['schedule_end'] = f'WARNING: Î±Ì…_T={alphas_bar[-1]:.4f} å¤ªå¤§'
        else:
            self.diagnostics['schedule_end'] = 'OK: ç»ˆç‚¹æ­£å¸¸'
            
    def _check_sampling_quality(self, num_samples):
        """æ£€æŸ¥é‡‡æ ·è´¨é‡"""
        print(f"3. æ£€æŸ¥é‡‡æ ·è´¨é‡ (ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬)...")
        
        samples = []
        for _ in range(num_samples):
            sample = ddpm_sample(self.model, (1, 3, 32, 32), device=self.device)
            samples.append(sample)
        samples = torch.cat(samples)
        
        # æ£€æŸ¥è¾“å‡ºèŒƒå›´
        sample_min = samples.min().item()
        sample_max = samples.max().item()
        
        if sample_min < -3 or sample_max > 3:
            self.diagnostics['output_range'] = f'WARNING: è¾“å‡ºèŒƒå›´å¼‚å¸¸ [{sample_min:.2f}, {sample_max:.2f}]'
        else:
            self.diagnostics['output_range'] = 'OK: è¾“å‡ºèŒƒå›´æ­£å¸¸'
            
        # æ£€æŸ¥å¤šæ ·æ€§
        if num_samples > 1:
            diversity = samples.std(dim=0).mean().item()
            if diversity < 0.1:
                self.diagnostics['diversity'] = f'WARNING: æ ·æœ¬å¤šæ ·æ€§è¿‡ä½ (std={diversity:.3f})'
            else:
                self.diagnostics['diversity'] = 'OK: æ ·æœ¬å¤šæ ·æ€§æ­£å¸¸'
                
    def _check_performance(self):
        """æ£€æŸ¥æ€§èƒ½"""
        print("4. æ£€æŸ¥æ€§èƒ½...")
        
        import time
        shape = (1, 3, 32, 32)
        
        # æµ‹è¯•å•æ­¥æ—¶é—´
        x = torch.randn(shape, device=self.device)
        t = torch.tensor([500], device=self.device)
        
        # é¢„çƒ­
        for _ in range(10):
            _ = self.model(x, t)
        torch.cuda.synchronize()
        
        # è®¡æ—¶
        start = time.time()
        for _ in range(100):
            _ = self.model(x, t)
        torch.cuda.synchronize()
        step_time = (time.time() - start) / 100
        
        total_time = step_time * 1000
        if total_time > 60:
            self.diagnostics['performance'] = f'WARNING: é¢„è®¡é‡‡æ ·æ—¶é—´è¿‡é•¿ ({total_time:.1f}ç§’)'
        else:
            self.diagnostics['performance'] = f'OK: é¢„è®¡é‡‡æ ·æ—¶é—´ {total_time:.1f}ç§’'
            
    def _generate_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        print("\n=== è¯Šæ–­æŠ¥å‘Š ===")
        
        warnings = 0
        for key, value in self.diagnostics.items():
            if value.startswith('WARNING'):
                print(f"âŒ {value}")
                warnings += 1
            else:
                print(f"âœ… {value}")
                
        print(f"\næ€»ç»“: {len(self.diagnostics)}é¡¹æ£€æŸ¥, {warnings}ä¸ªè­¦å‘Š")
        
        if warnings > 0:
            print("\nå»ºè®®çš„ä¿®å¤æ­¥éª¤:")
            if 'model_mode' in self.diagnostics and 'WARNING' in self.diagnostics['model_mode']:
                print("- è°ƒç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
            if 'schedule_end' in self.diagnostics and 'WARNING' in self.diagnostics['schedule_end']:
                print("- å¢åŠ æ€»æ—¶é—´æ­¥æ•°æˆ–è°ƒæ•´beta_end")
            if 'diversity' in self.diagnostics and 'WARNING' in self.diagnostics['diversity']:
                print("- æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¨¡å¼å´©å¡Œ")
            if 'performance' in self.diagnostics and 'WARNING' in self.diagnostics['performance']:
                print("- è€ƒè™‘ä½¿ç”¨DDIMæˆ–å…¶ä»–å¿«é€Ÿé‡‡æ ·æ–¹æ³•")

# ä½¿ç”¨ç¤ºä¾‹
diagnostics = DDPMSamplingDiagnostics(model)
diagnostics.run_full_diagnostics()</pre>
                <p><strong>è¯Šæ–­å·¥å…·çš„æ‰©å±•</strong>ï¼šå¯ä»¥æ·»åŠ æ›´å¤šæ£€æŸ¥é¡¹ï¼Œå¦‚ï¼š</p>
                <ul>
                    <li>æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†EMAæ¨¡å‹</li>
                    <li>éªŒè¯æ¡ä»¶ç”Ÿæˆçš„æ­£ç¡®æ€§</li>
                    <li>æ£€æµ‹ç‰¹å®šçš„è§†è§‰ä¼ªå½±ï¼ˆæ£‹ç›˜æ•ˆåº”ã€è‰²å½©åç§»ç­‰ï¼‰</li>
                    <li>ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„ç»Ÿè®¡å¯¹æ¯”</li>
                </ul>
            </div>
        </div>
        
        <h2>3.6 å®Œæ•´å®ç°ï¼šæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªDDPM</h2>
        <p>æœ¬èŠ‚å°†æŠŠå‰é¢å­¦åˆ°çš„æ‰€æœ‰æ¦‚å¿µæ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„DDPMå®ç°ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå¯ä»¥åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å®Œæ•´ç³»ç»Ÿã€‚</p>
        
        <h3>3.6.1 æ¨¡å‹æ¶æ„</h3>
        <p>é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªé€‚åˆDDPMçš„U-Netæ¶æ„ã€‚è¿™ä¸ªæ¶æ„éœ€è¦ï¼š</p>
        <ul>
            <li>æ¥å—å¸¦å™ªå£°çš„å›¾åƒ $x_t$ ä½œä¸ºè¾“å…¥</li>
            <li>æ¥å—æ—¶é—´æ­¥ $t$ ä½œä¸ºæ¡ä»¶ä¿¡æ¯</li>
            <li>è¾“å‡ºé¢„æµ‹çš„å™ªå£° $\epsilon_\theta(x_t, t)$</li>
        </ul>

        <div class="code-container">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SinusoidalPositionalEmbedding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç ï¼Œç”¨äºæ—¶é—´æ­¥åµŒå…¥"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    
    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings

class ResidualBlock(nn.Module):
    """å¸¦æ—¶é—´åµŒå…¥çš„æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.time_emb = nn.Linear(time_emb_dim, out_channels)
        self.dropout = nn.Dropout(dropout)
        self.norm1 = nn.GroupNorm(8, out_channels)
        self.norm2 = nn.GroupNorm(8, out_channels)
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
        
    def forward(self, x, t):
        h = self.conv1(x)
        h = self.norm1(h)
        h = F.silu(h)
        
        # æ·»åŠ æ—¶é—´åµŒå…¥
        h = h + self.time_emb(F.silu(t))[:, :, None, None]
        
        h = self.conv2(h)
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        
        return h + self.shortcut(x)

class AttentionBlock(nn.Module):
    """è‡ªæ³¨æ„åŠ›å—"""
    def __init__(self, channels, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.norm = nn.GroupNorm(8, channels)
        self.qkv = nn.Conv2d(channels, channels * 3, 1)
        self.proj = nn.Conv2d(channels, channels, 1)
        
    def forward(self, x):
        B, C, H, W = x.shape
        h = self.norm(x)
        qkv = self.qkv(h)
        q, k, v = qkv.chunk(3, dim=1)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = q.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        k = k.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        v = v.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scale = (C // self.num_heads) ** -0.5
        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * scale, dim=-1)
        out = torch.matmul(attn, v)
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        out = out.transpose(2, 3).contiguous().view(B, C, H, W)
        return x + self.proj(out)</pre>
        </div>

        <div class="important-box">
            <div class="box-title">æ¶æ„è®¾è®¡è¦ç‚¹</div>
            <ul>
                <li><strong>æ—¶é—´åµŒå…¥</strong>ï¼šä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç å°†ç¦»æ•£æ—¶é—´æ­¥è½¬æ¢ä¸ºè¿ç»­è¡¨ç¤º</li>
                <li><strong>æ®‹å·®è¿æ¥</strong>ï¼šæ¯ä¸ªå—éƒ½åŒ…å«æ®‹å·®è¿æ¥ï¼Œæœ‰åŠ©äºæ¢¯åº¦æµåŠ¨</li>
                <li><strong>æ³¨æ„åŠ›æœºåˆ¶</strong>ï¼šåœ¨ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ä¸Šä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼Œæ•è·é•¿ç¨‹ä¾èµ–</li>
                <li><strong>GroupNorm</strong>ï¼šä½¿ç”¨ç»„å½’ä¸€åŒ–è€Œéæ‰¹å½’ä¸€åŒ–ï¼Œæ›´é€‚åˆå°æ‰¹é‡è®­ç»ƒ</li>
            </ul>
        </div>

        <h4>è½»é‡çº§DDPM U-Net</h4>
        <p>å¯¹äºç®€å•ä»»åŠ¡ï¼ˆå¦‚MNISTï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ›´è½»é‡çš„æ¶æ„ï¼š</p>

        <div class="code-container">
            <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(this)">å¤åˆ¶ä»£ç </button>
            </div>
            <pre>class SimpleDDPMUNet(nn.Module):
    """è½»é‡çº§DDPM U-Netï¼Œé€‚ç”¨äºMNISTç­‰ç®€å•æ•°æ®é›†"""
    def __init__(self, image_channels=1, n_channels=32, ch_mults=(1, 2, 2, 4),
                 n_blocks=2):
        super().__init__()
        
        # æ—¶é—´åµŒå…¥
        self.time_emb = nn.Sequential(
            SinusoidalPositionalEmbedding(n_channels),
            nn.Linear(n_channels, n_channels * 4),
            nn.GELU(),
            nn.Linear(n_channels * 4, n_channels * 4)
        )
        
        # è¾“å…¥å±‚
        self.conv_in = nn.Conv2d(image_channels, n_channels, 3, padding=1)
        
        # ä¸‹é‡‡æ ·
        self.downs = nn.ModuleList()
        chs = [n_channels]
        now_ch = n_channels
        
        for i, mult in enumerate(ch_mults):
            out_ch = n_channels * mult
            for _ in range(n_blocks):
                self.downs.append(ResidualBlock(now_ch, out_ch, n_channels * 4))
                now_ch = out_ch
                chs.append(now_ch)
            
            if i < len(ch_mults) - 1:
                self.downs.append(nn.Conv2d(now_ch, now_ch, 3, stride=2, padding=1))
                chs.append(now_ch)
        
        # ä¸­é—´å±‚
        self.middle = nn.ModuleList([
            ResidualBlock(now_ch, now_ch, n_channels * 4),
            ResidualBlock(now_ch, now_ch, n_channels * 4)
        ])
        
        # ä¸Šé‡‡æ ·
        self.ups = nn.ModuleList()
        for i, mult in reversed(list(enumerate(ch_mults))):
            out_ch = n_channels * mult
            
            for _ in range(n_blocks + 1):
                self.ups.append(ResidualBlock(chs.pop() + now_ch, out_ch, n_channels * 4))
                now_ch = out_ch
            
            if i > 0:
                self.ups.append(nn.ConvTranspose2d(now_ch, now_ch, 4, stride=2, padding=1))
        
        # è¾“å‡ºå±‚
        self.conv_out = nn.Sequential(
            nn.GroupNorm(8, now_ch),
            nn.SiLU(),
            nn.Conv2d(now_ch, image_channels, 3, padding=1)
        )
    
    def forward(self, x, t):
        # è·å–æ—¶é—´åµŒå…¥
        t = self.time_emb(t)
        
        # åˆå§‹å·ç§¯
        h = self.conv_in(x)
        
        # ä¸‹é‡‡æ ·
        hs = [h]
        for layer in self.downs:
            if isinstance(layer, ResidualBlock):
                h = layer(h, t)
            else:
                h = layer(h)
            hs.append(h)
        
        # ä¸­é—´å±‚
        for layer in self.middle:
            h = layer(h, t)
        
        # ä¸Šé‡‡æ ·
        for layer in self.ups:
            if isinstance(layer, ResidualBlock):
                h = layer(torch.cat([h, hs.pop()], dim=1), t)
            else:
                h = layer(h)
        
        # è¾“å‡º
        return self.conv_out(h)</pre>
        </div>

        <div class="exercise">
            <div class="exercise-title">ç»ƒä¹  3.6.1ï¼šæ¨¡å‹å‚æ•°è®¡ç®—</div>
            <p>å®ç°ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—U-Netæ¨¡å‹çš„å‚æ•°é‡ï¼Œå¹¶æ¯”è¾ƒä¸åŒé…ç½®çš„æ¨¡å‹å¤§å°ã€‚</p>
            <button class="answer-toggle" onclick="toggleAnswer('answer3_6_1')">æ˜¾ç¤ºç­”æ¡ˆ</button>
            <div id="answer3_6_1" class="answer">
                <pre>def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def compare_model_sizes():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹é…ç½®çš„å‚æ•°é‡"""
    configs = [
        {"name": "Tiny", "n_channels": 16, "ch_mults": (1, 2, 2)},
        {"name": "Small", "n_channels": 32, "ch_mults": (1, 2, 2, 4)},
        {"name": "Base", "n_channels": 64, "ch_mults": (1, 2, 4, 8)},
        {"name": "Large", "n_channels": 128, "ch_mults": (1, 2, 4, 8)}
    ]
    
    for config in configs:
        model = SimpleDDPMUNet(
            n_channels=config["n_channels"],
            ch_mults=config["ch_mults"]
        )
        params = count_parameters(model)
        print(f"{config['name']}: {params:,} parameters ({params/1e6:.2f}M)")

# è¾“å‡ºç¤ºä¾‹ï¼š
# Tiny: 461,729 parameters (0.46M)
# Small: 3,652,481 parameters (3.65M)
# Base: 35,742,785 parameters (35.74M)
# Large: 142,836,097 parameters (142.84M)</pre>
            </div>
        </div>
        
        <h3>3.6.2 è®­ç»ƒå¾ªç¯</h3>
        <p>[å¾…å®Œæˆï¼šå®Œæ•´çš„è®­ç»ƒä»£ç ]</p>
        
        <h3>3.6.3 è¯„ä¼°ä¸å¯è§†åŒ–</h3>
        <p>[å¾…å®Œæˆï¼šFIDã€ISç­‰æŒ‡æ ‡çš„è®¡ç®—]</p>
        
        <h2>3.7 DDPMçš„å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘</h2>
        <p>[å¾…å®Œæˆï¼šé‡‡æ ·é€Ÿåº¦æ…¢ã€æ–¹å·®å›ºå®šç­‰é—®é¢˜ï¼Œå¼•å‡ºåç»­ç« èŠ‚]</p>
        
        <div class="chapter-summary">
            <h2>æœ¬ç« å°ç»“</h2>
            <p>[å¾…å®Œæˆï¼šæ€»ç»“DDPMçš„å…³é”®è´¡çŒ®ï¼Œé¢„å‘ŠDDIMç­‰æ”¹è¿›æ–¹æ³•]</p>
        </div>
    </div>
</body>
</html>