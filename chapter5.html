<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第5章：连续时间扩散模型 (PDE/SDE) - 扩散模型教程</title>
    <link rel="stylesheet" href="common.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script src="common.js"></script>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="chapter4.html">← 上一章</a>
            <span>第5章 / 共14章</span>
            <a href="chapter6.html">下一章 →</a>
        </div>
        
        <h1>第5章：连续时间扩散模型 (PDE/SDE)</h1>
        
        <div class="chapter-intro">
            到目前为止，我们学习的扩散模型都是在离散时间步上定义的。但如果我们让时间步数趋于无穷，会发生什么？答案是：我们得到了随机微分方程（SDE）！Song等人在2021年的工作"Score-Based Generative Modeling through Stochastic Differential Equations"统一了之前的所有方法，并开启了连续时间建模的新纪元。本章将深入探讨SDE框架，以及相关的概率流ODE和Fokker-Planck方程。
        </div>

        <h2>5.1 从离散到连续：极限过程</h2>
        
        <h3>5.1.1 离散扩散过程的回顾</h3>
        
        <p>在前面的章节中，我们学习了离散时间的扩散模型。让我们回顾其核心结构，为理解连续时间做准备。</p>
        
        <h4>DDPM的前向过程</h4>
        
        <p>DDPM定义了一个马尔可夫链，逐步向数据添加噪声：</p>
        
        <div class="formula">
            $$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$$
        </div>
        
        <p>其中 $\beta_t$ 是预定义的噪声调度。通过重参数化技巧，我们可以直接从 $x_0$ 采样 $x_t$：</p>
        
        <div class="formula">
            $$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$
        </div>
        
        <p>其中 $\alpha_t = 1 - \beta_t$，$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$。</p>
        
        <h4>关键观察：小步长近似</h4>
        
        <div class="note-box">
            <h4>离散步长的含义</h4>
            <p>当 $\beta_t$ 很小时，前向过程可以理解为：</p>
            <div class="formula">
                $$x_t = x_{t-1} - \frac{\beta_t}{2} x_{t-1} + \sqrt{\beta_t} z_{t-1}, \quad z_{t-1} \sim \mathcal{N}(0, I)$$
            </div>
            
            <p>这看起来像是一个离散化的方程：</p>
            <ul>
                <li>漂移项：$-\frac{\beta_t}{2} x_{t-1}$（向原点收缩）</li>
                <li>扩散项：$\sqrt{\beta_t} z_{t-1}$（添加随机性）</li>
            </ul>
        </div>
        
        <h4>Score-based模型的视角</h4>
        
        <p>在NCSN中，我们考虑不同噪声水平下的分布：</p>
        
        <div class="formula">
            $$p_\sigma(x) = \int p_{data}(x') \mathcal{N}(x; x', \sigma^2 I) dx'$$
        </div>
        
        <p>如果我们让 $\sigma$ 随时间连续变化，$\sigma(t)$，会发生什么？</p>
        
        <div class="example-box">
            <div class="example-title">离散与连续的对应</div>
            <table style="width: 100%; border-collapse: collapse; margin-top: 10px;">
                <tr style="background-color: #f0f0f0;">
                    <th style="border: 1px solid #ddd; padding: 8px;">离散时间</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">连续时间</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">含义</th>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$t \in \{0, 1, ..., T\}$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">$t \in [0, T]$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">时间变量</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$x_t - x_{t-1}$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">$dx_t$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">无穷小变化</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$\beta_t$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">$\beta(t)dt$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">噪声强度</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$z_t \sim \mathcal{N}(0, I)$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">$dW_t$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">布朗运动增量</td>
                </tr>
            </table>
        </div>
        
        <div class="code-block">
<pre># 可视化离散步长的影响
import torch
import numpy as np

def discrete_diffusion_path(x0, betas, return_all=True):
    """模拟离散扩散路径"""
    x = x0.clone()
    path = [x.clone()]
    
    for beta in betas:
        # 前向扩散步
        noise = torch.randn_like(x)
        x = np.sqrt(1 - beta) * x + np.sqrt(beta) * noise
        
        if return_all:
            path.append(x.clone())
    
    return torch.stack(path) if return_all else x

# 比较不同步数的路径
def compare_discretizations(x0, T=1.0, num_steps_list=[10, 50, 1000]):
    """比较不同离散化的效果"""
    paths = {}
    
    for num_steps in num_steps_list:
        # 线性噪声调度
        betas = torch.linspace(0.0001, 0.02, num_steps)
        
        # 调整到相同的总噪声量
        betas = betas * T / num_steps
        
        path = discrete_diffusion_path(x0, betas)
        paths[num_steps] = path
    
    # 分析路径的统计性质
    for num_steps, path in paths.items():
        final_mean = path[-1].mean()
        final_std = path[-1].std()
        print(f"步数={num_steps:4d}: 最终均值={final_mean:.3f}, 标准差={final_std:.3f}")
    
    return paths

# 演示
x0 = torch.randn(1000, 2)  # 1000个2D点
paths = compare_discretizations(x0)
print("\n观察：随着步数增加，离散路径趋于某个极限过程")</pre>
        </div>
        
        <h4>为什么需要连续时间？</h4>
        
        <div class="note-box">
            <h4>连续时间的优势</h4>
            <ol>
                <li><strong>理论优雅</strong>：可以使用强大的SDE理论工具</li>
                <li><strong>灵活采样</strong>：可以在任意时刻停止或评估</li>
                <li><strong>数值方法</strong>：可以使用高阶ODE/SDE求解器</li>
                <li><strong>统一框架</strong>：不同的离散模型成为同一SDE的不同离散化</li>
            </ol>
        </div>
        
        <h3>5.1.2 时间步趋于无穷的极限</h3>
        
        <p>现在让我们严格地推导当时间步数趋于无穷时会发生什么。这个过程揭示了SDE的自然出现。</p>
        
        <h4>极限过程的设置</h4>
        
        <p>考虑将时间区间 $[0, T]$ 分成 $N$ 份，每份长度 $\Delta t = T/N$。在离散设置中：</p>
        
        <div class="formula">
            $$x_{k+1} = \sqrt{1 - \beta_k} x_k + \sqrt{\beta_k} z_k, \quad z_k \sim \mathcal{N}(0, I)$$
        </div>
        
        <p>为了保持合理的扩散速度，我们需要让 $\beta_k = \tilde{\beta}(t_k) \Delta t$，其中 $\tilde{\beta}(t)$ 是连续函数。</p>
        
        <div class="theorem-box">
            <div class="theorem-title">关键洞察：Taylor展开</div>
            <p>当 $\Delta t \to 0$ 时，我们可以展开：</p>
            <div class="formula">
                $$\sqrt{1 - \tilde{\beta}(t)\Delta t} = 1 - \frac{\tilde{\beta}(t)}{2}\Delta t + O((\Delta t)^2)$$
            </div>
            
            <p>因此离散更新变为：</p>
            <div class="formula">
                $$x_{k+1} - x_k = -\frac{\tilde{\beta}(t_k)}{2} x_k \Delta t + \sqrt{\tilde{\beta}(t_k)\Delta t} z_k$$
            </div>
        </div>
        
        <h4>布朗运动的出现</h4>
        
        <p>关键观察：$\sqrt{\Delta t} z_k$ 在极限下收敛到布朗运动的增量！</p>
        
        <div class="note-box">
            <h4>从离散噪声到布朗运动</h4>
            <p>定义 $W_N(t) = \sum_{k=0}^{\lfloor t/\Delta t \rfloor} \sqrt{\Delta t} z_k$，则：</p>
            <ul>
                <li>$\mathbb{E}[W_N(t)] = 0$</li>
                <li>$\mathbb{E}[W_N(t)^2] = t$</li>
                <li>增量独立且正态分布</li>
            </ul>
            <p>根据Donsker定理，$W_N(t) \xrightarrow{d} W(t)$（标准布朗运动）。</p>
        </div>
        
        <h4>SDE的导出</h4>
        
        <p>取极限 $N \to \infty$（即 $\Delta t \to 0$），我们得到：</p>
        
        <div class="formula">
            $$dx_t = -\frac{\tilde{\beta}(t)}{2} x_t dt + \sqrt{\tilde{\beta}(t)} dW_t$$
        </div>
        
        <p>更一般地，我们可以写成：</p>
        
        <div class="formula">
            $$dx_t = f(x_t, t) dt + g(t) dW_t$$
        </div>
        
        <p>其中 $f(x_t, t) = -\frac{\tilde{\beta}(t)}{2} x_t$ 是漂移系数，$g(t) = \sqrt{\tilde{\beta}(t)}$ 是扩散系数。</p>
        
        <div class="example-box">
            <div class="example-title">具体例子：VP (Variance Preserving) SDE</div>
            <p>DDPM的连续时间极限给出VP-SDE：</p>
            <div class="formula">
                $$dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)} dW_t$$
            </div>
            
            <p>其中 $\beta(t)$ 是连续的噪声调度函数。常见选择：</p>
            <ul>
                <li>线性：$\beta(t) = \beta_{\min} + t(\beta_{\max} - \beta_{\min})$</li>
                <li>余弦：$\beta(t) = \pi \sin^2(\frac{t}{T} \cdot \frac{\pi}{2})$</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 数值验证：离散过程收敛到SDE
import torch
import numpy as np
from scipy.stats import kstest

class DiscreteToSDE:
    """验证离散过程收敛到SDE的数值实验"""
    
    def __init__(self, beta_fn, T=1.0):
        self.beta_fn = beta_fn
        self.T = T
    
    def discrete_evolution(self, x0, N):
        """离散扩散过程"""
        dt = self.T / N
        x = x0.clone()
        
        for k in range(N):
            t = k * dt
            beta_dt = self.beta_fn(t) * dt
            
            # 离散更新
            noise = torch.randn_like(x)
            x = np.sqrt(1 - beta_dt) * x + np.sqrt(beta_dt) * noise
        
        return x
    
    def sde_solution(self, x0, t):
        """VP-SDE的解析解（当beta为常数时）"""
        # 对于一般的beta(t)，需要数值积分
        # 这里简化为常数情况
        beta_avg = self.beta_fn(self.T/2)  # 近似
        
        mean_factor = np.exp(-0.5 * beta_avg * t)
        var_factor = 1 - np.exp(-beta_avg * t)
        
        mean = mean_factor * x0
        std = np.sqrt(var_factor)
        
        return mean, std
    
    def test_convergence(self, x0, N_values=[10, 50, 100, 500, 1000]):
        """测试收敛性"""
        n_samples = 10000
        x0_batch = x0.repeat(n_samples, 1)
        
        results = {}
        
        for N in N_values:
            # 运行离散过程
            x_final = self.discrete_evolution(x0_batch, N)
            
            # 理论分布
            mean_theory, std_theory = self.sde_solution(x0, self.T)
            
            # 比较统计量
            mean_empirical = x_final.mean(dim=0)
            std_empirical = x_final.std(dim=0)
            
            # KS检验（对第一个维度）
            x_normalized = (x_final[:, 0] - mean_theory[0]) / std_theory
            ks_stat, p_value = kstest(x_normalized.numpy(), 'norm')
            
            results[N] = {
                'mean_error': torch.norm(mean_empirical - mean_theory).item(),
                'std_error': torch.norm(std_empirical - std_theory).item(),
                'ks_stat': ks_stat,
                'p_value': p_value
            }
        
        return results

# 运行实验
def demonstrate_convergence():
    # 定义beta函数
    beta_fn = lambda t: 0.1 + 10 * t  # 线性调度
    
    # 初始点
    x0 = torch.tensor([1.0, -0.5])
    
    # 测试收敛
    tester = DiscreteToSDE(beta_fn)
    results = tester.test_convergence(x0)
    
    print("离散过程 → SDE 收敛性分析")
    print("="*60)
    print(f"{'N':>6} | {'均值误差':>10} | {'标准差误差':>10} | {'KS统计量':>10} | {'p值':>10}")
    print("-"*60)
    
    for N, res in results.items():
        print(f"{N:6d} | {res['mean_error']:10.6f} | {res['std_error']:10.6f} | "
              f"{res['ks_stat']:10.6f} | {res['p_value']:10.6f}")
    
    print("\n结论：随着N增加，离散过程的分布收敛到SDE的理论分布")

demonstrate_convergence()</pre>
        </div>
        
        <h4>数学严格性</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">收敛定理（简化版）</div>
            <p>设离散过程 $\{X^N_k\}$ 由以下递归定义：</p>
            <div class="formula">
                $$X^N_{k+1} = X^N_k + f(X^N_k, t_k)\Delta t + g(t_k)\sqrt{\Delta t} Z_k$$
            </div>
            
            <p>在适当的正则性条件下（Lipschitz连续性等），当 $N \to \infty$ 时：</p>
            <div class="formula">
                $$X^N_{\lfloor t/\Delta t \rfloor} \xrightarrow{d} X_t$$
            </div>
            
            <p>其中 $X_t$ 是SDE的解：$dX_t = f(X_t, t)dt + g(t)dW_t$。</p>
        </div>
        
        <h3>5.1.3 SDE的直观理解</h3>
        
        <p>随机微分方程（SDE）初看起来可能很抽象，但它描述的是一个非常自然的现象：带有随机扰动的动力系统。让我们通过多个角度来建立直观理解。</p>
        
        <h4>粒子运动的视角</h4>
        
        <div class="note-box">
            <h4>布朗运动的发现</h4>
            <p>1827年，植物学家Robert Brown观察到花粉在水中的无规则运动。Einstein在1905年解释了这一现象：</p>
            <ul>
                <li>花粉受到水分子的随机碰撞</li>
                <li>宏观运动 = 确定性漂移 + 随机扰动</li>
                <li>这正是SDE描述的内容！</li>
            </ul>
        </div>
        
        <p>SDE的一般形式 $dx_t = f(x_t, t)dt + g(t)dW_t$ 可以理解为：</p>
        
        <div class="example-box">
            <div class="example-title">物理类比</div>
            <table style="width: 100%; border-collapse: collapse; margin-top: 10px;">
                <tr style="background-color: #f0f0f0;">
                    <th style="border: 1px solid #ddd; padding: 8px;">SDE项</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">物理含义</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">在扩散模型中</th>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$f(x_t, t)dt$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">确定性力（如重力、摩擦）</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">向噪声分布的漂移</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$g(t)dW_t$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">随机碰撞（热运动）</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">注入的高斯噪声</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">$x_t$</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">粒子位置</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">数据点的状态</td>
                </tr>
            </table>
        </div>
        
        <h4>信号处理的视角</h4>
        
        <p>在信号处理中，SDE描述了信号如何被噪声逐渐破坏：</p>
        
        <div class="formula">
            $$\text{带噪信号}(t) = \text{衰减} \cdot \text{原始信号}(t) + \text{累积噪声}(t)$$
        </div>
        
        <p>这正对应于扩散模型的前向过程：清晰图像逐渐变成噪声。</p>
        
        <h4>概率演化的视角</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">从点到分布</div>
            <p>SDE不仅描述单个粒子的轨迹，更重要的是描述概率分布的演化：</p>
            <ul>
                <li>$x_0$ 开始是一个确定的点（或某个初始分布）</li>
                <li>随着时间推移，不确定性增加</li>
                <li>$p(x_t|x_0)$ 变得越来越分散</li>
                <li>最终收敛到某个稳态分布（如标准正态）</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 可视化SDE的直观含义
import torch
import numpy as np

class SDEVisualization:
    """通过模拟展示SDE的不同方面"""
    
    def __init__(self, drift_fn, diffusion_fn):
        """
        Args:
            drift_fn: f(x, t) - 漂移函数
            diffusion_fn: g(t) - 扩散系数函数
        """
        self.f = drift_fn
        self.g = diffusion_fn
    
    def simulate_paths(self, x0, T, dt=0.01, n_paths=100):
        """模拟多条SDE路径"""
        n_steps = int(T / dt)
        paths = torch.zeros(n_paths, n_steps + 1, x0.shape[-1])
        paths[:, 0] = x0
        
        for i in range(n_steps):
            t = i * dt
            x = paths[:, i]
            
            # Euler-Maruyama方法
            drift = self.f(x, t) * dt
            diffusion = self.g(t) * np.sqrt(dt) * torch.randn_like(x)
            
            paths[:, i + 1] = x + drift + diffusion
        
        return paths
    
    def analyze_distribution_evolution(self, x0, T, checkpoints=[0.1, 0.5, 1.0, 2.0]):
        """分析分布随时间的演化"""
        print("分布演化分析")
        print("="*50)
        
        for t in checkpoints:
            if t > T:
                continue
                
            # 模拟到时刻t
            paths = self.simulate_paths(x0, t, n_paths=10000)
            final_x = paths[:, -1]
            
            # 统计量
            mean = final_x.mean(dim=0)
            std = final_x.std(dim=0)
            
            print(f"t = {t:.1f}:")
            print(f"  均值: {mean.numpy()}")
            print(f"  标准差: {std.numpy()}")
            print(f"  数据范围: [{final_x.min():.2f}, {final_x.max():.2f}]")
            print()

# 示例1：Ornstein-Uhlenbeck过程（均值回归）
def ou_drift(x, t, theta=1.0, mu=0.0):
    """OU过程的漂移：回归到均值mu"""
    return theta * (mu - x)

def constant_diffusion(t, sigma=1.0):
    """常数扩散系数"""
    return sigma

print("示例1: Ornstein-Uhlenbeck过程（金融中的均值回归模型）")
print("-"*50)
ou_sde = SDEVisualization(ou_drift, constant_diffusion)
x0 = torch.tensor([5.0])  # 从远离均值的点开始
ou_sde.analyze_distribution_evolution(x0, T=5.0)

# 示例2：扩散模型的VP-SDE
def vp_drift(x, t, beta_min=0.1, beta_max=20.0):
    """VP-SDE的漂移"""
    beta_t = beta_min + t * (beta_max - beta_min)
    return -0.5 * beta_t * x

def vp_diffusion(t, beta_min=0.1, beta_max=20.0):
    """VP-SDE的扩散系数"""
    beta_t = beta_min + t * (beta_max - beta_min)
    return np.sqrt(beta_t)

print("\n示例2: VP-SDE（扩散模型）")
print("-"*50)
vp_sde = SDEVisualization(vp_drift, vp_diffusion)
x0 = torch.randn(2)  # 2D随机初始点
vp_sde.analyze_distribution_evolution(x0, T=1.0)</pre>
        </div>
        
        <h4>几何视角：流形上的随机游走</h4>
        
        <p>在高维空间中，SDE可以理解为数据流形上的随机游走：</p>
        
        <div class="note-box">
            <h4>数据流形的破坏与重建</h4>
            <ol>
                <li><strong>前向SDE</strong>：将数据从低维流形"推离"到整个高维空间</li>
                <li><strong>反向SDE</strong>：学习如何将散布的点"拉回"到原始流形</li>
                <li><strong>分数函数</strong>：在每个点指示回到流形的方向</li>
            </ol>
        </div>
        
        <h4>控制论视角：噪声作为正则化</h4>
        
        <div class="example-box">
            <div class="example-title">为什么要加噪声？</div>
            <p>添加噪声看似是破坏信息，但实际上有多个好处：</p>
            <ul>
                <li><strong>覆盖支撑集</strong>：确保模型见过所有可能的输入</li>
                <li><strong>平滑优化景观</strong>：避免分数函数的奇异性</li>
                <li><strong>连接数据点</strong>：在数据点之间建立概率路径</li>
                <li><strong>隐式正则化</strong>：防止模型记忆训练数据</li>
            </ul>
        </div>
        
        <h4>信息论视角：熵的增加与减少</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">熵的演化</div>
            <p>前向SDE增加熵（不确定性），反向SDE减少熵：</p>
            <div class="formula">
                $$H[p_t] = H[p_0] + \int_0^t \mathbb{E}_{p_s}\left[\frac{|g(s)|^2}{2}\right] ds$$
            </div>
            <p>这解释了为什么：</p>
            <ul>
                <li>前向过程最终收敛到最大熵分布（高斯分布）</li>
                <li>反向过程需要学习分数函数来"注入"信息</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 演示信息论视角
def entropy_evolution_demo():
    """展示熵随时间的变化"""
    import torch.distributions as dist
    
    # 初始分布：混合高斯（低熵）
    mix_weights = torch.tensor([0.3, 0.7])
    components = [
        dist.Normal(-2.0, 0.5),
        dist.Normal(2.0, 0.5)
    ]
    
    def estimate_entropy(samples):
        """估计样本的差分熵（使用KDE）"""
        # 简化：使用高斯核密度估计
        n = len(samples)
        h = 1.06 * samples.std() * (n ** (-1/5))  # Silverman's rule
        
        # 计算每个点的密度
        densities = []
        for x in samples[:100]:  # 子采样以加速
            kde = torch.exp(-0.5 * ((samples - x) / h) ** 2) / (h * np.sqrt(2 * np.pi))
            density = kde.mean()
            densities.append(density)
        
        # 熵 = -E[log p(x)]
        log_densities = torch.log(torch.tensor(densities) + 1e-10)
        entropy = -log_densities.mean()
        return entropy.item()
    
    # 模拟扩散过程
    t_values = [0, 0.1, 0.5, 1.0, 2.0]
    n_samples = 5000
    
    print("熵的演化（扩散过程）")
    print("="*40)
    
    for t in t_values:
        # 采样初始分布
        component_idx = torch.multinomial(mix_weights, n_samples, replacement=True)
        samples = torch.zeros(n_samples)
        for i, comp in enumerate(components):
            mask = component_idx == i
            samples[mask] = comp.sample((mask.sum(),))
        
        # 应用扩散（简化：直接加噪声）
        noise_scale = np.sqrt(1 - np.exp(-t))  # 对应VP-SDE
        signal_scale = np.exp(-t/2)
        
        diffused_samples = signal_scale * samples + noise_scale * torch.randn_like(samples)
        
        # 估计熵
        entropy = estimate_entropy(diffused_samples)
        
        # 理论最大熵（标准正态分布）
        max_entropy = 0.5 * np.log(2 * np.pi * np.e)
        
        print(f"t = {t:.1f}: 熵 ≈ {entropy:.3f} (最大熵 = {max_entropy:.3f})")
    
    print("\n观察：熵单调增加，趋向于高斯分布的最大熵")

entropy_evolution_demo()</pre>
        </div>
        
        <h4>实践指南：选择SDE的艺术</h4>
        
        <div class="note-box">
            <h4>不同SDE的特点</h4>
            <table style="width: 100%; border-collapse: collapse; margin-top: 10px;">
                <tr style="background-color: #f0f0f0;">
                    <th style="border: 1px solid #ddd; padding: 8px;">SDE类型</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">特点</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">适用场景</th>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">VP-SDE</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">方差保持，信号逐渐衰减</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">图像生成（DDPM类）</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">VE-SDE</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">方差爆炸，信号保持</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">分数匹配（NCSN类）</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;">sub-VP-SDE</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">介于两者之间</td>
                    <td style="border: 1px solid #ddd; padding: 8px;">通用框架</td>
                </tr>
            </table>
        </div>

        <h2>5.2 前向SDE：连续时间的扩散过程</h2>
        
        <h3>5.2.1 SDE的一般形式</h3>
        
        <p>现在让我们系统地研究用于扩散模型的SDE。我们将看到，不同的SDE选择对应于不同的离散扩散模型。</p>
        
        <h4>扩散SDE的标准形式</h4>
        
        <p>用于生成建模的前向SDE通常具有以下形式：</p>
        
        <div class="formula">
            $$dx = f(x, t) dt + g(t) dW_t$$
        </div>
        
        <p>其中：</p>
        <ul>
            <li>$x \in \mathbb{R}^d$ 是状态变量（如图像）</li>
            <li>$f: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移系数</li>
            <li>$g: [0, T] \to \mathbb{R}$ 是扩散系数（标量函数）</li>
            <li>$W_t$ 是标准布朗运动</li>
        </ul>
        
        <div class="note-box">
            <h4>为什么g(t)是标量？</h4>
            <p>在大多数扩散模型中，我们假设噪声在各个维度上是独立同分布的。这简化了理论分析和实际实现。更一般的情况下，$g(t)$ 可以是矩阵值函数。</p>
        </div>
        
        <h4>边缘分布的演化</h4>
        
        <p>给定初始分布 $p_0(x) = p_{data}(x)$，SDE诱导了一个时变的边缘分布 $p_t(x)$。我们希望：</p>
        
        <div class="theorem-box">
            <div class="theorem-title">设计目标</div>
            <ol>
                <li><strong>覆盖数据分布</strong>：$p_0(x) = p_{data}(x)$</li>
                <li><strong>收敛到已知分布</strong>：$p_T(x) \approx \pi(x)$，其中 $\pi$ 是易于采样的先验分布</li>
                <li><strong>平滑过渡</strong>：$p_t$ 随 $t$ 连续变化</li>
                <li><strong>可逆性</strong>：存在反向SDE从 $p_T$ 回到 $p_0$</li>
            </ol>
        </div>
        
        <h4>三种经典SDE家族</h4>
        
        <p>Song等人(2021)总结了三种主要的SDE家族：</p>
        
        <div class="example-box">
            <div class="example-title">1. Variance Exploding (VE) SDE</div>
            <div class="formula">
                $$dx = \sqrt{\frac{d[\sigma^2(t)]}{dt}} dW_t$$
            </div>
            <p>特点：</p>
            <ul>
                <li>没有漂移项（$f(x,t) = 0$）</li>
                <li>方差随时间增加：$\mathbb{E}[||x_t||^2] = ||x_0||^2 + \sigma^2(t)$</li>
                <li>对应于NCSN中的噪声注入过程</li>
            </ul>
        </div>
        
        <div class="example-box">
            <div class="example-title">2. Variance Preserving (VP) SDE</div>
            <div class="formula">
                $$dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)} dW_t$$
            </div>
            <p>特点：</p>
            <ul>
                <li>线性漂移项使信号衰减</li>
                <li>在适当的 $\beta(t)$ 下，方差保持接近常数</li>
                <li>对应于DDPM的连续时间扩展</li>
            </ul>
        </div>
        
        <div class="example-box">
            <div class="example-title">3. Sub-VP SDE</div>
            <div class="formula">
                $$dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)(1-e^{-2\int_0^t \beta(s)ds})} dW_t$$
            </div>
            <p>特点：</p>
            <ul>
                <li>漂移项与VP-SDE相同</li>
                <li>扩散系数被调整以确保良好的收敛性质</li>
                <li>提供更灵活的框架</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 实现三种SDE家族
import torch
import numpy as np
from abc import ABC, abstractmethod

class SDE(ABC):
    """SDE基类"""
    
    def __init__(self, T=1.0):
        self.T = T
    
    @abstractmethod
    def drift(self, x, t):
        """漂移系数 f(x,t)"""
        pass
    
    @abstractmethod
    def diffusion(self, t):
        """扩散系数 g(t)"""
        pass
    
    @abstractmethod
    def marginal_prob(self, x0, t):
        """边缘分布 p(x_t|x_0) 的均值和标准差"""
        pass
    
    def sample_trajectory(self, x0, n_steps=1000):
        """使用Euler-Maruyama方法采样轨迹"""
        dt = self.T / n_steps
        trajectory = [x0]
        x = x0.clone()
        
        for i in range(n_steps):
            t = i * dt
            drift = self.drift(x, t) * dt
            diffusion = self.diffusion(t) * np.sqrt(dt) * torch.randn_like(x)
            x = x + drift + diffusion
            trajectory.append(x.clone())
        
        return torch.stack(trajectory)

class VESDE(SDE):
    """Variance Exploding SDE"""
    
    def __init__(self, sigma_min=0.01, sigma_max=50.0, T=1.0):
        super().__init__(T)
        self.sigma_min = sigma_min
        self.sigma_max = sigma_max
    
    def sigma(self, t):
        """噪声调度函数"""
        return self.sigma_min * (self.sigma_max / self.sigma_min) ** (t / self.T)
    
    def drift(self, x, t):
        return torch.zeros_like(x)
    
    def diffusion(self, t):
        sigma_t = self.sigma(t)
        # d\sigma^2/dt = 2\sigma d\sigma/dt
        return sigma_t * np.sqrt(2 * np.log(self.sigma_max / self.sigma_min) / self.T)
    
    def marginal_prob(self, x0, t):
        sigma_t = self.sigma(t)
        mean = x0
        std = sigma_t
        return mean, std

class VPSDE(SDE):
    """Variance Preserving SDE"""
    
    def __init__(self, beta_min=0.1, beta_max=20.0, T=1.0):
        super().__init__(T)
        self.beta_min = beta_min
        self.beta_max = beta_max
    
    def beta(self, t):
        """线性噪声调度"""
        return self.beta_min + (self.beta_max - self.beta_min) * t / self.T
    
    def drift(self, x, t):
        return -0.5 * self.beta(t) * x
    
    def diffusion(self, t):
        return np.sqrt(self.beta(t))
    
    def marginal_prob(self, x0, t):
        # 线性SDE的解析解
        log_mean_coeff = -0.25 * t**2 * (self.beta_max - self.beta_min) / self.T - 0.5 * t * self.beta_min
        mean = torch.exp(log_mean_coeff) * x0
        std = torch.sqrt(1 - torch.exp(2 * log_mean_coeff))
        return mean, std

class SubVPSDE(SDE):
    """Sub-VP SDE"""
    
    def __init__(self, beta_min=0.1, beta_max=20.0, T=1.0):
        super().__init__(T)
        self.beta_min = beta_min
        self.beta_max = beta_max
    
    def beta(self, t):
        return self.beta_min + (self.beta_max - self.beta_min) * t / self.T
    
    def drift(self, x, t):
        return -0.5 * self.beta(t) * x
    
    def diffusion(self, t):
        # 简化：使用近似积分
        integral = 0.5 * t**2 * (self.beta_max - self.beta_min) / self.T + t * self.beta_min
        return np.sqrt(self.beta(t) * (1 - np.exp(-2 * integral)))
    
    def marginal_prob(self, x0, t):
        # 与VP-SDE相同的边缘均值
        log_mean_coeff = -0.25 * t**2 * (self.beta_max - self.beta_min) / self.T - 0.5 * t * self.beta_min
        mean = torch.exp(log_mean_coeff) * x0
        # 但标准差不同
        integral = 0.5 * t**2 * (self.beta_max - self.beta_min) / self.T + t * self.beta_min
        std = torch.sqrt(1 - torch.exp(-integral))
        return mean, std

# 比较不同SDE的性质
def compare_sdes():
    """比较三种SDE的边缘分布"""
    x0 = torch.randn(2)  # 2D初始点
    
    sdes = {
        'VE-SDE': VESDE(),
        'VP-SDE': VPSDE(),
        'Sub-VP': SubVPSDE()
    }
    
    t_values = torch.linspace(0, 1.0, 5)
    
    print("不同SDE的边缘分布演化")
    print("="*70)
    print(f"{'SDE类型':^10} | {'t':^5} | {'均值范数':^12} | {'标准差':^12} | {'信噪比':^12}")
    print("-"*70)
    
    for name, sde in sdes.items():
        for t in t_values:
            mean, std = sde.marginal_prob(x0, t)
            mean_norm = torch.norm(mean)
            snr = mean_norm / (std + 1e-8)  # 信噪比
            
            print(f"{name:^10} | {t:5.2f} | {mean_norm:12.6f} | {std:12.6f} | {snr:12.6f}")
        print("-"*70)

compare_sdes()</pre>
        </div>
        
        <h4>从ODE视角理解SDE</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">确定性 vs 随机性</div>
            <p>SDE可以看作是ODE加上随机扰动：</p>
            <div class="formula">
                $$\underbrace{dx = f(x,t)dt}_{\text{ODE部分}} + \underbrace{g(t)dW_t}_{\text{随机扰动}}$$
            </div>
            
            <p>这种分解有助于：</p>
            <ul>
                <li>理解概率流ODE（去除随机项后的确定性动力学）</li>
                <li>设计数值求解器（借鉴ODE方法）</li>
                <li>分析稳定性和收敛性</li>
            </ul>
        </div>
        
        <h4>选择SDE的实用指南</h4>
        
        <div class="note-box">
            <h4>如何选择适合的SDE？</h4>
            <ol>
                <li><strong>VE-SDE</strong>：
                    <ul>
                        <li>适合高分辨率图像</li>
                        <li>保留原始信号结构</li>
                        <li>但最终分布难以控制</li>
                    </ul>
                </li>
                <li><strong>VP-SDE</strong>：
                    <ul>
                        <li>最终收敛到标准正态</li>
                        <li>理论分析更简单</li>
                        <li>与DDPM兼容</li>
                    </ul>
                </li>
                <li><strong>Sub-VP-SDE</strong>：
                    <ul>
                        <li>更灵活的框架</li>
                        <li>可以调节收敛速度</li>
                        <li>数值稳定性更好</li>
                    </ul>
                </li>
            </ol>
        </div>
        
        <h3>5.2.2 常见的SDE选择</h3>
        
        <p>在实践中，选择合适的SDE至关重要。不同的选择会影响模型的训练稳定性、生成质量和采样效率。让我们深入探讨实际应用中的SDE设计。</p>
        
        <h4>噪声调度的设计</h4>
        
        <p>SDE的核心是噪声调度函数，它控制着扩散过程的速度和特性。</p>
        
        <div class="example-box">
            <div class="example-title">常见的噪声调度</div>
            <ol>
                <li><strong>线性调度</strong>（Linear Schedule）
                    <div class="formula">$$\beta(t) = \beta_{\text{min}} + t(\beta_{\text{max}} - \beta_{\text{min}})$$</div>
                    <ul>
                        <li>简单直观</li>
                        <li>DDPM的原始选择</li>
                        <li>可能在开始时太快，结束时太慢</li>
                    </ul>
                </li>
                
                <li><strong>余弦调度</strong>（Cosine Schedule）
                    <div class="formula">$$\bar{\alpha}(t) = \cos\left(\frac{t/T + s}{1 + s} \cdot \frac{\pi}{2}\right)^2$$</div>
                    <ul>
                        <li>由Nichol & Dhariwal (2021)提出</li>
                        <li>在整个过程中更均匀地破坏信息</li>
                        <li>特别适合高分辨率图像</li>
                    </ul>
                </li>
                
                <li><strong>二次调度</strong>（Quadratic Schedule）
                    <div class="formula">$$\beta(t) = \beta_{\text{min}} + (\beta_{\text{max}} - \beta_{\text{min}})t^2$$</div>
                    <ul>
                        <li>开始时缓慢，后期加速</li>
                        <li>保留更多的早期信息</li>
                    </ul>
                </li>
            </ol>
        </div>
        
        <div class="code-block">
<pre># 实现和比较不同的噪声调度
import torch
import numpy as np

class NoiseSchedule:
    """噪声调度的基类"""
    
    def __init__(self, T=1.0):
        self.T = T
    
    def beta(self, t):
        """返回时刻t的beta值"""
        raise NotImplementedError
    
    def alpha_bar(self, t):
        """返回累积alpha值"""
        # 对于连续时间，需要积分
        # 这里使用数值近似
        n_steps = 1000
        dt = t / n_steps
        alpha_bar = 1.0
        
        for i in range(n_steps):
            t_i = i * dt
            alpha_bar *= (1 - self.beta(t_i) * dt)
        
        return alpha_bar
    
    def snr(self, t):
        """信噪比 (Signal-to-Noise Ratio)"""
        alpha_bar = self.alpha_bar(t)
        return alpha_bar / (1 - alpha_bar + 1e-8)

class LinearSchedule(NoiseSchedule):
    """线性噪声调度"""
    
    def __init__(self, beta_min=0.0001, beta_max=0.02, T=1.0):
        super().__init__(T)
        self.beta_min = beta_min
        self.beta_max = beta_max
    
    def beta(self, t):
        return self.beta_min + (t / self.T) * (self.beta_max - self.beta_min)

class CosineSchedule(NoiseSchedule):
    """余弦噪声调度"""
    
    def __init__(self, s=0.008, T=1.0):
        super().__init__(T)
        self.s = s
    
    def alpha_bar(self, t):
        # 直接定义alpha_bar而不是beta
        return np.cos((t / self.T + self.s) / (1 + self.s) * np.pi / 2) ** 2
    
    def beta(self, t):
        # 从alpha_bar推导beta
        dt = 1e-5
        alpha_bar_t = self.alpha_bar(t)
        alpha_bar_t_dt = self.alpha_bar(min(t + dt, self.T))
        
        # beta = 1 - alpha_t = 1 - alpha_bar_t / alpha_bar_{t-1}
        return 1 - alpha_bar_t_dt / (alpha_bar_t + 1e-8)

class QuadraticSchedule(NoiseSchedule):
    """二次噪声调度"""
    
    def __init__(self, beta_min=0.0001, beta_max=0.02, T=1.0):
        super().__init__(T)
        self.beta_min = beta_min
        self.beta_max = beta_max
    
    def beta(self, t):
        return self.beta_min + (t / self.T) ** 2 * (self.beta_max - self.beta_min)

# 比较不同调度的特性
def compare_schedules():
    """可视化和比较不同的噪声调度"""
    schedules = {
        'Linear': LinearSchedule(),
        'Cosine': CosineSchedule(),
        'Quadratic': QuadraticSchedule()
    }
    
    t_values = np.linspace(0, 1.0, 11)
    
    print("噪声调度比较")
    print("="*80)
    print(f"{'Schedule':^10} | {'t':^5} | {'beta(t)':^10} | {'alpha_bar(t)':^12} | {'SNR':^10} | {'log10(SNR)':^10}")
    print("-"*80)
    
    for name, schedule in schedules.items():
        for t in t_values:
            beta_t = schedule.beta(t)
            alpha_bar_t = schedule.alpha_bar(t)
            snr_t = schedule.snr(t)
            log_snr = np.log10(snr_t + 1e-10)
            
            print(f"{name:^10} | {t:5.2f} | {beta_t:10.6f} | {alpha_bar_t:12.6f} | {snr_t:10.2f} | {log_snr:10.2f}")
        print("-"*80)
    
    # 分析关键指标
    print("\n关键观察：")
    print("1. Linear: SNR下降最快，可能导致早期信息丢失过快")
    print("2. Cosine: SNR下降更均匀，在中间阶段保留更多信息")
    print("3. Quadratic: 早期保留最多信息，后期快速下降")

compare_schedules()</pre>
        </div>
        
        <h4>离散化与连续时间的对应</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">从离散到连续的映射</div>
            <p>给定离散扩散模型的参数 $\{\beta_i\}_{i=1}^T$，如何构造对应的连续SDE？</p>
            
            <ol>
                <li><strong>时间映射</strong>：将离散步骤 $i \in \{1, ..., T\}$ 映射到连续时间 $t \in [0, 1]$：
                    <div class="formula">$$t = i/T$$</div>
                </li>
                
                <li><strong>插值beta函数</strong>：
                    <div class="formula">$$\beta(t) = T \cdot \beta_{\lfloor tT \rfloor}$$</div>
                    需要乘以T来保持正确的时间尺度。
                </li>
                
                <li><strong>验证等价性</strong>：确保离散采样和SDE模拟给出相似的边缘分布。</li>
            </ol>
        </div>
        
        <h4>特殊SDE设计</h4>
        
        <div class="example-box">
            <div class="example-title">1. 保持数据范围的SDE</div>
            <p>对于图像数据（通常在[-1, 1]范围内），我们可能希望设计保持这个范围的SDE：</p>
            
            <div class="formula">
                $$dx = -\frac{\beta(t)}{2}(x - \tanh(x))dt + \sqrt{\beta(t)} dW_t$$
            </div>
            
            <p>这里的非线性漂移项 $\tanh(x)$ 在边界附近提供"推力"，防止样本逃离有效范围。</p>
        </div>
        
        <div class="example-box">
            <div class="example-title">2. 条件SDE</div>
            <p>对于条件生成，我们可以修改SDE以包含条件信息 $y$：</p>
            
            <div class="formula">
                $$dx = f(x, t, y)dt + g(t)dW_t$$
            </div>
            
            <p>常见选择：</p>
            <ul>
                <li>条件漂移：$f(x, t, y) = -\frac{\beta(t)}{2}x + h(y, t)$</li>
                <li>条件扩散：$g(t, y) = \sqrt{\beta(t)} \cdot \sigma(y)$</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 特殊SDE的实现
class BoundedSDE(SDE):
    """保持数据在有界范围内的SDE"""
    
    def __init__(self, beta_fn, bounds=(-1, 1), T=1.0):
        super().__init__(T)
        self.beta_fn = beta_fn
        self.lower, self.upper = bounds
        self.range = self.upper - self.lower
    
    def drift(self, x, t):
        beta_t = self.beta_fn(t)
        # 归一化到[-1, 1]
        x_norm = 2 * (x - self.lower) / self.range - 1
        # 非线性漂移
        drift_norm = -0.5 * beta_t * (x_norm - torch.tanh(x_norm))
        # 转换回原始范围
        return drift_norm * self.range / 2
    
    def diffusion(self, t):
        return np.sqrt(self.beta_fn(t))

class ConditionalVPSDE(SDE):
    """条件VP-SDE"""
    
    def __init__(self, beta_min=0.1, beta_max=20.0, condition_dim=128, T=1.0):
        super().__init__(T)
        self.beta_min = beta_min
        self.beta_max = beta_max
        self.condition_dim = condition_dim
        
        # 条件编码器（简化示例）
        self.condition_encoder = torch.nn.Sequential(
            torch.nn.Linear(condition_dim, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 1)
        )
    
    def beta(self, t):
        return self.beta_min + (self.beta_max - self.beta_min) * t / self.T
    
    def drift(self, x, t, condition=None):
        base_drift = -0.5 * self.beta(t) * x
        
        if condition is not None:
            # 条件调制
            with torch.no_grad():
                modulation = self.condition_encoder(condition)
                base_drift = base_drift * (1 + 0.1 * modulation)
        
        return base_drift
    
    def diffusion(self, t, condition=None):
        base_diffusion = np.sqrt(self.beta(t))
        
        if condition is not None:
            # 条件可以影响噪声强度
            return base_diffusion
        
        return base_diffusion

# 测试特殊SDE
def test_special_sdes():
    """测试特殊设计的SDE"""
    print("\n特殊SDE测试")
    print("="*60)
    
    # 1. 有界SDE
    print("1. 有界SDE（保持数据在[-1, 1]内）")
    beta_fn = lambda t: 0.1 + 10 * t
    bounded_sde = BoundedSDE(beta_fn, bounds=(-1, 1))
    
    # 测试边界行为
    x_boundary = torch.tensor([0.9, -0.9, 0.0])
    drift = bounded_sde.drift(x_boundary, 0.5)
    print(f"边界点的漂移: {drift.numpy()}")
    print("观察：接近边界的点有向内的漂移\n")
    
    # 2. 条件SDE
    print("2. 条件SDE")
    cond_sde = ConditionalVPSDE()
    
    # 不同条件下的漂移
    x = torch.randn(3)
    t = 0.5
    
    # 无条件
    drift_uncond = cond_sde.drift(x, t, condition=None)
    
    # 有条件
    condition = torch.randn(128)
    drift_cond = cond_sde.drift(x, t, condition=condition)
    
    print(f"无条件漂移: {drift_uncond.numpy()}")
    print(f"有条件漂移: {drift_cond.numpy()}")
    print(f"差异: {(drift_cond - drift_uncond).numpy()}")

test_special_sdes()</pre>
        </div>
        
        <h4>实用建议：如何选择和调试SDE</h4>
        
        <div class="note-box">
            <h4>SDE设计清单</h4>
            <ol>
                <li><strong>检查信噪比曲线</strong>
                    <ul>
                        <li>log SNR应该从正值（高信号）单调下降到负值（高噪声）</li>
                        <li>下降速度影响信息保留和生成质量的平衡</li>
                    </ul>
                </li>
                
                <li><strong>验证最终分布</strong>
                    <ul>
                        <li>$p_T(x)$应该接近先验分布（如标准正态）</li>
                        <li>可以通过蒙特卡罗模拟验证</li>
                    </ul>
                </li>
                
                <li><strong>测试数值稳定性</strong>
                    <ul>
                        <li>确保drift和diffusion项在整个时间范围内有界</li>
                        <li>避免在t=0或t=T附近出现数值问题</li>
                    </ul>
                </li>
                
                <li><strong>考虑计算效率</strong>
                    <ul>
                        <li>简单的函数形式（如线性）计算更快</li>
                        <li>复杂的调度可能提供更好的质量但增加计算成本</li>
                    </ul>
                </li>
            </ol>
        </div>
        
        <div class="example-box">
            <div class="example-title">经验法则</div>
            <ul>
                <li><strong>低分辨率图像</strong>：线性调度通常足够</li>
                <li><strong>高分辨率图像</strong>：余弦调度表现更好</li>
                <li><strong>非图像数据</strong>：可能需要专门设计的SDE</li>
                <li><strong>条件生成</strong>：考虑让条件影响噪声调度</li>
            </ul>
        </div>
        
        <h3>5.2.3 边缘分布的演化</h3>
        
        <p>理解边缘分布 $p_t(x)$ 如何随时间演化是掌握扩散模型的关键。这不仅关系到理论分析，更直接影响到实际的训练和采样。</p>
        
        <h4>边缘分布的定义</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">边缘分布</div>
            <p>给定SDE $dx_t = f(x_t, t)dt + g(t)dW_t$ 和初始分布 $p_0(x)$，边缘分布定义为：</p>
            <div class="formula">
                $$p_t(x) = \int p(x_t = x | x_0) p_0(x_0) dx_0$$
            </div>
            
            <p>其中 $p(x_t | x_0)$ 是转移核（transition kernel），描述了从 $x_0$ 到 $x_t$ 的概率转移。</p>
        </div>
        
        <h4>线性SDE的解析解</h4>
        
        <p>对于线性SDE（如VP-SDE），我们可以得到边缘分布的显式解。</p>
        
        <div class="example-box">
            <div class="example-title">以VP-SDE为例</div>
            <p>对于 $dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)} dW_t$，转移核为：</p>
            <div class="formula">
                $$p(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}(t)} x_0, (1-\bar{\alpha}(t))I)$$
            </div>
            
            <p>其中：</p>
            <div class="formula">
                $$\bar{\alpha}(t) = \exp\left(-\int_0^t \beta(s) ds\right)$$
            </div>
            
            <p>这个结果告诉我们：</p>
            <ul>
                <li>均值随时间指数衰减</li>
                <li>方差逐渐增加到接近1</li>
                <li>最终分布接近标准正态</li>
            </ul>
        </div>
        
        <div class="code-block">
<pre># 可视化边缘分布的演化
import torch
import numpy as np
from scipy import stats

class MarginalDistribution:
    """计算和分析SDE的边缘分布"""
    
    def __init__(self, sde):
        self.sde = sde
    
    def sample_marginal(self, x0, t, n_samples=1000):
        """通过蒙特卡罗采样边缘分布"""
        if hasattr(self.sde, 'marginal_prob'):
            # 如果有解析解，直接使用
            mean, std = self.sde.marginal_prob(x0, t)
            samples = mean + std * torch.randn(n_samples, *x0.shape)
        else:
            # 否则通过模拟
            samples = []
            for _ in range(n_samples):
                trajectory = self.sde.sample_trajectory(x0, n_steps=100)
                samples.append(trajectory[-1])
            samples = torch.stack(samples)
        
        return samples
    
    def analyze_evolution(self, x0, time_points):
        """分析边缘分布随时间的变化"""
        results = []
        
        for t in time_points:
            samples = self.sample_marginal(x0, t, n_samples=5000)
            
            # 统计量
            mean = samples.mean(dim=0)
            std = samples.std(dim=0)
            
            # 峰度和偏度（用于检测非高斯性）
            kurtosis = ((samples - mean) ** 4).mean() / (std ** 4) - 3
            skewness = ((samples - mean) ** 3).mean() / (std ** 3)
            
            # KL散度（与标准正态的距离）
            # 简化：使用moment matching估计
            kl_div = 0.5 * (mean.norm()**2 + std.norm()**2 - std.log().sum() - len(mean))
            
            results.append({
                't': t,
                'mean': mean.numpy(),
                'std': std.numpy(), 
                'kurtosis': kurtosis.item(),
                'skewness': skewness.item(),
                'kl_to_normal': kl_div.item()
            })
        
        return results

# 演示不同SDE的边缘分布演化
def demonstrate_marginal_evolution():
    """演示不同SDE的边缘分布演化"""
    # 初始化不同SDE
    from functools import partial
    
    # 重新定义简单的SDE类以避免循环引用
    class SimpleVPSDE:
        def __init__(self, beta_min=0.1, beta_max=20.0, T=1.0):
            self.beta_min = beta_min
            self.beta_max = beta_max
            self.T = T
        
        def marginal_prob(self, x0, t):
            log_mean_coeff = -0.25 * t**2 * (self.beta_max - self.beta_min) / self.T - 0.5 * t * self.beta_min
            mean = torch.exp(log_mean_coeff) * x0
            std = torch.sqrt(1 - torch.exp(2 * log_mean_coeff))
            return mean, std
    
    class SimpleVESDE:
        def __init__(self, sigma_min=0.01, sigma_max=50.0, T=1.0):
            self.sigma_min = sigma_min
            self.sigma_max = sigma_max
            self.T = T
        
        def marginal_prob(self, x0, t):
            sigma_t = self.sigma_min * (self.sigma_max / self.sigma_min) ** (t / self.T)
            mean = x0
            std = sigma_t
            return mean, std
    
    # 初始化
    x0 = torch.tensor([1.0, -0.5])  # 2D初始点
    time_points = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
    
    sdes = {
        'VP-SDE': SimpleVPSDE(),
        'VE-SDE': SimpleVESDE()
    }
    
    print("边缘分布演化分析")
    print("="*90)
    
    for sde_name, sde in sdes.items():
        print(f"\n{sde_name}：")
        print("-"*90)
        print(f"{'t':^5} | {'均值范数':^12} | {'标准差':^12} | {'峰度':^10} | {'偏度':^10} | {'KL散度':^12}")
        print("-"*90)
        
        analyzer = MarginalDistribution(sde)
        results = analyzer.analyze_evolution(x0, time_points)
        
        for res in results:
            mean_norm = np.linalg.norm(res['mean'])
            std_avg = np.mean(res['std'])
            
            print(f"{res['t']:5.2f} | {mean_norm:12.6f} | {std_avg:12.6f} | "
                  f"{res['kurtosis']:10.4f} | {res['skewness']:10.4f} | {res['kl_to_normal']:12.6f}")
    
    print("\n关键观察：")
    print("1. VP-SDE: 均值逐渐衰减到零，标准差趋近于1")
    print("2. VE-SDE: 均值保持不变，标准差爆炸式增长")
    print("3. 两者的峰度和偏度都接近零，说明分布接近高斯")

demonstrate_marginal_evolution()</pre>
        </div>
        
        <h4>Fokker-Planck方程：密度演化的PDE</h4>
        
        <p>边缘分布的演化可以用Fokker-Planck方程（也称为Kolmogorov前向方程）来描述：</p>
        
        <div class="theorem-box">
            <div class="theorem-title">Fokker-Planck方程</div>
            <p>对于SDE $dx = f(x,t)dt + g(t)dW_t$，概率密度 $p_t(x)$ 满足：</p>
            <div class="formula">
                $$\frac{\partial p_t(x)}{\partial t} = -\nabla \cdot (f(x,t)p_t(x)) + \frac{g(t)^2}{2} \Delta p_t(x)$$
            </div>
            
            <p>其中：</p>
            <ul>
                <li>$\nabla \cdot$ 是散度算子</li>
                <li>$\Delta$ 是拉普拉斯算子</li>
                <li>第一项是漂移项（传输）</li>
                <li>第二项是扩散项（平滑）</li>
            </ul>
        </div>
        
        <h4>分数函数与边缘分布</h4>
        
        <p>分数函数 $\nabla \log p_t(x)$ 在扩散模型中扮演着核心角色。它描述了概率密度增长最快的方向。</p>
        
        <div class="note-box">
            <h4>分数函数的性质</h4>
            <ol>
                <li><strong>梯度流</strong>：$\nabla \log p_t(x)$ 指向高概率区域</li>
                <li><strong>归一化</strong>：不需要知道归一化常数</li>
                <li><strong>光滑性</strong>：随着噪声增加，分数函数变得更平滑</li>
                <li><strong>可学习性</strong>：可以用神经网络近似</li>
            </ol>
        </div>
        
        <div class="example-box">
            <div class="example-title">特殊情况：高斯分布</div>
            <p>对于高斯分布 $p(x) = \mathcal{N}(x; \mu, \Sigma)$：</p>
            <div class="formula">
                $$\nabla \log p(x) = -\Sigma^{-1}(x - \mu)$$
            </div>
            
            <p>这是一个线性函数，指向均值点！</p>
        </div>
        
        <div class="code-block">
<pre># 分析分数函数随时间的变化
class ScoreFunctionAnalysis:
    """分析分数函数的演化"""
    
    def __init__(self, sde):
        self.sde = sde
    
    def analytical_score(self, x, x0, t):
        """计算解析分数函数（仅适用于线性SDE）"""
        if hasattr(self.sde, 'marginal_prob'):
            mean, std = self.sde.marginal_prob(x0, t)
            # 对于高斯分布：score = -(x - mean) / std^2
            score = -(x - mean) / (std ** 2 + 1e-8)
            return score
        else:
            raise NotImplementedError("需要解析边缘分布")
    
    def score_magnitude_analysis(self, x0, t_values, x_test_points):
        """分析分数函数的幅度"""
        results = []
        
        for t in t_values:
            scores = []
            for x in x_test_points:
                score = self.analytical_score(x, x0, t)
                scores.append(torch.norm(score).item())
            
            results.append({
                't': t,
                'mean_magnitude': np.mean(scores),
                'max_magnitude': np.max(scores),
                'min_magnitude': np.min(scores)
            })
        
        return results

# 演示分数函数的演化
def demonstrate_score_evolution():
    """演示分数函数随时间的变化"""
    # 使用VP-SDE
    class SimpleVPSDE:
        def __init__(self, beta_min=0.1, beta_max=20.0, T=1.0):
            self.beta_min = beta_min
            self.beta_max = beta_max
            self.T = T
        
        def marginal_prob(self, x0, t):
            log_mean_coeff = -0.25 * t**2 * (self.beta_max - self.beta_min) / self.T - 0.5 * t * self.beta_min
            mean = torch.exp(log_mean_coeff) * x0
            std = torch.sqrt(1 - torch.exp(2 * log_mean_coeff))
            return mean, std
    
    sde = SimpleVPSDE()
    analyzer = ScoreFunctionAnalysis(sde)
    
    # 设置
    x0 = torch.tensor([1.0])
    t_values = [0.1, 0.3, 0.5, 0.7, 0.9]
    x_test_points = [torch.tensor([x]) for x in np.linspace(-3, 3, 20)]
    
    print("\n分数函数幅度分析")
    print("="*60)
    print(f"{'t':^5} | {'平均幅度':^12} | {'最大幅度':^12} | {'最小幅度':^12}")
    print("-"*60)
    
    results = analyzer.score_magnitude_analysis(x0, t_values, x_test_points)
    
    for res in results:
        print(f"{res['t']:5.2f} | {res['mean_magnitude']:12.6f} | "
              f"{res['max_magnitude']:12.6f} | {res['min_magnitude']:12.6f}")
    
    print("\n观察：")
    print("1. 随着t增加，分数函数的幅度逐渐减小")
    print("2. 这反映了分布变得更加平坦（接近均匀分布）")
    print("3. 在噪声很大时，分数函数几乎为零")

demonstrate_score_evolution()</pre>
        </div>
        
        <h4>实际应用：训练时的边缘分布</h4>
        
        <div class="note-box">
            <h4>训练时的采样策略</h4>
            <p>在训练扩散模型时，我们需要：</p>
            <ol>
                <li><strong>采样时间 $t \sim \mathcal{U}[0, T]$</strong></li>
                <li><strong>采样数据点 $x_0 \sim p_{data}$</strong></li>
                <li><strong>根据 $p(x_t|x_0)$ 生成噪声样本 $x_t$</strong></li>
                <li><strong>训练模型预测噪声或分数</strong></li>
            </ol>
            
            <p>边缘分布的解析形式使得第3步变得非常高效！</p>
        </div>
        
        <div class="example-box">
            <div class="example-title">重要性采样</div>
            <p>不同的时间点对学习的难度不同。我们可以使用重要性采样：</p>
            
            <div class="formula">
                $$p(t) \propto \mathbb{E}_{x_0, x_t}[||\nabla_{x_t} \log p(x_t|x_0)||^2]$$
            </div>
            
            <p>这使得模型更多地关注"难"的时间点。</p>
        </div>

        <h2>5.3 反向时间SDE：去噪过程</h2>
        
        <h3>5.3.1 Anderson定理</h3>
        
        <p>Anderson定理是扩散模型理论的基石之一。它告诉我们，任何满足一定条件的前向SDE都存在一个对应的反向时间SDE，而这正是生成过程的数学基础。</p>
        
        <h4>定理的背景</h4>
        
        <p>在物理学中，时间反演（time reversal）是一个重要概念。对于确定性系统，时间反演通常很简单。但对于随机过程，情况就复杂得多。Anderson在1982年的工作回答了这个问题。</p>
        
        <div class="theorem-box">
            <div class="theorem-title">Anderson定理（简化版）</div>
            <p>考虑前向SDE：</p>
            <div class="formula">
                $$dx = f(x, t) dt + g(t) dW_t, \quad t \in [0, T]$$
            </div>
            
            <p>定义反向时间 $\tau = T - t$，则存在反向SDE：</p>
            <div class="formula">
                $$dx = [f(x, T-\tau) - g(T-\tau)^2 \nabla_x \log p_{T-\tau}(x)] d\tau + g(T-\tau) d\bar{W}_\tau$$
            </div>
            
            <p>其中：</p>
            <ul>
                <li>$\bar{W}_\tau$ 是关于反向时间的布朗运动</li>
                <li>$p_t(x)$ 是前向过程在时刻 $t$ 的边缘分布</li>
                <li>$\nabla_x \log p_t(x)$ 是分数函数</li>
            </ul>
        </div>
        
        <h4>直观理解</h4>
        
        <div class="note-box">
            <h4>为什么需要分数函数？</h4>
            <p>反向过程不仅仅是前向过程的"倒带"。关键差异在于：</p>
            <ol>
                <li><strong>信息不对称</strong>：前向过程丢失信息，反向过程需要恢复信息</li>
                <li><strong>概率流</strong>：反向过程需要知道"哪里来的概率更高"</li>
                <li><strong>分数作为指引</strong>：$\nabla \log p_t(x)$ 正好指向高概率方向</li>
            </ol>
        </div>
        
        <div class="example-box">
            <div class="example-title">一个简单的例子：Ornstein-Uhlenbeck过程</div>
            <p>考虑前向OU过程：</p>
            <div class="formula">
                $$dx = -\theta x dt + \sigma dW_t$$
            </div>
            
            <p>其稳态分布为 $\mathcal{N}(0, \frac{\sigma^2}{2\theta})$。分数函数为：</p>
            <div class="formula">
                $$\nabla \log p_{\infty}(x) = -\frac{2\theta}{\sigma^2} x$$
            </div>
            
            <p>因此反向SDE为：</p>
            <div class="formula">
                $$dx = \left[-\theta x - \sigma^2 \cdot \left(-\frac{2\theta}{\sigma^2} x\right)\right] d\tau + \sigma d\bar{W}_\tau = \theta x d\tau + \sigma d\bar{W}_\tau$$
            </div>
            
            <p>注意漂移项的符号变了！</p>
        </div>
        
        <div class="code-block">
<pre># 验证Anderson定理
import torch
import numpy as np

class SDEReversal:
    """验证和演示时间反演SDE"""
    
    def __init__(self, forward_drift, forward_diffusion, score_fn):
        """
        Args:
            forward_drift: f(x, t) - 前向漂移
            forward_diffusion: g(t) - 前向扩散
            score_fn: \nabla log p_t(x) - 分数函数
        """
        self.f = forward_drift
        self.g = forward_diffusion
        self.score = score_fn
    
    def reverse_drift(self, x, t, T):
        """计算反向SDE的漂移项"""
        # 时间变换
tau = t
        forward_time = T - tau
        
        # Anderson公式
        f_reverse = self.f(x, forward_time) - self.g(forward_time)**2 * self.score(x, forward_time)
        
        return f_reverse
    
    def simulate_forward_backward(self, x0, T, n_steps=100):
        """模拟前向和反向过程"""
        dt = T / n_steps
        
        # 前向过程
        forward_path = [x0]
        x = x0.clone()
        
        for i in range(n_steps):
            t = i * dt
            drift = self.f(x, t) * dt
            diffusion = self.g(t) * np.sqrt(dt) * torch.randn_like(x)
            x = x + drift + diffusion
            forward_path.append(x.clone())
        
        # 反向过程
        reverse_path = [x]
        
        for i in range(n_steps):
            tau = i * dt
            drift = self.reverse_drift(x, tau, T) * dt
            diffusion = self.g(T - tau) * np.sqrt(dt) * torch.randn_like(x)
            x = x + drift + diffusion
            reverse_path.append(x.clone())
        
        return torch.stack(forward_path), torch.stack(reverse_path)

# 示例：VP-SDE的时间反演
def demonstrate_vp_sde_reversal():
    """演示VP-SDE的时间反演"""
    # VP-SDE参数
    beta_min, beta_max = 0.1, 20.0
    T = 1.0
    
    def beta(t):
        return beta_min + (beta_max - beta_min) * t / T
    
    def forward_drift(x, t):
        return -0.5 * beta(t) * x
    
    def forward_diffusion(t):
        return np.sqrt(beta(t))
    
    def score_fn(x, t):
        # 对于VP-SDE，边缘分布是高斯的
        # p_t(x|x_0) = N(x; sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) * I)
        # 但我们需要边缘分数 \nabla log p_t(x)
        # 这在实践中是通过神经网络学习的
        # 这里用一个简化的近似
        alpha_bar = np.exp(-0.5 * beta_min * t - 0.25 * (beta_max - beta_min) * t**2 / T)
        return -x / (1 - alpha_bar + 1e-8)
    
    # 创建反演器
    reverser = SDEReversal(forward_drift, forward_diffusion, score_fn)
    
    # 模拟
    x0 = torch.randn(2)  # 2D初始点
    forward_path, reverse_path = reverser.simulate_forward_backward(x0, T, n_steps=100)
    
    # 分析结果
    print("时间反演SDE分析")
    print("="*60)
    print(f"初始点: {x0.numpy()}")
    print(f"前向终点: {forward_path[-1].numpy()}")
    print(f"反向终点: {reverse_path[-1].numpy()}")
    print(f"\n前向过程统计:")
    print(f"  初始范数: {torch.norm(forward_path[0]).item():.3f}")
    print(f"  终点范数: {torch.norm(forward_path[-1]).item():.3f}")
    print(f"\n反向过程统计:")
    print(f"  初始范数: {torch.norm(reverse_path[0]).item():.3f}")
    print(f"  终点范数: {torch.norm(reverse_path[-1]).item():.3f}")
    
    # 路径对比
    forward_norms = [torch.norm(x).item() for x in forward_path]
    reverse_norms = [torch.norm(x).item() for x in reverse_path]
    
    print(f"\n路径分析:")
    print(f"前向路径范数变化: {forward_norms[0]:.3f} → {forward_norms[-1]:.3f}")
    print(f"反向路径范数变化: {reverse_norms[0]:.3f} → {reverse_norms[-1]:.3f}")
    print("\n注意: 由于随机性，反向过程不会完美回到原点，")
    print("但会回到同样的分布！")

demonstrate_vp_sde_reversal()</pre>
        </div>
        
        <h4>数学严格性</h4>
        
        <div class="theorem-box">
            <div class="theorem-title">存在性条件</div>
            <p>Anderson定理成立需要以下条件：</p>
            <ol>
                <li><strong>正则性</strong>：$f(x,t)$ 和 $g(t)$ 满足Lipschitz条件</li>
                <li><strong>非退化性</strong>：$g(t) > 0$ 对所有 $t \in [0,T]$</li>
                <li><strong>分数存在</strong>：$\nabla \log p_t(x)$ 存在且满足适当的增长条件</li>
            </ol>
        </div>
        
        <h4>与其他理论的联系</h4>
        
        <div class="note-box">
            <h4>联系与应用</h4>
            <ol>
                <li><strong>Jarzynski等式</strong>：在非平衡统计物理中的应用</li>
                <li><strong>最优传输</strong>：Schrödinger bridge问题的特例</li>
                <li><strong>信息论</strong>：与信息熵的增减相关</li>
                <li><strong>BSDE</strong>：反向SDE可以看作一类特殊的BSDE</li>
            </ol>
        </div>
        
        <div class="example-box">
            <div class="example-title">实践意义</div>
            <p>Anderson定理对扩散模型的重要性：</p>
            <ul>
                <li><strong>理论保证</strong>：确保了反向过程的存在性</li>
                <li><strong>学习目标</strong>：明确了需要学习的是分数函数</li>
                <li><strong>采样算法</strong>：提供了从噪声生成数据的数学公式</li>
                <li><strong>理论分析</strong>：可以分析生成过程的性质</li>
            </ul>
        </div>
        
        <h3>5.3.2 反向SDE的推导</h3>
        [内容待补充]
        
        <h3>5.3.3 分数函数的作用</h3>
        [内容待补充]

        <h2>5.4 概率流ODE：确定性的替代</h2>
        
        <h3>5.4.1 从SDE到ODE</h3>
        [内容待补充]
        
        <h3>5.4.2 概率流的性质</h3>
        [内容待补充]
        
        <h3>5.4.3 ODE vs SDE：权衡与选择</h3>
        [内容待补充]

        <h2>5.5 Fokker-Planck方程：密度视角</h2>
        
        <h3>5.5.1 从粒子到密度</h3>
        [内容待补充]
        
        <h3>5.5.2 Fokker-Planck方程的推导</h3>
        [内容待补充]
        
        <h3>5.5.3 与分数函数的联系</h3>
        [内容待补充]

        <h2>5.6 统一框架：Score SDE</h2>
        
        <h3>5.6.1 VP-SDE、VE-SDE和sub-VP-SDE</h3>
        [内容待补充]
        
        <h3>5.6.2 离散模型作为SDE的特例</h3>
        [内容待补充]
        
        <h3>5.6.3 新的可能性</h3>
        [内容待补充]

        <h2>5.7 数值方法与实现</h2>
        
        <h3>5.7.1 SDE的数值解法</h3>
        [内容待补充]
        
        <h3>5.7.2 ODE求解器的应用</h3>
        [内容待补充]
        
        <h3>5.7.3 实现细节与技巧</h3>
        [内容待补充]

        <h2>5.8 理论深入</h2>
        
        <h3>5.8.1 存在性与唯一性</h3>
        [内容待补充]
        
        <h3>5.8.2 收敛性分析</h3>
        [内容待补充]
        
        <h3>5.8.3 与最优传输的联系</h3>
        [内容待补充]

        <h2>5.9 练习题</h2>
        [练习题待补充]

        <div class="chapter-summary">
            <h2>本章小结</h2>
            [小结待补充]
        </div>
    </div>
</body>
</html>