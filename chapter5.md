[← 上一章](chapter4.md) | 第5章 / 共14章 | [下一章 →](chapter6.md)

# 第5章：连续时间扩散模型 (PDE/SDE)

到目前为止，我们学习的扩散模型都是在离散时间步上定义的。但如果我们让时间步数趋于无穷，会发生什么？答案是：我们得到了一个更强大、更灵活的数学框架——随机微分方程（Stochastic Differential Equations, SDEs）。Song等人在2021年的工作《Score-Based Generative Modeling through Stochastic Differential Equations》中，将DDPM和NCSN等模型统一在SDE的视角下，开启了连续时间生成建模的新纪元。本章将深入探讨SDE框架，理解其与离散模型的联系，并介绍其对应的反向SDE、概率流ODE和Fokker-Planck方程等核心概念。

## 5.1 从离散到连续：SDE的极限之美

### 5.1.1 离散过程的极限

想象你在拍摄一个物体从清晰逐渐模糊的过程。如果你每秒拍一张照片，得到的是一个离散的序列；但如果拍摄速度越来越快，最终你会得到一个连续的视频。扩散模型从离散到连续的转变正是这样一个过程。

让我们回顾DDPM的离散前向过程：
$x_k = \sqrt{1-\beta_k} x_{k-1} + \sqrt{\beta_k} z_{k-1}, \quad z_{k-1} \sim \mathcal{N}(0, I)$

这个过程有一个美妙的物理类比：想象一滴墨水在水中扩散。每一个时间步，墨水分子都会：
1. **保持一部分原位置**：这对应 $\sqrt{1-\beta_k} x_{k-1}$ 项，表示墨水的"惯性"
2. **加入随机扰动**：这对应 $\sqrt{\beta_k} z_{k-1}$ 项，表示分子的布朗运动

当 $\beta_k$ 很小时，过程变化缓慢，我们可以用泰勒展开来近似：
$\sqrt{1 - \beta_k} \approx 1 - \frac{\beta_k}{2} - \frac{\beta_k^2}{8} + O(\beta_k^3)$

保留一阶项，更新步骤变为：
$x_k - x_{k-1} \approx -\frac{\beta_k}{2} x_{k-1} + \sqrt{\beta_k} z_{k-1}$

💡 **直觉理解**：左边是位置的变化量，右边第一项是一个"向原点的拉力"（因为系数为负），第二项是随机扰动。这就像一个被橡皮筋拴在原点的粒子，在随机力的作用下运动。

现在进行时间的连续化。将时间区间 $[0, T]$ 分成 $N$ 份，令 $\Delta t = T/N$，并设 $\beta_k = b(t_k)\Delta t$，其中 $b(t)$ 是噪声调度函数。代入后：
$\frac{x(t_k) - x(t_{k-1})}{\Delta t} \approx -\frac{b(t_{k-1})}{2} x(t_{k-1}) + \sqrt{b(t_{k-1})} \frac{z_{k-1}}{\sqrt{\Delta t}}$

这里的关键洞察是：当 $\Delta t \to 0$ 时，
- 左边收敛到导数 $\frac{dx}{dt}$
- 右边第一项保持不变
- 右边第二项 $\frac{z_{k-1}}{\sqrt{\Delta t}}$ 看起来会爆炸！

但奇妙的是，这个"爆炸"的项正是白噪声的正确缩放。根据中心极限定理的连续时间版本，当我们累加许多独立的小随机扰动时，结果收敛到布朗运动 $W_t$。具体地，$\sum_{i=1}^k \sqrt{\Delta t} z_i \to W_{t_k}$，因此 $z_k / \sqrt{\Delta t}$ 的"导数"形式正是 $dW_t/dt$，即白噪声。

最终，我们得到了随机微分方程（SDE）：
$dx_t = -\frac{b(t)}{2} x_t dt + \sqrt{b(t)} dW_t$

这就是DDPM在连续时间下的极限形式，被称为方差保持（Variance Preserving, VP）SDE。

🎯 **为什么叫"方差保持"？** 如果初始数据 $x_0$ 的方差为1，那么在适当选择 $b(t)$ 的情况下，任意时刻 $x_t$ 的方差都近似为1。这避免了数值不稳定，是VP-SDE的一大优势。

<details>
<summary><strong>深入探索：从离散到连续的数学严格性</strong></summary>

上述推导虽然直观，但数学上需要更严格的处理：

1. **收敛性**：需要证明离散过程 $\{x_k\}$ 在某种意义下（如弱收敛）收敛到连续过程 $\{x_t\}$
2. **唯一性**：需要证明极限SDE有唯一解
3. **正则性**：需要保证系数函数 $b(t)$ 满足某些条件（如Lipschitz连续性）

相关定理包括：
- **Donsker不变原理**：随机游走收敛到布朗运动
- **Stroock-Varadhan定理**：离散马尔可夫链收敛到扩散过程
- **Wong-Zakai逼近**：光滑随机过程逼近白噪声驱动的SDE

研究方向：
- 探索非标准缩放下的极限行为（如重尾噪声）
- 研究时间非均匀离散化的极限
- 分析数值误差的传播和累积

</details>

### 5.1.2 SDE的统一框架

SDE为我们提供了一个统一的语言来描述各种扩散模型。一个通用的前向SDE可以写成：
$dx_t = f(x_t, t) dt + g(t) dW_t$

这个方程包含两个关键组件：
- **漂移系数 $f(x_t, t)$**：描述了数据演化的确定性趋势，像是一个"力场"在引导数据的运动
- **扩散系数 $g(t)$**：控制着随机噪声的强度，决定了过程的随机性程度

#### 三大SDE家族的深入理解

> **定义：SDE家族**
> 
> **1. VP-SDE (Variance Preserving) - 方差保持型**
> 
> 对应DDPM，其形式为：
> $dx_t = -\frac{1}{2} \beta(t) x_t dt + \sqrt{\beta(t)} dW_t$
> 
> 其中 $\beta(t)$ 是噪声调度函数。VP-SDE的精妙之处在于：
> - **物理直觉**：像是一个弹簧振子在粘性介质中的运动，既有回复力（$-\frac{1}{2}\beta(t)x_t$），又有随机扰动
> - **方差特性**：如果 $\mathbb{E}[x_0^Tx_0] = d$（$d$ 是数据维度），那么对于适当的 $\beta(t)$，有 $\mathbb{E}[x_t^Tx_t] \approx d$
> - **数值稳定性**：避免了数值爆炸或消失，特别适合深度网络训练
> 
> **2. VE-SDE (Variance Exploding) - 方差爆炸型**
> 
> 对应NCSN，其形式为：
> $dx_t = \sqrt{\frac{d[\sigma^2(t)]}{dt}} dW_t$
> 
> 注意没有漂移项！这意味着：
> - **物理直觉**：纯粹的扩散过程，像是热传导或分子扩散
> - **方差演化**：$\mathbb{E}[||x_t||^2] = \mathbb{E}[||x_0||^2] + d\sigma^2(t)$，方差单调增长
> - **多尺度特性**：通过选择 $\sigma(t) = \sigma_{\min} \left(\frac{\sigma_{\max}}{\sigma_{\min}}\right)^t$，可以覆盖多个噪声尺度
> 
> **3. sub-VP-SDE - 次方差保持型**
> 
> 这是VP-SDE的改进版本：
> $dx_t = -\frac{1}{2} \beta(t) x_t dt + \sqrt{\beta(t)(1-e^{-2\int_0^t \beta(s)ds})} dW_t$
> 
> - **理论优势**：保证了精确的方差守恒，而不是近似
> - **实践意义**：在长时间演化中更稳定

#### SDE选择的艺术

选择哪种SDE并非随意，而是要考虑数据特性和计算效率：

1. **数据分布的考虑**：
   - 如果数据天然具有单位方差（如标准化后的图像），VP-SDE是自然选择
   - 如果数据分布在多个尺度上（如自然图像的多分辨率结构），VE-SDE可能更合适

2. **训练稳定性**：
   - VP-SDE通常更稳定，因为方差有界
   - VE-SDE需要仔细设计 $\sigma(t)$ 的增长速度

3. **采样效率**：
   - VP-SDE的轨迹更"直"，可能需要更少的采样步数
   - VE-SDE的轨迹更"曲"，但可能探索空间更充分

<details>
<summary><strong>高级话题：设计新的SDE</strong></summary>

SDE的设计空间远不止VP和VE。一些前沿研究方向包括：

1. **数据适应型SDE**：
   - 根据数据的局部几何结构调整 $f$ 和 $g$
   - 例如：$f(x,t) = -\nabla U(x,t)$，其中 $U$ 是学习到的势能函数

2. **流形上的SDE**：
   - 当数据位于低维流形上时，标准SDE可能效率低下
   - 可以设计保持在流形上的SDE：$dx_t = P_x f(x,t)dt + P_x g(t)dW_t$
   - 其中 $P_x$ 是投影到切空间的算子

3. **各向异性SDE**：
   - 让扩散系数依赖于方向：$g(t) \to G(x,t)$（矩阵值函数）
   - 可以更好地适应数据的协方差结构

4. **时间反演对称性**：
   - 设计满足某种对称性的SDE，使得前向和反向过程更相似
   - 可能导致更高效的采样

PyTorch中相关的工具：
- `torch.nn.functional.normalize` - 用于方差归一化
- `torch.autograd` - 计算分数函数
- `torchdiffeq` - 求解SDE/ODE

</details>

💡 **开放问题**：
1. **最优SDE设计**：给定数据分布，是否存在某种意义下"最优"的SDE？优化目标可能包括采样效率、训练稳定性、生成质量等。
2. **SDE的组合**：能否在不同时间段使用不同的SDE（如开始用VE探索，后期用VP精调）？
3. **离散数据的SDE**：如何为文本、图等离散数据设计合适的"连续化"SDE？

## 5.2 反向时间SDE：学习去噪

如果前向SDE描述了数据如何被噪声破坏，那么我们如何构建一个反向的过程来从噪声中恢复数据呢？这个问题的答案揭示了扩散模型的深刻数学结构。

### 5.2.1 时间反演的魔法

想象你在看一段视频：墨水在清水中扩散，从一个集中的墨滴逐渐弥漫开来。现在，如果你倒放这段视频，会看到什么？分散的墨水神奇地聚集回原点！这正是反向SDE要实现的：时间的反演。

但这里有一个关键问题：在物理世界中，扩散是不可逆的（热力学第二定律）。那么数学上如何实现这种"反熵"过程呢？答案是：我们需要额外的信息——分数函数。

> **定理：Anderson反向时间SDE (1982)**
> 
> 对于前向SDE：
> $dx = f(x, t)dt + g(t)dW_t, \quad t \in [0, T]$
> 
> 其对应的反向时间SDE（从时间 $T$ 到 $0$）为：
> $dx_t = [f(x_t, t) - g(t)^2 \nabla_{x_t} \log p_t(x_t)] dt + g(t) d\bar{W}_t$
> 
> 其中：
> - $d\bar{W}_t$ 是反向时间的布朗运动（独立于前向的 $dW_t$）
> - $\nabla_{x_t} \log p_t(x_t)$ 是时刻 $t$ 的分数函数
> - $p_t(x)$ 是前向过程在时刻 $t$ 的边缘概率密度

### 5.2.2 直觉理解：为什么需要分数？

反向SDE的漂移项可以分解为两部分：
$$\underbrace{f(x_t, t)}_{\text{原始漂移}} - \underbrace{g(t)^2 \nabla_{x_t} \log p_t(x_t)}_{\text{分数修正项}}$$

1. **原始漂移项**：如果只有这一项，时间反演后的过程仍会向同一方向演化（想象一个向下流的河流，倒放视频它还是向下流）

2. **分数修正项**：这是使过程真正反向的关键！
   - 分数 $\nabla \log p_t$ 指向概率密度增加最快的方向
   - 系数 $g(t)^2$ 确保修正强度与噪声强度匹配
   - 负号使得过程向高概率区域移动

🎯 **物理类比**：想象粒子在一个势能场中运动：
- 前向过程：粒子从势能低处（数据）滚向高处（噪声），同时受到随机扰动
- 反向过程：粒子需要知道"哪里是下坡"（分数函数），才能滚回原处

### 5.2.3 分数函数的核心地位

Anderson定理揭示了一个深刻的事实：**扩散模型的本质是学习分数函数**。这统一了看似不同的两种方法：

1. **DDPM视角**：训练网络预测噪声 $\epsilon_\theta(x_t, t)$
2. **Score Matching视角**：训练网络预测分数 $s_\theta(x_t, t) \approx \nabla \log p_t(x_t)$

它们之间的关系是：
$$s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sqrt{1 - \bar{\alpha}_t}}$$

这个关系的推导基于一个关键观察：在VP-SDE下，$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$，因此：
$$\nabla_{x_t} \log p_t(x_t) = \nabla_{x_t} \log \mathbb{E}_{x_0, \epsilon}[\delta(x_t - \sqrt{\bar{\alpha}_t} x_0 - \sqrt{1-\bar{\alpha}_t} \epsilon)] = -\frac{\epsilon}{\sqrt{1-\bar{\alpha}_t}}$$

### 5.2.4 实现细节与挑战

⚡ **实现挑战**：

1. **分数函数的参数化**：
   - 直接参数化：$s_\theta(x_t, t)$ 直接输出分数
   - 噪声参数化：$\epsilon_\theta(x_t, t)$ 预测噪声，然后转换为分数
   - 实践中噪声参数化通常更稳定

2. **时间编码**：
   - 网络需要知道当前时间 $t$ 以给出正确的分数
   - 常用方法：正弦编码、可学习的嵌入、FiLM层

3. **数值稳定性**：
   - 在 $t \approx 0$ 时，分数可能很大（概率集中）
   - 在 $t \approx T$ 时，分数接近零（接近标准正态）
   - 需要合适的归一化和数值技巧

<details>
<summary><strong>深入探索：反向SDE的推导思路</strong></summary>

Anderson定理的证明涉及高深的随机分析，但核心思想可以这样理解：

1. **Girsanov定理**：描述了如何通过改变漂移项来改变概率测度
2. **时间反演公式**：对于马尔可夫过程，存在时间反演的一般理论
3. **Doob's h-transform**：通过乘以一个正函数来构造新的马尔可夫过程

关键步骤：
- 定义反向时间过程 $\hat{x}_s = x_{T-s}$
- 使用Bayes定理计算反向转移概率
- 应用Girsanov定理得到反向SDE的形式

这个推导的美妙之处在于，它将看似不可能的任务（时间反演）转化为一个可学习的问题（估计分数函数）。

</details>

<details>
<summary><strong>练习 5.1：推导反向SDE</strong></summary>

1. **VP-SDE的反向过程**：
   给定前向VP-SDE：$dx = -\frac{1}{2} \beta(t) x dt + \sqrt{\beta(t)} dW_t$
   
   应用Anderson定理，写出反向SDE：
   $dx_t = [-\frac{1}{2} \beta(t) x_t - \beta(t) \nabla_{x_t} \log p_t(x_t)] dt + \sqrt{\beta(t)} d\bar{W}_t$
   
   简化为：$dx_t = \frac{1}{2} \beta(t) [x_t + 2\nabla_{x_t} \log p_t(x_t)] dt + \sqrt{\beta(t)} d\bar{W}_t$

2. **与DDPM的联系**：
   在DDPM框架下，$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$，其中 $\epsilon \sim \mathcal{N}(0, I)$。
   
   利用这个重参数化，证明：
   - $\nabla_{x_t} \log p_t(x_t|x_0) = -\frac{\epsilon}{\sqrt{1-\bar{\alpha}_t}}$
   - 因此，如果 $\epsilon_\theta(x_t, t) \approx \epsilon$，则 $s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sqrt{1-\bar{\alpha}_t}}$

3. **研究思路**：
   - **光滑性要求**：Anderson定理要求 $p_t$ 足够光滑。探索什么条件保证这一点
   - **各向异性扩散**：当 $g(t) = G(x,t)$ 是矩阵时，反向SDE变为：
     $dx_t = [f(x_t,t) - \nabla \cdot (G(x_t,t)G(x_t,t)^T) - G(x_t,t)G(x_t,t)^T \nabla \log p_t(x_t)]dt + G(x_t,t)d\bar{W}_t$
   - **非马尔可夫情况**：如果前向过程有记忆，反向过程会如何变化？

</details>

## 5.3 概率流ODE：确定性的生成路径

SDE的采样过程是随机的，意味着从同一个噪声 $x_T$ 出发，每次得到的 $x_0$ 都会略有不同。这种随机性有时是优点（增加多样性），有时是缺点（难以复现、调试困难）。是否存在一种确定性的路径，也能将噪声映射到数据呢？

### 5.3.1 从随机到确定：概率流的发现

想象一条河流中的叶子。每片叶子的轨迹都是随机的（受到涡流影响），但整体的流动模式是确定的。概率流ODE捕捉的正是这种"平均流动"。

> **定义：概率流ODE**
> 
> 对于SDE：$dx_t = f(x_t, t) dt + g(t) dW_t$
> 
> 存在唯一的ODE，使得其解的分布与SDE相同：
> $dx_t = \left[f(x_t, t) - \frac{1}{2} g(t)^2 \nabla_{x_t} \log p_t(x_t)\right] dt$
> 
> 这个ODE被称为概率流（Probability Flow）ODE。注意：
> - 没有随机项 $dW_t$，完全确定性
> - 漂移项 = 原始漂移 - (1/2) × 扩散强度 × 分数
> - 边缘分布 $p_t(x)$ 与原SDE完全相同

### 5.3.2 为什么概率流ODE有效？

这个结果初看令人惊讶：随机过程和确定性过程怎么会有相同的分布演化？关键在于理解两种不同的视角：

1. **粒子视角（SDE）**：
   - 跟踪单个粒子的随机轨迹
   - 每个粒子受到随机力的影响
   - 多次运行得到不同结果

2. **流体视角（ODE）**：
   - 跟踪概率密度的演化
   - 描述"概率流体"的速度场
   - 确定性的演化规律

数学上，这两种视角通过Fokker-Planck方程联系起来。SDE和其对应的概率流ODE都满足同一个Fokker-Planck方程，因此具有相同的密度演化。

🎨 **可视化理解**：
```
SDE轨迹（多条随机路径）：        概率流ODE（确定性流线）：
    噪声                              噪声
     ↓ ～～～                          ↓
     ↓   ～～                          ↓
     ↓ ～～～                          ↓
     ↓   ～～                          ↓
    数据                             数据
```

### 5.3.3 概率流ODE的推导直觉

概率流ODE中的修正项 $-\frac{1}{2}g(t)^2\nabla\log p_t$ 从何而来？这里有一个优美的解释：

在SDE中，随机项 $g(t)dW_t$ 造成了两种效应：
1. **扩散效应**：使分布变宽
2. **漂移效应**：由于扩散的不均匀性产生的净流动

概率流ODE需要在确定性框架下复现这两种效应。扩散效应已经体现在密度演化中，而漂移效应正是由 $-\frac{1}{2}g(t)^2\nabla\log p_t$ 项捕捉的。这个项确保了：
- 在高密度区域，粒子倾向于向外流动
- 在低密度区域，粒子流动较慢
- 整体效果等价于随机扩散

**概率流ODE的优势**：
1.  **确定性采样**：从一个 $x_T$ 出发，总能得到完全相同的 $x_0$ 。这对于需要可复现生成的任务非常有用。
2.  **更快的采样**：作为ODE，我们可以使用各种现成的高阶数值求解器（如Runge-Kutta法），可以用比SDE求解器少得多的步数（例如50-100步 vs 1000步）得到高质量的样本。这是DDIM等快速采样算法的理论基础。
3.  **精确的似然计算**：通过ODE的瞬时变量变换公式，可以精确计算出数据点 $x_0$ 的对数似然，而SDE只能计算一个下界。

🌟 **理论空白**：SDE和ODE提供了两种不同的采样路径。SDE路径是随机的、高维的，而ODE路径是确定性的、低维的。这两种路径的几何性质有何不同？它们在数据流形上是如何移动的？理解这一点可能有助于设计出更优的采样算法。

## 5.4 Fokker-Planck方程：从粒子到密度的演化

SDE和ODE描述了单个数据点（粒子）的轨迹。如果我们想从宏观上描述整个概率密度 $p_t(x)$ 的演化，就需要偏微分方程（Partial Differential Equation, PDE）的语言，这就是Fokker-Planck方程。

> **定义：Fokker-Planck方程**
> 对于一个SDE $dx = f(x,t)dt + g(t)dW_t$ ，其概率密度 $p_t(x)$ 的演化遵循Fokker-Planck方程：
> $\frac{\partial p_t(x)}{\partial t} = -\nabla \cdot (f(x,t)p_t(x)) + \frac{1}{2} g(t)^2 \Delta p_t(x)$
> 其中 $\nabla \cdot$ 是散度算子， $\Delta$ 是拉普拉斯算子。

这个方程的直观含义是：概率密度的变化率 = 由漂移项引起的输运（advection）+ 由扩散项引起的平滑（diffusion）。它将微观的粒子随机运动与宏观的概率密度演化联系在了一起。虽然在实践中我们通常不直接求解这个PDE，但它为理解扩散模型提供了强大的理论工具，并与物理学中的热力学、流体力学等领域建立了深刻的联系。

🔬 **研究线索**：Fokker-Planck方程与最优传输理论中的Wasserstein梯度流有深刻联系。扩散模型可以被看作是在概率分布空间中，沿着某种能量泛函的梯度方向进行演化。探索这种几何观点是当前理论研究的一大热点。

<details>
<summary><strong>练习 5.2：SDE, ODE, PDE的联系与区别</strong></summary>

1.  **写出ODE**：给定前向VP-SDE，写出其对应的概率流ODE的表达式。
2.  **写出PDE**：给定前向VP-SDE，写出其对应的Fokker-Planck方程。
3.  **比较与分析**：总结SDE、ODE、PDE这三种数学工具在描述扩散模型时的角色和优缺点。
    *   SDE的优点是什么？（提示：与分数匹配的联系）
    *   ODE的优点是什么？（提示：采样速度和似然计算）
    *   PDE的优点是什么？（提示：宏观理论分析）
4.  **开放探索**：概率流ODE提供了一条确定性的从噪声到数据的路径。这本质上定义了一个可逆的函数 $g: x_T \to x_0$ 。这与连续归一化流（Continuous Normalizing Flows, CNF）有何联系？分析两种模型的异同。

</details>

## 本章小结

本章将我们对扩散模型的理解从离散时间步提升到了连续时间的SDE/PDE框架，这是一个更深刻、更统一的视角。

- **从离散到连续**：我们展示了当时间步数趋于无穷时，离散的DDPM和NCSN过程如何自然地收敛到连续的SDE。
- **反向时间SDE**：我们学习了Anderson定理，它揭示了反向去噪过程的核心是学习分数函数 $\nabla_{x_t} \log p_t(x)$ ，从而将DDPM和分数匹配统一起来。
- **概率流ODE**：我们发现每个SDE都对应一个确定性的ODE，它不仅能实现更快的采样，还能进行精确的似然计算。
- **Fokker-Planck方程**：我们引入了描述概率密度演化的PDE，为宏观理论分析提供了工具。

这个连续时间框架不仅统一了现有的模型，更为未来的创新（如设计新的SDE、开发更快的求解器）提供了无限可能。下一章，我们将探讨另一个优雅的连续时间框架——流匹配（Flow Matching），它从最优传输的视角为生成建模提供了新的思路。
