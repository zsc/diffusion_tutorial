[â† è¿”å›ç›®å½•](index.md) | ç¬¬11ç«  / å…±14ç«  | [ä¸‹ä¸€ç«  â†’](chapter12.md)

# ç¬¬11ç« ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹

è§†é¢‘ç”Ÿæˆæ˜¯æ‰©æ•£æ¨¡å‹é¢ä¸´çš„æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ã€‚ä¸é™æ€å›¾åƒä¸åŒï¼Œè§†é¢‘éœ€è¦åœ¨æ—¶é—´ç»´åº¦ä¸Šä¿æŒè¿è´¯æ€§ï¼ŒåŒæ—¶å¤„ç†æ›´é«˜ç»´åº¦çš„æ•°æ®ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä»æ—¶åºå»ºæ¨¡çš„åŸºæœ¬åŸç†åˆ°3Dæ¶æ„è®¾è®¡ï¼Œå†åˆ°è¿åŠ¨åŠ¨åŠ›å­¦çš„å»ºæ¨¡ã€‚æ‚¨å°†å­¦ä¹ å¦‚ä½•å¤„ç†æ—¶é—´ä¸€è‡´æ€§ã€è¿åŠ¨æ¨¡ç³Šã€é•¿ç¨‹ä¾èµ–ç­‰è§†é¢‘ç‰¹æœ‰çš„æŒ‘æˆ˜ï¼Œå¹¶æŒæ¡è®¾è®¡é«˜æ•ˆè§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„å…³é”®æŠ€æœ¯ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨å°†ç†è§£Soraã€Runwayç­‰å‰æ²¿è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒŒåçš„æŠ€æœ¯åŸç†ã€‚

## ç« èŠ‚å¤§çº²

### 11.1 è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡
- æ—¶åºä¸€è‡´æ€§è¦æ±‚
- è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆ
- è¿åŠ¨è¡¨ç¤ºä¸å»ºæ¨¡
- æ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡

### 11.2 æ—¶åºæ‰©æ•£æ¨¡å‹æ¶æ„
- 3D U-Netä¸å› å­åŒ–å·ç§¯
- æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶
- å¸§é—´ä¿¡æ¯ä¼ æ’­
- åˆ†å±‚æ—¶åºå»ºæ¨¡

### 11.3 æ¡ä»¶æ§åˆ¶ä¸è¿åŠ¨å¼•å¯¼
- æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ
- å›¾åƒåŠ¨ç”»åŒ–
- è¿åŠ¨è½¨è¿¹æ§åˆ¶
- é£æ ¼ä¸å†…å®¹è§£è€¦

### 11.4 é«˜æ•ˆè®­ç»ƒä¸æ¨ç†ç­–ç•¥
- è§†é¢‘å‹ç¼©ä¸æ½œåœ¨ç©ºé—´
- çº§è”ç”Ÿæˆæ¡†æ¶
- å¸§æ’å€¼ä¸è¶…åˆ†è¾¨ç‡
- åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯

### 11.5 åº”ç”¨æ¡ˆä¾‹ä¸æœªæ¥æ–¹å‘
- è§†é¢‘ç¼–è¾‘ä¸ä¿®å¤
- è™šæ‹Ÿç°å®å†…å®¹ç”Ÿæˆ
- å®æ—¶è§†é¢‘åˆæˆ
- å¤šæ¨¡æ€è§†é¢‘ç†è§£

## 11.1 è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡

### 11.1.1 æ—¶åºä¸€è‡´æ€§è¦æ±‚

è§†é¢‘ç”Ÿæˆçš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ä¿æŒæ—¶é—´ä¸Šçš„è¿è´¯æ€§ï¼š

**1. å¯¹è±¡æŒç»­æ€§**ï¼š
- ç‰©ä½“èº«ä»½åœ¨å¸§é—´ä¿æŒä¸€è‡´
- å¤–è§‚ç‰¹å¾ï¼ˆé¢œè‰²ã€çº¹ç†ï¼‰ç¨³å®š
- å½¢çŠ¶å˜åŒ–ç¬¦åˆç‰©ç†è§„å¾‹

**2. è¿åŠ¨è¿ç»­æ€§**ï¼š
- è½¨è¿¹å¹³æ»‘è‡ªç„¶
- é€Ÿåº¦å’ŒåŠ é€Ÿåº¦åˆç†
- é®æŒ¡å…³ç³»æ­£ç¡®

**3. å…‰ç…§ä¸€è‡´æ€§**ï¼š
- é˜´å½±éšç‰©ä½“ç§»åŠ¨
- åå°„å’Œé«˜å…‰ç¨³å®š
- ç¯å¢ƒå…‰ç…§æ¸å˜

```python
class TemporalConsistencyLoss(nn.Module):
    """æ—¶åºä¸€è‡´æ€§æŸå¤±"""
    def __init__(self, flow_model=None):
        super().__init__()
        self.flow_model = flow_model  # å…‰æµä¼°è®¡æ¨¡å‹
        self.perceptual = lpips.LPIPS(net='vgg')
        
    def forward(self, video_pred, video_gt=None):
        """
        video_pred: [B, T, C, H, W]
        """
        losses = {}
        
        # 1. çŸ­ç¨‹ä¸€è‡´æ€§ï¼ˆç›¸é‚»å¸§ï¼‰
        frame_diff = video_pred[:, 1:] - video_pred[:, :-1]
        losses['short_term'] = torch.abs(frame_diff).mean()
        
        # 2. å…‰æµä¸€è‡´æ€§
        if self.flow_model is not None:
            for t in range(video_pred.shape[1] - 1):
                flow = self.flow_model(video_pred[:, t], video_pred[:, t+1])
                warped = self.warp_frame(video_pred[:, t], flow)
                losses[f'flow_{t}'] = F.l1_loss(warped, video_pred[:, t+1])
        
        # 3. æ„ŸçŸ¥ä¸€è‡´æ€§
        perceptual_loss = 0
        for t in range(video_pred.shape[1] - 1):
            perceptual_loss += self.perceptual(
                video_pred[:, t], 
                video_pred[:, t+1]
            ).mean()
        losses['perceptual'] = perceptual_loss
        
        return losses
```

ğŸ’¡ **å…³é”®æ´å¯Ÿï¼šæ—¶åºæ­£åˆ™åŒ–çš„é‡è¦æ€§**  
å•çº¯çš„å¸§çº§æŸå¤±ä¼šå¯¼è‡´é—ªçƒã€‚å¿…é¡»æ˜¾å¼åœ°é¼“åŠ±æ—¶åºå¹³æ»‘æ€§ï¼Œä½†è¿‡åº¦å¹³æ»‘ä¼šå¤±å»è¿åŠ¨ç»†èŠ‚ã€‚å¹³è¡¡æ˜¯å…³é”®ã€‚

### 11.1.2 è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆ

è§†é¢‘æ•°æ®çš„é«˜ç»´ç‰¹æ€§å¸¦æ¥å·¨å¤§æŒ‘æˆ˜ï¼š

**ç»´åº¦çˆ†ç‚¸**ï¼š
- å›¾åƒï¼š`[B, C, H, W]` â†’ 4Då¼ é‡
- è§†é¢‘ï¼š`[B, T, C, H, W]` â†’ 5Då¼ é‡
- å†…å­˜éœ€æ±‚ï¼šTå€å¢é•¿

**è®¡ç®—å¤æ‚åº¦åˆ†æ**ï¼š
```python
def estimate_video_memory(batch_size, frames, height, width, channels=3):
    """ä¼°ç®—è§†é¢‘æ¨¡å‹å†…å­˜éœ€æ±‚"""
    # è¾“å…¥è§†é¢‘
    input_size = batch_size * frames * channels * height * width * 4  # float32
    
    # 3Då·ç§¯ç‰¹å¾ï¼ˆå‡è®¾æœ€å¤§é€šé“æ•°1024ï¼‰
    feature_size = batch_size * frames * 1024 * (height//8) * (width//8) * 4
    
    # æ—¶ç©ºæ³¨æ„åŠ›
    seq_len = frames * (height//16) * (width//16)
    attention_size = batch_size * seq_len * seq_len * 4
    
    total_gb = (input_size + feature_size + attention_size) / (1024**3)
    
    print(f"è¾“å…¥: {input_size / 1024**3:.2f} GB")
    print(f"ç‰¹å¾: {feature_size / 1024**3:.2f} GB")
    print(f"æ³¨æ„åŠ›: {attention_size / 1024**3:.2f} GB")
    print(f"æ€»è®¡: {total_gb:.2f} GB")
    
    return total_gb

# ç¤ºä¾‹ï¼š16å¸§ 512x512 è§†é¢‘
estimate_video_memory(1, 16, 512, 512)
# è¾“å‡ºï¼šæ€»è®¡çº¦ 20GBï¼
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šé«˜æ•ˆæ—¶ç©ºè¡¨ç¤º**  
å¦‚ä½•è®¾è®¡æ›´é«˜æ•ˆçš„æ—¶ç©ºè¡¨ç¤ºï¼Ÿåˆ†è§£æ–¹æ³•ï¼ˆç©ºé—´+æ—¶é—´ï¼‰ã€ç¨€ç–è¡¨ç¤ºã€æˆ–æ˜¯å…¨æ–°çš„æ¶æ„ï¼Ÿè¿™æ˜¯æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚

### 11.1.3 è¿åŠ¨è¡¨ç¤ºä¸å»ºæ¨¡

**è¿åŠ¨çš„å¤šå°ºåº¦ç‰¹æ€§**ï¼š
1. **åƒç´ çº§è¿åŠ¨**ï¼šå…‰æµã€å½¢å˜åœº
2. **å¯¹è±¡çº§è¿åŠ¨**ï¼šè½¨è¿¹ã€æ—‹è½¬ã€ç¼©æ”¾
3. **åœºæ™¯çº§è¿åŠ¨**ï¼šç›¸æœºè¿åŠ¨ã€è§†è§’å˜åŒ–

**è¿åŠ¨è¡¨ç¤ºæ–¹æ³•**ï¼š
```python
class MotionRepresentation:
    """ä¸åŒçš„è¿åŠ¨è¡¨ç¤ºæ–¹æ³•"""
    
    @staticmethod
    def optical_flow(frame1, frame2):
        """å…‰æµè¡¨ç¤º"""
        # ä½¿ç”¨é¢„è®­ç»ƒçš„å…‰æµæ¨¡å‹
        flow = flow_model(frame1, frame2)
        return flow  # [B, 2, H, W]
    
    @staticmethod
    def motion_patches(video):
        """è¿åŠ¨å—è¡¨ç¤º"""
        # è®¡ç®—æ—¶ç©ºæ¢¯åº¦
        dt = video[:, 1:] - video[:, :-1]  # æ—¶é—´å¯¼æ•°
        dx = video[:, :, :, :, 1:] - video[:, :, :, :, :-1]  # ç©ºé—´å¯¼æ•°x
        dy = video[:, :, :, 1:] - video[:, :, :, :-1]  # ç©ºé—´å¯¼æ•°y
        
        # è¿åŠ¨èƒ½é‡
        motion_energy = torch.sqrt(dt**2 + dx**2 + dy**2)
        return motion_energy
    
    @staticmethod
    def trajectory_encoding(keypoints_sequence):
        """è½¨è¿¹ç¼–ç """
        # å°†å…³é”®ç‚¹åºåˆ—ç¼–ç ä¸ºè½¨è¿¹ç‰¹å¾
        velocities = keypoints_sequence[1:] - keypoints_sequence[:-1]
        accelerations = velocities[1:] - velocities[:-1]
        
        return {
            'positions': keypoints_sequence,
            'velocities': velocities,
            'accelerations': accelerations
        }
```

### 11.1.4 æ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡

**ä¸»è¦æ•°æ®é›†**ï¼š

| æ•°æ®é›† | è§„æ¨¡ | åˆ†è¾¨ç‡ | ç‰¹ç‚¹ |
|--------|------|---------|------|
| UCF-101 | 13K videos | 240p | äººç±»åŠ¨ä½œ |
| Kinetics | 650K videos | å˜åŒ– | å¤šæ ·åŠ¨ä½œ |
| WebVid-10M | 10M videos | 360p | æ–‡æœ¬é…å¯¹ |
| HD-VILA-100M | 100M videos | 720p | é«˜è´¨é‡ |

**è¯„ä¼°æŒ‡æ ‡**ï¼š
```python
class VideoMetrics:
    """è§†é¢‘ç”Ÿæˆè´¨é‡è¯„ä¼°"""
    
    def __init__(self):
        self.i3d_model = load_i3d_model()  # ç”¨äºFVD
        self.flow_model = load_flow_model()  # ç”¨äºæ—¶åºæŒ‡æ ‡
        
    def fvd(self, real_videos, fake_videos):
        """FrÃ©chet Video Distance"""
        # æå–I3Dç‰¹å¾
        real_features = self.i3d_model(real_videos)
        fake_features = self.i3d_model(fake_videos)
        
        # è®¡ç®—FVDï¼ˆç±»ä¼¼FIDï¼‰
        mu_real = real_features.mean(0)
        mu_fake = fake_features.mean(0)
        sigma_real = torch.cov(real_features.T)
        sigma_fake = torch.cov(fake_features.T)
        
        return calculate_frechet_distance(
            mu_real, sigma_real, mu_fake, sigma_fake
        )
    
    def temporal_consistency(self, video):
        """æ—¶åºä¸€è‡´æ€§è¯„åˆ†"""
        consistency_scores = []
        
        for t in range(len(video) - 1):
            # è®¡ç®—å…‰æµ
            flow = self.flow_model(video[t], video[t+1])
            
            # warpè¯¯å·®
            warped = warp_frame(video[t], flow)
            error = torch.abs(warped - video[t+1]).mean()
            
            consistency_scores.append(1.0 / (1.0 + error))
            
        return torch.tensor(consistency_scores).mean()
```

<details>
<summary>**ç»ƒä¹  11.1ï¼šåˆ†æè§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜**</summary>

æ·±å…¥ç†è§£è§†é¢‘ç”Ÿæˆçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚

1. **æ—¶åºå»ºæ¨¡å®éªŒ**ï¼š
   - å®ç°ç®€å•çš„å¸§æ’å€¼åŸºçº¿
   - æµ‹è¯•ä¸åŒçš„æ—¶åºä¸€è‡´æ€§æŸå¤±
   - åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼ˆé—ªçƒã€æ¼‚ç§»ç­‰ï¼‰

2. **å†…å­˜ä¼˜åŒ–æ¢ç´¢**ï¼š
   - æ¯”è¾ƒä¸åŒçš„è§†é¢‘è¡¨ç¤ºï¼ˆRGB vs å…‰æµï¼‰
   - å®ç°æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜
   - æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒæ•ˆæœ

3. **è¿åŠ¨åˆ†æ**ï¼š
   - å¯è§†åŒ–ä¸åŒç±»å‹çš„è¿åŠ¨æ¨¡å¼
   - å®ç°è¿åŠ¨åˆ†è§£ï¼ˆå…¨å±€vså±€éƒ¨ï¼‰
   - ç ”ç©¶è¿åŠ¨å…ˆéªŒçš„ä½œç”¨

4. **æ•°æ®é›†æ„å»º**ï¼š
   - è®¾è®¡è§†é¢‘è´¨é‡ç­›é€‰pipeline
   - å®ç°é«˜æ•ˆçš„è§†é¢‘é¢„å¤„ç†
   - åˆ›å»ºä¸“é—¨çš„è¯„æµ‹åŸºå‡†

</details>

### 11.1.5 è§†é¢‘æ‰©æ•£çš„ç‹¬ç‰¹æœºé‡

**1. å¼ºå¤§çš„æ—¶åºå…ˆéªŒ**ï¼š
- ç‰©ç†è§„å¾‹ï¼ˆé‡åŠ›ã€æƒ¯æ€§ï¼‰
- å› æœå…³ç³»
- å‘¨æœŸæ€§æ¨¡å¼

**2. å¤šæ¨¡æ€ä¿¡æ¯**ï¼š
- è§†è§‰+éŸ³é¢‘åŒæ­¥
- æ–‡æœ¬æè¿°çš„æ—¶åºç»“æ„
- åŠ¨ä½œæ ‡ç­¾åºåˆ—

**3. åˆ†å±‚è¡¨ç¤º**ï¼š
```python
class HierarchicalVideoRepresentation:
    """åˆ†å±‚è§†é¢‘è¡¨ç¤º"""
    
    def __init__(self):
        self.scene_encoder = SceneEncoder()      # åœºæ™¯çº§
        self.object_encoder = ObjectEncoder()    # å¯¹è±¡çº§
        self.motion_encoder = MotionEncoder()    # è¿åŠ¨çº§
        self.texture_encoder = TextureEncoder()  # çº¹ç†çº§
        
    def encode(self, video):
        # åˆ†å±‚ç¼–ç 
        scene_features = self.scene_encoder(video)
        object_features = self.object_encoder(video)
        motion_features = self.motion_encoder(video)
        texture_features = self.texture_encoder(video)
        
        return {
            'scene': scene_features,      # å…¨å±€åœºæ™¯ä¿¡æ¯
            'objects': object_features,   # å¯¹è±¡èº«ä»½å’Œä½ç½®
            'motion': motion_features,    # è¿åŠ¨æ¨¡å¼
            'texture': texture_features   # ç²¾ç»†çº¹ç†ç»†èŠ‚
        }
```

ğŸŒŸ **å‰æ²¿æ€è€ƒï¼šè§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€**  
è§†é¢‘ç†è§£æ¨¡å‹ï¼ˆå¦‚VideoMAEï¼‰çš„è¡¨ç¤ºèƒ½å¦ç›´æ¥ç”¨äºç”Ÿæˆï¼Ÿå¦‚ä½•è®¾è®¡æ—¢èƒ½ç†è§£åˆèƒ½ç”Ÿæˆçš„ç»Ÿä¸€æ¶æ„ï¼Ÿ

### 11.1.6 æŠ€æœ¯è·¯çº¿é€‰æ‹©

**ä¸»è¦æŠ€æœ¯è·¯çº¿å¯¹æ¯”**ï¼š

1. **ç›´æ¥3Dæ‰©æ•£**ï¼š
   - ä¼˜ç‚¹ï¼šç«¯åˆ°ç«¯å»ºæ¨¡
   - ç¼ºç‚¹ï¼šè®¡ç®—é‡å·¨å¤§

2. **çº§è”ç”Ÿæˆ**ï¼š
   - ä¼˜ç‚¹ï¼šåˆ†è€Œæ²»ä¹‹ï¼Œæ˜“äºæ§åˆ¶
   - ç¼ºç‚¹ï¼šè¯¯å·®ç´¯ç§¯

3. **æ½œåœ¨ç©ºé—´æ‰©æ•£**ï¼š
   - ä¼˜ç‚¹ï¼šé«˜æ•ˆ
   - ç¼ºç‚¹ï¼šéœ€è¦å¥½çš„è§†é¢‘ç¼–ç å™¨

4. **æ··åˆæ–¹æ³•**ï¼š
   ```python
   class HybridVideoGeneration:
       def __init__(self):
           # å…³é”®å¸§ç”Ÿæˆå™¨ï¼ˆ2Dæ‰©æ•£ï¼‰
           self.keyframe_generator = ImageDiffusion()
           
           # è¿åŠ¨ç”Ÿæˆå™¨ï¼ˆè½»é‡3Dï¼‰
           self.motion_generator = MotionDiffusion()
           
           # ç»†èŠ‚å¡«å……å™¨
           self.detail_filler = DetailRefinement()
       
       def generate(self, text_prompt, num_frames):
           # 1. ç”Ÿæˆå…³é”®å¸§
           keyframes = self.keyframe_generator(
               text_prompt, 
               num_keyframes=num_frames // 4
           )
           
           # 2. ç”Ÿæˆè¿åŠ¨
           motion = self.motion_generator(
               keyframes, 
               text_prompt
           )
           
           # 3. å¡«å……ç»†èŠ‚
           full_video = self.detail_filler(
               keyframes, 
               motion
           )
           
           return full_video
   ```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å…·ä½“çš„æ¨¡å‹æ¶æ„è®¾è®¡...

## 11.2 æ—¶åºæ‰©æ•£æ¨¡å‹æ¶æ„

### 11.2.1 3D U-Netä¸å› å­åŒ–å·ç§¯

å°†2D U-Netæ‰©å±•åˆ°3Dæ˜¯æœ€ç›´æ¥çš„æ–¹æ³•ï¼Œä½†éœ€è¦ä»”ç»†è®¾è®¡ä»¥æ§åˆ¶å‚æ•°é‡ï¼š

**å®Œæ•´3Då·ç§¯**ï¼š
```python
class Full3DConv(nn.Module):
    """å®Œæ•´çš„3Då·ç§¯å—"""
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super().__init__()
        self.conv = nn.Conv3d(
            in_channels, 
            out_channels,
            kernel_size=(kernel_size, kernel_size, kernel_size),
            padding=kernel_size//2
        )
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        return self.conv(x)
```

**å› å­åŒ–å·ç§¯ï¼ˆæ›´é«˜æ•ˆï¼‰**ï¼š
```python
class FactorizedConv3D(nn.Module):
    """(2+1)Då› å­åŒ–å·ç§¯ï¼šç©ºé—´å·ç§¯ + æ—¶é—´å·ç§¯"""
    def __init__(self, in_channels, out_channels, spatial_kernel=3, temporal_kernel=3):
        super().__init__()
        # ç©ºé—´å·ç§¯
        self.spatial_conv = nn.Conv3d(
            in_channels, 
            out_channels,
            kernel_size=(1, spatial_kernel, spatial_kernel),
            padding=(0, spatial_kernel//2, spatial_kernel//2)
        )
        
        # æ—¶é—´å·ç§¯
        self.temporal_conv = nn.Conv3d(
            out_channels,
            out_channels,
            kernel_size=(temporal_kernel, 1, 1),
            padding=(temporal_kernel//2, 0, 0)
        )
        
        self.norm = nn.GroupNorm(32, out_channels)
        self.activation = nn.SiLU()
        
    def forward(self, x):
        # å…ˆç©ºé—´åæ—¶é—´
        x = self.spatial_conv(x)
        x = self.temporal_conv(x)
        x = self.norm(x)
        x = self.activation(x)
        return x
```

**ä¼ª3Då·ç§¯ï¼ˆPseudo-3Dï¼‰**ï¼š
```python
class Pseudo3DBlock(nn.Module):
    """äº¤æ›¿è¿›è¡Œ2Dç©ºé—´å’Œ1Dæ—¶é—´å¤„ç†"""
    def __init__(self, channels, num_frames):
        super().__init__()
        # ç©ºé—´å¤„ç†ï¼ˆè·¨å¸§å…±äº«ï¼‰
        self.spatial_block = ResBlock2D(channels)
        
        # æ—¶é—´æ··åˆ
        self.temporal_mixer = nn.Conv1d(
            channels * num_frames,
            channels * num_frames,
            kernel_size=3,
            padding=1,
            groups=channels  # æŒ‰é€šé“åˆ†ç»„
        )
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        B, C, T, H, W = x.shape
        
        # ç©ºé—´å¤„ç†ï¼šåˆå¹¶batchå’Œtimeç»´åº¦
        x_2d = rearrange(x, 'b c t h w -> (b t) c h w')
        x_2d = self.spatial_block(x_2d)
        x = rearrange(x_2d, '(b t) c h w -> b c t h w', b=B, t=T)
        
        # æ—¶é—´æ··åˆï¼šå°†ç©ºé—´ç»´åº¦å±•å¹³
        x_1d = rearrange(x, 'b c t h w -> b (c h w) t')
        x_1d = self.temporal_mixer(x_1d)
        x = rearrange(x_1d, 'b (c h w) t -> b c t h w', c=C, h=H, w=W)
        
        return x
```

ğŸ’¡ **è®¾è®¡æƒè¡¡ï¼šè®¡ç®—æ•ˆç‡ vs è¡¨è¾¾èƒ½åŠ›**  
- å®Œæ•´3Dï¼šæœ€å¼ºè¡¨è¾¾åŠ›ï¼Œè®¡ç®—é‡ O(kÂ³)
- å› å­åŒ–ï¼šå¹³è¡¡é€‰æ‹©ï¼Œè®¡ç®—é‡ O(kÂ² + k)
- ä¼ª3Dï¼šæœ€é«˜æ•ˆï¼Œä½†æ—¶ç©ºäº¤äº’å—é™

### 11.2.2 æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶

æ³¨æ„åŠ›åœ¨è§†é¢‘æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œä½†éœ€è¦ç²¾å¿ƒè®¾è®¡ä»¥æ§åˆ¶å¤æ‚åº¦ï¼š

**å…¨æ—¶ç©ºæ³¨æ„åŠ›ï¼ˆè®¡ç®—å¯†é›†ï¼‰**ï¼š
```python
class FullSpatioTemporalAttention(nn.Module):
    """å¯¹æ‰€æœ‰æ—¶ç©ºä½ç½®è®¡ç®—æ³¨æ„åŠ›"""
    def __init__(self, dim, heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, heads)
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        B, C, T, H, W = x.shape
        
        # å±•å¹³ä¸ºåºåˆ—
        x = rearrange(x, 'b c t h w -> b (t h w) c')
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆå¤æ‚åº¦ï¼šO(TÂ²HÂ²WÂ²)ï¼‰
        x, _ = self.attention(x, x, x)
        
        # æ¢å¤å½¢çŠ¶
        x = rearrange(x, 'b (t h w) c -> b c t h w', t=T, h=H, w=W)
        return x
```

**åˆ†è§£çš„æ—¶ç©ºæ³¨æ„åŠ›ï¼ˆé«˜æ•ˆï¼‰**ï¼š
```python
class FactorizedSpatioTemporalAttention(nn.Module):
    """å…ˆç©ºé—´æ³¨æ„åŠ›ï¼Œåæ—¶é—´æ³¨æ„åŠ›"""
    def __init__(self, dim, spatial_heads=8, temporal_heads=4):
        super().__init__()
        self.spatial_attn = SpatialAttention(dim, spatial_heads)
        self.temporal_attn = TemporalAttention(dim, temporal_heads)
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        
        # ç©ºé—´æ³¨æ„åŠ›ï¼ˆåœ¨æ¯å¸§å†…ï¼‰
        x = self.spatial_attn(x)
        
        # æ—¶é—´æ³¨æ„åŠ›ï¼ˆè·¨å¸§ï¼‰
        x = self.temporal_attn(x)
        
        return x

class SpatialAttention(nn.Module):
    def __init__(self, dim, heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, heads)
        
    def forward(self, x):
        B, C, T, H, W = x.shape
        
        # å¯¹æ¯å¸§ç‹¬ç«‹è®¡ç®—ç©ºé—´æ³¨æ„åŠ›
        outputs = []
        for t in range(T):
            frame = x[:, :, t, :, :]  # [B, C, H, W]
            frame = rearrange(frame, 'b c h w -> b (h w) c')
            frame_out, _ = self.attention(frame, frame, frame)
            frame_out = rearrange(frame_out, 'b (h w) c -> b c h w', h=H, w=W)
            outputs.append(frame_out)
            
        return torch.stack(outputs, dim=2)

class TemporalAttention(nn.Module):
    def __init__(self, dim, heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(dim, heads)
        
    def forward(self, x):
        B, C, T, H, W = x.shape
        
        # å¯¹æ¯ä¸ªç©ºé—´ä½ç½®è®¡ç®—æ—¶é—´æ³¨æ„åŠ›
        x = rearrange(x, 'b c t h w -> (b h w) t c')
        x, _ = self.attention(x, x, x)
        x = rearrange(x, '(b h w) t c -> b c t h w', b=B, h=H, w=W)
        
        return x
```

**åˆ†å—æ—¶ç©ºæ³¨æ„åŠ›ï¼ˆå†…å­˜å‹å¥½ï¼‰**ï¼š
```python
class WindowedSpatioTemporalAttention(nn.Module):
    """çª—å£å†…çš„æ—¶ç©ºæ³¨æ„åŠ›ï¼Œç±»ä¼¼Video Swin Transformer"""
    def __init__(self, dim, window_size=(4, 7, 7), heads=8):
        super().__init__()
        self.window_size = window_size
        self.attention = nn.MultiheadAttention(dim, heads)
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        B, C, T, H, W = x.shape
        Tw, Hw, Ww = self.window_size
        
        # åˆ’åˆ†çª—å£
        x = rearrange(x, 'b c (tw nw) (hw ph) (ww pw) -> b (nw ph pw) (tw hw ww) c',
                     tw=Tw, hw=Hw, ww=Ww)
        
        # çª—å£å†…æ³¨æ„åŠ›
        x, _ = self.attention(x, x, x)
        
        # æ¢å¤å½¢çŠ¶
        x = rearrange(x, 'b (nw ph pw) (tw hw ww) c -> b c (tw nw) (hw ph) (ww pw)',
                     tw=Tw, hw=Hw, ww=Ww, nw=T//Tw, ph=H//Hw, pw=W//Ww)
        
        return x
```

ğŸ”¬ **ç ”ç©¶æ–¹å‘ï¼šè‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å¼**  
èƒ½å¦å­¦ä¹ æ•°æ®ç›¸å…³çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Ÿä¾‹å¦‚ï¼Œå¿«é€Ÿè¿åŠ¨åŒºåŸŸä½¿ç”¨å¯†é›†æ—¶é—´æ³¨æ„åŠ›ï¼Œé™æ€åŒºåŸŸä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›ã€‚

### 11.2.3 å¸§é—´ä¿¡æ¯ä¼ æ’­

ç¡®ä¿ä¿¡æ¯åœ¨å¸§é—´æœ‰æ•ˆæµåŠ¨æ˜¯å…³é”®ï¼š

**å¾ªç¯è¿æ¥**ï¼š
```python
class RecurrentPropagation(nn.Module):
    """ä½¿ç”¨å¾ªç¯ç»“æ„ä¼ æ’­æ—¶åºä¿¡æ¯"""
    def __init__(self, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        B, C, T, H, W = x.shape
        
        # ç©ºé—´poolingè·å¾—å¸§çº§ç‰¹å¾
        frame_features = x.mean(dim=(3, 4))  # [B, C, T]
        frame_features = frame_features.transpose(1, 2)  # [B, T, C]
        
        # GRUå¤„ç†
        hidden_states, _ = self.gru(frame_features)
        
        # å¹¿æ’­å›ç©ºé—´ç»´åº¦
        hidden_states = hidden_states.transpose(1, 2)  # [B, C, T]
        hidden_states = hidden_states.unsqueeze(3).unsqueeze(4)
        hidden_states = hidden_states.expand(-1, -1, -1, H, W)
        
        return x + hidden_states
```

**åŒå‘ä¼ æ’­**ï¼š
```python
class BidirectionalPropagation(nn.Module):
    """åŒå‘ä¼ æ’­ç¡®ä¿å‰åæ–‡ä¿¡æ¯"""
    def __init__(self, channels):
        super().__init__()
        self.forward_conv = nn.Conv3d(
            channels, channels, 
            kernel_size=(3, 1, 1),
            padding=(1, 0, 0)
        )
        self.backward_conv = nn.Conv3d(
            channels, channels,
            kernel_size=(3, 1, 1), 
            padding=(1, 0, 0)
        )
        
    def forward(self, x):
        # å‰å‘ä¼ æ’­
        forward_out = self.forward_conv(x)
        
        # åå‘ä¼ æ’­ï¼ˆç¿»è½¬æ—¶é—´ç»´åº¦ï¼‰
        x_flip = torch.flip(x, dims=[2])
        backward_out = self.backward_conv(x_flip)
        backward_out = torch.flip(backward_out, dims=[2])
        
        # èåˆåŒå‘ä¿¡æ¯
        return x + 0.5 * (forward_out + backward_out)
```

<details>
<summary>**ç»ƒä¹  11.2ï¼šè®¾è®¡é«˜æ•ˆçš„è§†é¢‘æ¶æ„**</summary>

æ¢ç´¢ä¸åŒçš„æ¶æ„è®¾è®¡é€‰æ‹©ã€‚

1. **æ¶æ„æ¯”è¾ƒ**ï¼š
   - å®ç°3ç§ä¸åŒçš„3Då·ç§¯å˜ä½“
   - æ¯”è¾ƒå‚æ•°é‡ã€FLOPså’Œå†…å­˜ä½¿ç”¨
   - åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½

2. **æ³¨æ„åŠ›ä¼˜åŒ–**ï¼š
   - å®ç°ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
   - æµ‹è¯•ä¸åŒçš„åˆ†è§£ç­–ç•¥
   - åˆ†ææ³¨æ„åŠ›å›¾çš„æ—¶ç©ºæ¨¡å¼

3. **ä¿¡æ¯æµåˆ†æ**ï¼š
   - å¯è§†åŒ–ç‰¹å¾åœ¨æ—¶é—´ç»´åº¦çš„ä¼ æ’­
   - æµ‹é‡æœ‰æ•ˆæ„Ÿå—é‡
   - è¯†åˆ«ä¿¡æ¯ç“¶é¢ˆ

4. **æ··åˆæ¶æ„è®¾è®¡**ï¼š
   - ç»“åˆCNNå’ŒTransformerçš„ä¼˜åŠ¿
   - è®¾è®¡è‡ªé€‚åº”çš„è®¡ç®—åˆ†é…
   - æ¢ç´¢æ—©æœŸèåˆvsæ™šæœŸèåˆ

</details>

### 11.2.4 åˆ†å±‚æ—¶åºå»ºæ¨¡

ä¸åŒæ—¶é—´å°ºåº¦éœ€è¦ä¸åŒçš„å¤„ç†ç­–ç•¥ï¼š

```python
class HierarchicalTemporalModeling(nn.Module):
    """å¤šå°ºåº¦æ—¶åºå»ºæ¨¡"""
    def __init__(self, base_channels=256):
        super().__init__()
        
        # çŸ­ç¨‹å»ºæ¨¡ï¼ˆ1-2å¸§ï¼‰
        self.short_term = ShortTermModule(base_channels)
        
        # ä¸­ç¨‹å»ºæ¨¡ï¼ˆ4-8å¸§ï¼‰  
        self.mid_term = MidTermModule(base_channels * 2)
        
        # é•¿ç¨‹å»ºæ¨¡ï¼ˆ16+å¸§ï¼‰
        self.long_term = LongTermModule(base_channels * 4)
        
    def forward(self, x):
        # x: [B, C, T, H, W]
        
        # çŸ­ç¨‹ç‰¹å¾
        short_features = self.short_term(x)
        
        # ä¸­ç¨‹ç‰¹å¾ï¼ˆé™é‡‡æ ·æ—¶é—´ç»´åº¦ï¼‰
        x_mid = F.avg_pool3d(x, kernel_size=(2, 1, 1))
        mid_features = self.mid_term(x_mid)
        mid_features = F.interpolate(mid_features, size=x.shape[2:])
        
        # é•¿ç¨‹ç‰¹å¾ï¼ˆæ›´æ¿€è¿›çš„é™é‡‡æ ·ï¼‰
        x_long = F.avg_pool3d(x, kernel_size=(4, 2, 2))
        long_features = self.long_term(x_long)
        long_features = F.interpolate(long_features, size=x.shape[2:])
        
        # å¤šå°ºåº¦èåˆ
        return short_features + mid_features + long_features

class ShortTermModule(nn.Module):
    """å¤„ç†å¿«é€Ÿè¿åŠ¨å’Œç»†èŠ‚å˜åŒ–"""
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv3d(
            channels, channels,
            kernel_size=(3, 3, 3),
            padding=(1, 1, 1)
        )
        
    def forward(self, x):
        return self.conv(x)

class MidTermModule(nn.Module):
    """å¤„ç†å¯¹è±¡è¿åŠ¨å’Œäº¤äº’"""
    def __init__(self, channels):
        super().__init__()
        self.temporal_conv = nn.Conv3d(
            channels, channels,
            kernel_size=(5, 1, 1),
            padding=(2, 0, 0)
        )
        self.spatial_attn = SpatialAttention(channels, heads=8)
        
    def forward(self, x):
        x = self.temporal_conv(x)
        x = self.spatial_attn(x)
        return x

class LongTermModule(nn.Module):
    """å¤„ç†åœºæ™¯å˜åŒ–å’Œå…¨å±€æ¨¡å¼"""
    def __init__(self, channels):
        super().__init__()
        self.temporal_attn = nn.MultiheadAttention(channels, num_heads=8)
        
    def forward(self, x):
        B, C, T, H, W = x.shape
        # å…¨å±€æ± åŒ–ç©ºé—´ç»´åº¦
        x_pooled = x.mean(dim=(3, 4))  # [B, C, T]
        x_pooled = x_pooled.permute(2, 0, 1)  # [T, B, C]
        
        # æ—¶é—´æ³¨æ„åŠ›
        attn_out, _ = self.temporal_attn(x_pooled, x_pooled, x_pooled)
        attn_out = attn_out.permute(1, 2, 0)  # [B, C, T]
        
        # å¹¿æ’­å›åŸå§‹å½¢çŠ¶
        attn_out = attn_out.unsqueeze(3).unsqueeze(4).expand(-1, -1, -1, H, W)
        
        return attn_out
```

### 11.2.5 Video DiTæ¶æ„

å°†DiTæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼š

```python
class VideoDiT(nn.Module):
    """Video Diffusion Transformer"""
    def __init__(
        self,
        spatial_patch_size=8,
        temporal_patch_size=2,
        in_channels=3,
        hidden_size=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
    ):
        super().__init__()
        self.spatial_patch_size = spatial_patch_size
        self.temporal_patch_size = temporal_patch_size
        
        # æ—¶ç©ºpatchify
        self.patchify = nn.Conv3d(
            in_channels,
            hidden_size,
            kernel_size=(temporal_patch_size, spatial_patch_size, spatial_patch_size),
            stride=(temporal_patch_size, spatial_patch_size, spatial_patch_size)
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        
        # Transformerå—
        self.blocks = nn.ModuleList([
            DiTBlock(hidden_size, num_heads, mlp_ratio)
            for _ in range(depth)
        ])
        
        # è¾“å‡ºæŠ•å½±
        self.final_layer = FinalLayer(hidden_size, spatial_patch_size, temporal_patch_size)
        
    def forward(self, x, t, y=None):
        # x: [B, C, T, H, W]
        # t: æ—¶é—´æ­¥
        # y: æ¡ä»¶ï¼ˆå¦‚ç±»åˆ«æ ‡ç­¾ï¼‰
        
        # Patchify
        x = self.patchify(x)  # [B, hidden_size, T', H', W']
        x = rearrange(x, 'b c t h w -> b (t h w) c')
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # æ—¶é—´å’Œæ¡ä»¶ç¼–ç 
        t_emb = timestep_embedding(t, self.hidden_size)
        if y is not None:
            y_emb = self.class_embedder(y)
            c = t_emb + y_emb
        else:
            c = t_emb
            
        # Transformerå¤„ç†
        for block in self.blocks:
            x = block(x, c)
            
        # è¾“å‡º
        x = self.final_layer(x, c)
        
        # Unpatchify
        x = rearrange(x, 'b (t h w) c -> b c t h w', 
                     t=T//self.temporal_patch_size,
                     h=H//self.spatial_patch_size,
                     w=W//self.spatial_patch_size)
        
        return x
```

ğŸŒŸ **å‰æ²¿æ¢ç´¢ï¼šè§†é¢‘ç”Ÿæˆçš„æ‰©å±•å®šå¾‹**  
DiTè¯æ˜äº†å›¾åƒç”Ÿæˆçš„æ‰©å±•å®šå¾‹ã€‚è§†é¢‘ç”Ÿæˆæ˜¯å¦æœ‰ç±»ä¼¼è§„å¾‹ï¼Ÿæ—¶é—´ç»´åº¦å¦‚ä½•å½±å“æ‰©å±•ï¼Ÿè¿™æ˜¯å¼€æ”¾çš„ç ”ç©¶é—®é¢˜ã€‚

### 11.2.6 è½»é‡çº§è§†é¢‘æ¶æ„

å¯¹äºå®æ—¶æˆ–ç§»åŠ¨åº”ç”¨ï¼Œéœ€è¦æ›´è½»é‡çš„è®¾è®¡ï¼š

```python
class LightweightVideoGenerator(nn.Module):
    """è½»é‡çº§è§†é¢‘ç”Ÿæˆæ¶æ„"""
    def __init__(self, base_channels=64):
        super().__init__()
        
        # å…±äº«çš„2D backbone
        self.shared_backbone = MobileNetV3()
        
        # è½»é‡æ—¶é—´å»ºæ¨¡
        self.temporal_module = DepthwiseSeparableConv3D(
            base_channels, base_channels
        )
        
        # è¿åŠ¨é¢„æµ‹å¤´
        self.motion_head = nn.Conv3d(
            base_channels, 2,  # 2D motion vectors
            kernel_size=1
        )
        
    def forward(self, x):
        B, C, T, H, W = x.shape
        
        # ä½¿ç”¨å…±äº«backboneå¤„ç†æ¯å¸§
        features = []
        for t in range(T):
            feat = self.shared_backbone(x[:, :, t])
            features.append(feat)
        features = torch.stack(features, dim=2)
        
        # è½»é‡æ—¶é—´å¤„ç†
        temporal_features = self.temporal_module(features)
        
        # é¢„æµ‹è¿åŠ¨
        motion = self.motion_head(temporal_features)
        
        # åŸºäºè¿åŠ¨çš„é«˜æ•ˆç”Ÿæˆ
        output = self.motion_based_synthesis(x[:, :, 0], motion)
        
        return output

class DepthwiseSeparableConv3D(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»3Då·ç§¯"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # æ·±åº¦å·ç§¯
        self.depthwise = nn.Conv3d(
            in_channels, in_channels,
            kernel_size=3, padding=1,
            groups=in_channels
        )
        # é€ç‚¹å·ç§¯
        self.pointwise = nn.Conv3d(
            in_channels, out_channels,
            kernel_size=1
        )
        
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
```

ğŸ’¡ **å®è·µå»ºè®®ï¼šæ¶æ„é€‰æ‹©æŒ‡å—**  
- é«˜è´¨é‡ç¦»çº¿ç”Ÿæˆï¼šä½¿ç”¨å®Œæ•´3Dæ¶æ„
- å®æ—¶åº”ç”¨ï¼šä½¿ç”¨å› å­åŒ–æˆ–ä¼ª3D
- ç§»åŠ¨è®¾å¤‡ï¼šä½¿ç”¨å…±äº«backbone + è½»é‡æ—¶é—´æ¨¡å—
- é•¿è§†é¢‘ï¼šä½¿ç”¨åˆ†å±‚æ¶æ„é¿å…å†…å­˜çˆ†ç‚¸

## 11.3 æ¡ä»¶æ§åˆ¶ä¸è¿åŠ¨å¼•å¯¼

### 11.3.1 æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ

æ–‡æœ¬æ¡ä»¶æ˜¯è§†é¢‘ç”Ÿæˆæœ€é‡è¦çš„æ§åˆ¶æ–¹å¼ï¼š

**æ—¶åºæ„ŸçŸ¥çš„æ–‡æœ¬ç¼–ç **ï¼š
```python
class TemporalTextEncoder(nn.Module):
    """ç¼–ç åŒ…å«æ—¶åºä¿¡æ¯çš„æ–‡æœ¬æè¿°"""
    def __init__(self, base_encoder='clip', temporal_layers=4):
        super().__init__()
        # åŸºç¡€æ–‡æœ¬ç¼–ç å™¨
        if base_encoder == 'clip':
            self.text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')
        elif base_encoder == 't5':
            self.text_encoder = T5EncoderModel.from_pretrained('t5-large')
        
        # æ—¶åºå¢å¼ºå±‚
        self.temporal_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=768,
                nhead=8,
                dim_feedforward=3072
            ) for _ in range(temporal_layers)
        ])
        
        # æ—¶åºæ ‡è®°
        self.temporal_tokens = nn.Parameter(torch.randn(1, 16, 768))  # 16ä¸ªæ—¶åºä½ç½®
        
    def forward(self, text, frame_indices=None):
        # åŸºç¡€æ–‡æœ¬ç¼–ç 
        text_features = self.text_encoder(text).last_hidden_state
        
        # æ·»åŠ æ—¶åºæ ‡è®°
        if frame_indices is not None:
            # æ ¹æ®å¸§ç´¢å¼•é€‰æ‹©æ—¶åºæ ‡è®°
            temporal_embeds = self.temporal_tokens[:, frame_indices]
            text_features = text_features + temporal_embeds
        
        # æ—¶åºå¢å¼º
        for layer in self.temporal_layers:
            text_features = layer(text_features)
            
        return text_features
```

**åŠ¨ä½œè¯æå–ä¸å¯¹é½**ï¼š
```python
class ActionAlignment(nn.Module):
    """å¯¹é½æ–‡æœ¬ä¸­çš„åŠ¨ä½œæè¿°ä¸è§†é¢‘æ—¶åº"""
    def __init__(self):
        super().__init__()
        # åŠ¨ä½œè¯æ£€æµ‹
        self.action_detector = ActionWordDetector()
        
        # æ—¶åºåˆ†é…ç½‘ç»œ
        self.temporal_allocator = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1),  # é¢„æµ‹æ—¶é—´ä½ç½®
            nn.Sigmoid()
        )
        
    def forward(self, text_features, num_frames):
        # æ£€æµ‹åŠ¨ä½œè¯
        action_masks = self.action_detector(text_features)
        action_features = text_features * action_masks.unsqueeze(-1)
        
        # åˆ†é…åˆ°æ—¶é—´è½´
        temporal_positions = self.temporal_allocator(action_features)
        temporal_positions = temporal_positions * num_frames
        
        # ç”Ÿæˆæ—¶åºattention mask
        attention_weights = self.gaussian_attention(
            temporal_positions, 
            num_frames, 
            sigma=2.0
        )
        
        return attention_weights
    
    def gaussian_attention(self, centers, length, sigma):
        """ç”Ÿæˆé«˜æ–¯å½¢çŠ¶çš„æ³¨æ„åŠ›æƒé‡"""
        positions = torch.arange(length).float()
        weights = []
        
        for center in centers:
            weight = torch.exp(-(positions - center)**2 / (2 * sigma**2))
            weights.append(weight)
            
        return torch.stack(weights)
```

ğŸ’¡ **å…³é”®æŠ€å·§ï¼šæ—¶åºæç¤ºå·¥ç¨‹**  
æœ‰æ•ˆçš„è§†é¢‘ç”Ÿæˆæç¤ºéœ€è¦åŒ…å«ï¼š
- æ˜ç¡®çš„æ—¶åºè¯æ±‡ï¼ˆ"é¦–å…ˆ"ã€"ç„¶å"ã€"æœ€å"ï¼‰
- åŠ¨ä½œçš„æŒç»­æ—¶é—´ï¼ˆ"ç¼“æ…¢åœ°"ã€"å¿«é€Ÿåœ°"ï¼‰
- è¿åŠ¨æ–¹å‘ï¼ˆ"ä»å·¦åˆ°å³"ã€"å‘ä¸Š"ï¼‰

### 11.3.2 å›¾åƒåŠ¨ç”»åŒ–

å°†é™æ€å›¾åƒè½¬æ¢ä¸ºåŠ¨æ€è§†é¢‘ï¼š

**å›¾åƒç¼–ç ä¸è¿åŠ¨é¢„æµ‹**ï¼š
```python
class Image2Video(nn.Module):
    """å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ"""
    def __init__(self, image_encoder, motion_predictor, video_decoder):
        super().__init__()
        self.image_encoder = image_encoder
        self.motion_predictor = motion_predictor
        self.video_decoder = video_decoder
        
    def forward(self, image, text_prompt=None, num_frames=16):
        # ç¼–ç è¾“å…¥å›¾åƒ
        image_features = self.image_encoder(image)
        
        # é¢„æµ‹è¿åŠ¨æ¨¡å¼
        if text_prompt is not None:
            # æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨
            motion_params = self.motion_predictor(
                image_features, 
                text_prompt
            )
        else:
            # è‡ªåŠ¨è¿åŠ¨é¢„æµ‹
            motion_params = self.motion_predictor(image_features)
        
        # ç”Ÿæˆè§†é¢‘å¸§
        frames = [image]  # ç¬¬ä¸€å¸§æ˜¯åŸå§‹å›¾åƒ
        current_features = image_features
        
        for t in range(1, num_frames):
            # åº”ç”¨è¿åŠ¨
            current_features = self.apply_motion(
                current_features, 
                motion_params, 
                t
            )
            
            # è§£ç ä¸ºå›¾åƒ
            frame = self.video_decoder(current_features)
            frames.append(frame)
            
        return torch.stack(frames, dim=1)
    
    def apply_motion(self, features, motion_params, t):
        """åº”ç”¨è¿åŠ¨å˜æ¢"""
        # æå–è¿åŠ¨å‚æ•°
        translation = motion_params['translation'] * t
        rotation = motion_params['rotation'] * t
        scaling = motion_params['scaling'] ** t
        
        # åº”ç”¨ç©ºé—´å˜æ¢
        transformed = self.spatial_transform(
            features, 
            translation, 
            rotation, 
            scaling
        )
        
        return transformed
```

**è¿åŠ¨ç±»å‹åˆ†è§£**ï¼š
```python
class MotionDecomposition(nn.Module):
    """å°†å¤æ‚è¿åŠ¨åˆ†è§£ä¸ºåŸºæœ¬ç»„ä»¶"""
    def __init__(self):
        super().__init__()
        # å…¨å±€è¿åŠ¨ï¼ˆç›¸æœºè¿åŠ¨ï¼‰
        self.global_motion = GlobalMotionPredictor()
        
        # å±€éƒ¨è¿åŠ¨ï¼ˆå¯¹è±¡è¿åŠ¨ï¼‰
        self.local_motion = LocalMotionPredictor()
        
        # å½¢å˜è¿åŠ¨ï¼ˆéåˆšæ€§å˜å½¢ï¼‰
        self.deformation = DeformationPredictor()
        
    def forward(self, image_features, motion_type='mixed'):
        motions = {}
        
        if motion_type in ['global', 'mixed']:
            # é¢„æµ‹ç›¸æœºè¿åŠ¨
            motions['camera'] = self.global_motion(image_features)
            
        if motion_type in ['local', 'mixed']:
            # é¢„æµ‹å¯¹è±¡è¿åŠ¨
            motions['objects'] = self.local_motion(image_features)
            
        if motion_type in ['deform', 'mixed']:
            # é¢„æµ‹å½¢å˜
            motions['deformation'] = self.deformation(image_features)
            
        return motions

class GlobalMotionPredictor(nn.Module):
    """é¢„æµ‹ç›¸æœºè¿åŠ¨å‚æ•°"""
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(256, 512, 3, 2, 1),
            nn.ReLU(),
            nn.Conv2d(512, 1024, 3, 2, 1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
        
        self.predictor = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 6)  # 6-DOFç›¸æœºè¿åŠ¨
        )
        
    def forward(self, features):
        encoded = self.encoder(features).squeeze(-1).squeeze(-1)
        motion_params = self.predictor(encoded)
        
        return {
            'translation': motion_params[:, :3],
            'rotation': motion_params[:, 3:]
        }
```

ğŸ”¬ **ç ”ç©¶æŒ‘æˆ˜ï¼šè¿åŠ¨çš„æ­§ä¹‰æ€§**  
åŒä¸€å¼ å›¾åƒå¯èƒ½å¯¹åº”å¤šç§åˆç†çš„è¿åŠ¨ã€‚å¦‚ä½•å¤„ç†è¿™ç§å¤šæ¨¡æ€æ€§ï¼Ÿå¯ä»¥ä½¿ç”¨å˜åˆ†æ–¹æ³•æˆ–æ¡ä»¶æµåŒ¹é…æ¥å»ºæ¨¡è¿åŠ¨åˆ†å¸ƒã€‚

### 11.3.3 è¿åŠ¨è½¨è¿¹æ§åˆ¶

ç²¾ç¡®æ§åˆ¶è§†é¢‘ä¸­çš„è¿åŠ¨è·¯å¾„ï¼š

**è½¨è¿¹è¡¨ç¤ºä¸ç¼–ç **ï¼š
```python
class TrajectoryEncoder(nn.Module):
    """ç¼–ç ç”¨æˆ·æŒ‡å®šçš„è¿åŠ¨è½¨è¿¹"""
    def __init__(self, hidden_dim=256):
        super().__init__()
        # è½¨è¿¹ç‚¹ç¼–ç 
        self.point_encoder = nn.Sequential(
            nn.Linear(2, 64),  # 2Dåæ ‡
            nn.ReLU(),
            nn.Linear(64, hidden_dim)
        )
        
        # æ—¶åºç¼–ç 
        self.temporal_encoder = nn.LSTM(
            hidden_dim, 
            hidden_dim, 
            num_layers=2,
            bidirectional=True
        )
        
        # è½¨è¿¹åˆ°ç‰¹å¾å›¾çš„æ˜ å°„
        self.trajectory_to_feature = nn.Conv2d(
            hidden_dim * 2,  # åŒå‘LSTM
            256,
            kernel_size=1
        )
        
    def forward(self, trajectory_points, num_frames, feature_size):
        # trajectory_points: [B, N, 2] Nä¸ªè½¨è¿¹ç‚¹çš„åæ ‡
        
        # ç¼–ç è½¨è¿¹ç‚¹
        point_features = self.point_encoder(trajectory_points)
        
        # æ—¶åºç¼–ç 
        lstm_out, _ = self.temporal_encoder(point_features)
        
        # æ’å€¼åˆ°ç›®æ ‡å¸§æ•°
        trajectory_features = F.interpolate(
            lstm_out.transpose(1, 2),
            size=num_frames,
            mode='linear'
        ).transpose(1, 2)
        
        # è½¬æ¢ä¸ºç©ºé—´ç‰¹å¾å›¾
        spatial_features = self.render_trajectory_map(
            trajectory_features, 
            trajectory_points,
            feature_size
        )
        
        return spatial_features
    
    def render_trajectory_map(self, features, points, size):
        """å°†è½¨è¿¹æ¸²æŸ“ä¸ºç‰¹å¾å›¾"""
        H, W = size
        feature_map = torch.zeros(features.shape[0], features.shape[-1], H, W)
        
        for b in range(features.shape[0]):
            for t, (x, y) in enumerate(points[b]):
                # å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºç‰¹å¾å›¾åæ ‡
                x_idx = int(x * W)
                y_idx = int(y * H)
                
                # é«˜æ–¯æ•£å¸ƒ
                self.gaussian_splat(
                    feature_map[b], 
                    features[b, t], 
                    x_idx, 
                    y_idx,
                    sigma=3.0
                )
                
        return feature_map
```

**ç¨€ç–æ§åˆ¶ç‚¹æ’å€¼**ï¼š
```python
class SparseControlInterpolation(nn.Module):
    """ä»ç¨€ç–æ§åˆ¶ç‚¹ç”Ÿæˆå¯†é›†è¿åŠ¨åœº"""
    def __init__(self):
        super().__init__()
        self.flow_basis = FlowBasisNetwork()
        self.interpolator = BilinearInterpolator()
        
    def forward(self, control_points, control_flows, image_size):
        """
        control_points: [B, K, 2] Kä¸ªæ§åˆ¶ç‚¹ä½ç½®
        control_flows: [B, K, T, 2] æ¯ä¸ªæ§åˆ¶ç‚¹çš„è¿åŠ¨å‘é‡
        """
        B, K, T, _ = control_flows.shape
        H, W = image_size
        
        # ç”ŸæˆæµåŸºå‡½æ•°
        basis_flows = self.flow_basis(control_points, image_size)
        
        # ç¨€ç–åˆ°å¯†é›†æ’å€¼
        dense_flows = []
        for t in range(T):
            # å½“å‰æ—¶åˆ»çš„æ§åˆ¶æµ
            flows_t = control_flows[:, :, t, :]
            
            # åŠ æƒç»„åˆåŸºå‡½æ•°
            weights = self.compute_weights(control_points, (H, W))
            dense_flow = torch.sum(
                basis_flows * weights.unsqueeze(-1).unsqueeze(-1),
                dim=1
            )
            
            dense_flows.append(dense_flow)
            
        return torch.stack(dense_flows, dim=1)

class DragGAN(nn.Module):
    """åŸºäºæ‹–æ‹½çš„è§†é¢‘ç¼–è¾‘"""
    def __init__(self):
        super().__init__()
        self.feature_extractor = FeatureExtractor()
        self.motion_predictor = MotionPredictor()
        self.warping_module = WarpingModule()
        
    def forward(self, video, source_points, target_points):
        """
        source_points: ç”¨æˆ·é€‰æ‹©çš„æºç‚¹
        target_points: ç”¨æˆ·æ‹–æ‹½çš„ç›®æ ‡ç‚¹
        """
        # æå–ç‰¹å¾
        features = self.feature_extractor(video)
        
        # é¢„æµ‹æ•´ä½“è¿åŠ¨åœº
        motion_field = self.motion_predictor(
            features, 
            source_points, 
            target_points
        )
        
        # åº”ç”¨è¿åŠ¨
        edited_video = self.warping_module(video, motion_field)
        
        return edited_video
```

<details>
<summary>**ç»ƒä¹  11.3ï¼šå®ç°äº¤äº’å¼è§†é¢‘æ§åˆ¶**</summary>

è®¾è®¡å’Œå®ç°å„ç§è§†é¢‘æ§åˆ¶æœºåˆ¶ã€‚

1. **æ–‡æœ¬æ§åˆ¶å®éªŒ**ï¼š
   - å®ç°æ—¶åºæ„ŸçŸ¥çš„æ–‡æœ¬ç¼–ç å™¨
   - æµ‹è¯•ä¸åŒçš„åŠ¨ä½œè¯å¯¹é½ç­–ç•¥
   - è¯„ä¼°ç”Ÿæˆè§†é¢‘ä¸æ–‡æœ¬çš„ä¸€è‡´æ€§

2. **è¿åŠ¨è½¨è¿¹è®¾è®¡**ï¼š
   - å®ç°åŸºäºè´å¡å°”æ›²çº¿çš„è½¨è¿¹
   - æ”¯æŒå¤šå¯¹è±¡ç‹¬ç«‹è½¨è¿¹
   - å¤„ç†è½¨è¿¹å†²çªå’Œé®æŒ¡

3. **äº¤äº’å¼ç¼–è¾‘**ï¼š
   - å®ç°æ‹–æ‹½å¼è§†é¢‘ç¼–è¾‘
   - æ”¯æŒå±€éƒ¨åŒºåŸŸçš„è¿åŠ¨æ§åˆ¶
   - ä¿æŒæœªç¼–è¾‘åŒºåŸŸçš„ç¨³å®šæ€§

4. **å¤šæ¨¡æ€æ§åˆ¶**ï¼š
   - ç»“åˆæ–‡æœ¬ã€è½¨è¿¹ã€å‚è€ƒè§†é¢‘
   - è®¾è®¡æ§åˆ¶ä¿¡å·çš„èåˆç­–ç•¥
   - å¤„ç†å†²çªçš„æ§åˆ¶æŒ‡ä»¤

</details>

### 11.3.4 é£æ ¼ä¸å†…å®¹è§£è€¦

åˆ†ç¦»è§†é¢‘çš„å†…å®¹ï¼ˆä»€ä¹ˆï¼‰å’Œé£æ ¼ï¼ˆå¦‚ä½•ï¼‰ï¼š

```python
class StyleContentDiSffusion(nn.Module):
    """é£æ ¼-å†…å®¹è§£è€¦çš„è§†é¢‘æ‰©æ•£æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        # å†…å®¹ç¼–ç å™¨ï¼ˆæå–è¯­ä¹‰å’Œç»“æ„ï¼‰
        self.content_encoder = ContentEncoder()
        
        # é£æ ¼ç¼–ç å™¨ï¼ˆæå–è§†è§‰é£æ ¼ï¼‰
        self.style_encoder = StyleEncoder()
        
        # è¿åŠ¨ç¼–ç å™¨ï¼ˆæå–è¿åŠ¨æ¨¡å¼ï¼‰
        self.motion_encoder = MotionEncoder()
        
        # è§£è€¦çš„å»å™ªç½‘ç»œ
        self.denoiser = DisentangledDenoiser()
        
    def forward(self, x_t, t, content_ref=None, style_ref=None, motion_ref=None):
        # æå–å„ç§å‚è€ƒç‰¹å¾
        if content_ref is not None:
            content_features = self.content_encoder(content_ref)
        else:
            content_features = None
            
        if style_ref is not None:
            style_features = self.style_encoder(style_ref)
        else:
            style_features = None
            
        if motion_ref is not None:
            motion_features = self.motion_encoder(motion_ref)
        else:
            motion_features = None
            
        # è§£è€¦å»å™ª
        noise_pred = self.denoiser(
            x_t, t,
            content=content_features,
            style=style_features,
            motion=motion_features
        )
        
        return noise_pred

class DisentangledDenoiser(nn.Module):
    """æ”¯æŒé£æ ¼-å†…å®¹-è¿åŠ¨è§£è€¦çš„å»å™ªå™¨"""
    def __init__(self, base_channels=256):
        super().__init__()
        # åŸºç¡€U-Net
        self.unet = UNet3D(base_channels)
        
        # æ¡ä»¶æ³¨å…¥æ¨¡å—
        self.content_inject = FiLMLayer(base_channels)
        self.style_inject = AdaIN(base_channels)
        self.motion_inject = SpatioTemporalModulation(base_channels)
        
    def forward(self, x_t, t, content=None, style=None, motion=None):
        # åŸºç¡€ç‰¹å¾æå–
        features = self.unet.encoder(x_t, t)
        
        # æ³¨å…¥å„ç§æ¡ä»¶
        if content is not None:
            features = self.content_inject(features, content)
            
        if style is not None:
            features = self.style_inject(features, style)
            
        if motion is not None:
            features = self.motion_inject(features, motion)
            
        # è§£ç 
        output = self.unet.decoder(features)
        
        return output
```

**æ—¶åºä¸€è‡´çš„é£æ ¼è¿ç§»**ï¼š
```python
class TemporalStyleTransfer(nn.Module):
    """ä¿æŒæ—¶åºä¸€è‡´æ€§çš„è§†é¢‘é£æ ¼è¿ç§»"""
    def __init__(self):
        super().__init__()
        self.style_extractor = VGGStyleExtractor()
        self.temporal_consistency = TemporalConsistencyModule()
        
    def forward(self, content_video, style_image):
        B, T, C, H, W = content_video.shape
        
        # æå–é£æ ¼ç‰¹å¾
        style_features = self.style_extractor(style_image)
        
        # é€å¸§é£æ ¼åŒ–
        stylized_frames = []
        prev_frame = None
        
        for t in range(T):
            frame = content_video[:, t]
            
            # åº”ç”¨é£æ ¼
            stylized = self.apply_style(frame, style_features)
            
            # ä¿æŒæ—¶åºä¸€è‡´æ€§
            if prev_frame is not None:
                stylized = self.temporal_consistency(
                    stylized, 
                    prev_frame,
                    content_video[:, t-1],
                    content_video[:, t]
                )
            
            stylized_frames.append(stylized)
            prev_frame = stylized
            
        return torch.stack(stylized_frames, dim=1)
```

### 11.3.5 ç»†ç²’åº¦å±æ€§æ§åˆ¶

æ§åˆ¶è§†é¢‘çš„ç‰¹å®šå±æ€§ï¼š

```python
class AttributeController(nn.Module):
    """ç»†ç²’åº¦å±æ€§æ§åˆ¶"""
    def __init__(self, attributes=['speed', 'direction', 'intensity']):
        super().__init__()
        self.attributes = attributes
        
        # æ¯ä¸ªå±æ€§çš„ç¼–ç å™¨
        self.encoders = nn.ModuleDict({
            attr: self.build_encoder(attr) 
            for attr in attributes
        })
        
        # å±æ€§èåˆ
        self.fusion = AttributeFusion(len(attributes))
        
    def build_encoder(self, attribute):
        if attribute == 'speed':
            return SpeedEncoder()
        elif attribute == 'direction':
            return DirectionEncoder()
        elif attribute == 'intensity':
            return IntensityEncoder()
        else:
            return GenericAttributeEncoder()
            
    def forward(self, attribute_values):
        encoded_attrs = {}
        
        for attr, value in attribute_values.items():
            if attr in self.encoders:
                encoded_attrs[attr] = self.encoders[attr](value)
                
        # èåˆæ‰€æœ‰å±æ€§
        control_signal = self.fusion(encoded_attrs)
        
        return control_signal

class SpeedController(nn.Module):
    """æ§åˆ¶è§†é¢‘æ’­æ”¾é€Ÿåº¦å’Œè¿åŠ¨é€Ÿåº¦"""
    def __init__(self):
        super().__init__()
        self.speed_embedding = nn.Embedding(10, 256)  # 10ä¸ªé€Ÿåº¦çº§åˆ«
        self.temporal_modulator = TemporalModulator()
        
    def forward(self, video_features, speed_level):
        # è·å–é€Ÿåº¦åµŒå…¥
        speed_emb = self.speed_embedding(speed_level)
        
        # è°ƒåˆ¶æ—¶é—´ç»´åº¦
        modulated = self.temporal_modulator(video_features, speed_emb)
        
        return modulated
```

ğŸŒŸ **å‰æ²¿æ–¹å‘ï¼šå¯ç»„åˆçš„è§†é¢‘æ§åˆ¶**  
å¦‚ä½•è®¾è®¡ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒä»»æ„ç»„åˆçš„æ§åˆ¶ä¿¡å·ï¼ˆæ–‡æœ¬+è½¨è¿¹+é£æ ¼+å±æ€§ï¼‰ï¼Ÿè¿™éœ€è¦è§£å†³æ§åˆ¶ä¿¡å·çš„å¯¹é½ã€èåˆå’Œå†²çªè§£å†³ã€‚

### 11.3.6 ç‰©ç†çº¦æŸä¸çœŸå®æ„Ÿ

ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨ç¬¦åˆç‰©ç†è§„å¾‹ï¼š

```python
class PhysicsAwareGeneration(nn.Module):
    """ç‰©ç†æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆ"""
    def __init__(self):
        super().__init__()
        # ç‰©ç†æ¨¡æ‹Ÿå™¨
        self.physics_engine = DifferentiablePhysics()
        
        # ç‰©ç†å‚æ•°é¢„æµ‹
        self.physics_predictor = nn.Sequential(
            nn.Conv3d(256, 128, 3, 1, 1),
            nn.ReLU(),
            nn.Conv3d(128, 64, 3, 1, 1),
            nn.ReLU(),
            nn.Conv3d(64, 5, 1)  # è´¨é‡ã€æ‘©æ“¦ã€å¼¹æ€§ç­‰
        )
        
    def forward(self, initial_state, num_frames):
        # é¢„æµ‹ç‰©ç†å‚æ•°
        physics_params = self.physics_predictor(initial_state)
        
        # ç‰©ç†æ¨¡æ‹Ÿ
        states = [initial_state]
        for t in range(1, num_frames):
            # åº”ç”¨ç‰©ç†è§„å¾‹
            next_state = self.physics_engine.step(
                states[-1], 
                physics_params,
                dt=1.0/30  # 30 FPS
            )
            states.append(next_state)
            
        return torch.stack(states, dim=1)

class DifferentiablePhysics(nn.Module):
    """å¯å¾®åˆ†çš„ç‰©ç†æ¨¡æ‹Ÿ"""
    def __init__(self):
        super().__init__()
        
    def step(self, state, params, dt):
        # æå–ç‰©ç†é‡
        position = state['position']
        velocity = state['velocity']
        mass = params['mass']
        
        # è®¡ç®—åŠ›ï¼ˆé‡åŠ›ã€æ‘©æ“¦ç­‰ï¼‰
        forces = self.compute_forces(state, params)
        
        # æ›´æ–°é€Ÿåº¦å’Œä½ç½®ï¼ˆæ¬§æ‹‰ç§¯åˆ†ï¼‰
        acceleration = forces / mass
        new_velocity = velocity + acceleration * dt
        new_position = position + new_velocity * dt
        
        # å¤„ç†ç¢°æ’
        new_position, new_velocity = self.handle_collisions(
            new_position, 
            new_velocity,
            params
        )
        
        return {
            'position': new_position,
            'velocity': new_velocity
        }
```

é€šè¿‡è¿™äº›æ¡ä»¶æ§åˆ¶æœºåˆ¶ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åº¦å¯æ§å’ŒçœŸå®çš„åŠ¨æ€å†…å®¹ã€‚ä¸‹ä¸€èŠ‚å°†æ¢è®¨å¦‚ä½•é«˜æ•ˆåœ°è®­ç»ƒå’Œéƒ¨ç½²è¿™äº›æ¨¡å‹ã€‚