[← 上一章](chapter1.md) | 第2章 / 共14章 | [下一章 →](chapter3.md)

# 第2章：神经网络架构：U-Net与ViT

扩散模型的成功离不开强大的神经网络架构。有趣的是，扩散模型并没有发明全新的网络结构，而是巧妙地借用了计算机视觉领域的两个里程碑式架构：U-Net和Vision Transformer (ViT)。本章将追溯这两种架构的历史发展，理解它们的设计初衷，并剖析它们为何能与扩散模型的去噪任务完美契合。这种“历史的巧合”不仅展示了深度学习领域知识迁移的魅力，也为我们设计未来更高效的生成模型提供了深刻的启示。

## 2.1 从图像分割到去噪：U-Net的历史演变

U-Net最初并非为生成模型设计。2015年，Ronneberger等人在论文《U-Net: Convolutional Networks for Biomedical Image Segmentation》中提出U-Net，旨在解决生物医学图像分割的难题。其独特的编码器-解码器结构配合跳跃连接（skip-connections），能够在保留高分辨率细节信息的同时进行深度的语义特征提取。这种多尺度处理能力恰好契合了扩散模型的需求。

> **定义：历史脉络**
> - **2015年**：U-Net诞生，用于医学图像分割，在ISBI细胞追踪挑战赛中一举成名。
> - **2017-2019年**：U-Net被广泛应用于各种像素级预测任务，如语义分割、深度估计等，成为该领域的标准架构之一。
> - **2020年**：Ho等人在DDPM中采用U-Net作为核心的去噪网络，开启了U-Net在生成模型领域的辉煌篇章。
> - **2021年至今**：各种改进版U-Net成为主流扩散模型的标配，例如加入注意力机制、自适应归一化等，如ADM、Stable Diffusion等。

为什么U-Net特别适合扩散模型？关键在于扩散模型的去噪任务本质上是一个“图像到图像”的转换问题：输入一张噪声图像 $\mathbf{x}_t$ ，输出对噪声 $\boldsymbol{\epsilon}$ 的预测。这与U-Net最初设计的分割任务（输入：原始图像 → 输出：分割掩码）在结构上高度相似。U-Net的几个关键特性使其成为理想选择：

- **多尺度特征提取**：编码器通过逐步下采样，捕获从局部纹理到全局结构的各层次特征。
- **跳跃连接保留细节**：将编码器的浅层特征直接传递给解码器，有效避免了关键细节信息在下采样过程中的丢失，对于高质量生成至关重要。
- **对称的U形结构**：编码器和解码器的对称设计，在直觉上与“加噪”和“去噪”这两个互逆的过程相呼应。
- **参数效率**：相比于全连接网络，卷积结构的局部连接和参数共享大大减少了模型参数量，使其能够处理高分辨率图像。

## 2.2 U-Net架构详解

### 2.2.1 现代U-Net：为扩散模型重新设计

当DDPM的作者们在2020年选择U-Net作为去噪网络时，他们面临着与原始分割任务完全不同的需求。因此，一个为扩散模型“现代化”的U-Net诞生了，它融合了自2015年以来深度学习架构的诸多进展。

> **定义：扩散U-Net的关键改进**
> | 组件 | 原始U-Net (2015) | 扩散U-Net (2020+) |
> | :--- | :--- | :--- |
> | **卷积类型** | Valid卷积 (无padding) | Same卷积 (保持尺寸) |
> | **归一化** | 无 (或后期加入BatchNorm) | GroupNorm (小批量稳定) |
> | **激活函数** | ReLU | SiLU / Swish (更平滑) |
> | **残差连接** | 无 | 每个块内部都有 (类ResNet) |
> | **注意力机制** | 无 | 多分辨率自注意力 |
> | **条件机制** | 无需条件 | 时间嵌入 (必需) |

让我们深入理解几个关键改进：

#### 1. 残差块 (ResNet Block)
现代U-Net的基本构建单元不再是简单的卷积层，而是借鉴了ResNet的残差块。一个典型的块流程如下：
1.  输入 `x` 首先通过 `GroupNorm` 和 `SiLU` 激活函数。
2.  经过一个3x3的 `Conv2d` 层。
3.  再次通过 `GroupNorm` 和 `SiLU`。
4.  经过第二个3x3的 `Conv2d` 层。
5.  将处理后的结果与原始输入 `x` 相加（残差连接）。

#### 2. 时间嵌入注入 (Time Embedding)
时间步 `t` 的信息至关重要。它通常通过一个小型MLP从正弦编码转换为嵌入向量，然后通过自适应归一化层（Adaptive Group Normalization, AdaGN）注入到每个残差块中。其核心思想是调制残差块的统计特性：
`h_out = GroupNorm(h_in) * (1 + scale(t)) + shift(t)`
其中 `scale(t)` 和 `shift(t)` 是从时间嵌入向量线性变换得到的。

#### 3. 自注意力 (Self-Attention)
为了捕获长程依赖关系，自注意力机制被引入到U-Net中。但由于其计算复杂度与像素数的平方成正比，它通常只在特征图分辨率较低的层级（如16x16或8x8）使用，以在计算效率和全局建模能力之间取得平衡。

### 2.2.2 采样方式的演进：从池化到可学习的卷积

“如何正确地降低和恢复分辨率”是U-Net设计的核心问题之一，其演进过程反映了深度学习架构设计的范式转变。

#### 下采样：从固定到可学习
- **最大池化 (Max Pooling)**：早期CNN（包括原始U-Net）的标配。通过`nn.MaxPool2d`实现。优点是计算简单，但缺点是它是一个固定的、不可学习的操作，会丢弃大量位置信息，对生成任务不利。
- **步进卷积 (Strided Convolution)**：DCGAN（2015）推广了这一理念，即用一个`stride=2`的`nn.Conv2d`来代替池化，让网络自己学习如何下采样。这是现代架构的标准。
- **现代最佳实践**：为了更稳定的训练，通常将下采样分解为两步：一个标准的`nn.Conv2d`用于特征变换，然后接一个`stride=2`的`nn.Conv2d`专门用于下采样。

#### 上采样：避免棋盘效应
- **转置卷积 (Transposed Convolution)**：作为步进卷积的逆操作，`nn.ConvTranspose2d`是自然的选择。然而，当其`kernel_size`不能被`stride`整除时，会因重叠不均而产生棋盘状的伪影（Checkerboard Artifacts），严重影响生成质量。
- **现代最佳实践**：为了彻底避免棋盘效应，现代架构倾向于将上采样和卷积解耦。最常用的方法是：先使用简单的最近邻插值（`nn.Upsample(scale_factor=2, mode='nearest')`）放大特征图，然后再进行一次标准的`nn.Conv2d`。这种方法简单、有效且不会产生伪影。

### 2.2.3 归一化技术：从BatchNorm到AdaGN的演进

归一化是深度学习中最关键也最令人困惑的组件之一。它在扩散模型中的演进尤为重要。

- **为什么不用BatchNorm？** `nn.BatchNorm2d`对扩散模型有几个致命问题：1) **批次依赖**，导致训练和推理行为不一致；2) **小批量下性能退化**，而扩散模型常因内存限制使用小批量；3) **混淆时间步信息**，将不同时间步的样本统计量混合在一起。

- **GroupNorm成为标准**：`nn.GroupNorm`解决了上述所有问题。它在每个样本内部、沿通道的分组上计算统计量，与批次大小无关。通过调整组数（`num_groups`），可以在`LayerNorm`（G=1）和`InstanceNorm`（G=C）之间进行权衡，通常`num_groups=32`是一个鲁棒的选择。

- **自适应归一化 (AdaGN) 的威力**：StyleGAN的革命性发现是，归一化可以作为注入外部信息的强大机制。扩散模型借鉴了这一思想，使用时间嵌入`t`来调制GroupNorm的增益（`scale`）和偏置（`shift`）。这比简单地将时间嵌入拼接到特征图上更有效，因为它允许模型根据时间步`t`动态地调整每一层的“去噪强度”。

<details>
<summary><strong>练习 2.1：U-Net架构的权衡分析</strong></summary>

1.  **深度 vs. 宽度**：分析U-Net的深度（下采样次数）和宽度（基础通道数）对模型性能和计算成本的影响。对于一个固定计算预算的模型，是更深好还是更宽好？
2.  **注意力位置**：讨论在U-Net的不同层级（高、中、低分辨率）插入自注意力模块的利弊。为什么大多数模型选择在中低分辨率层插入？
3.  **跳跃连接**：标准的跳跃连接使用拼接（concatenation）。分析如果改为逐元素相加（addition）会对信息流产生什么影响。在什么情况下相加可能是更好的选择？
4.  **开放探索**：设计一种“动态U-Net”，其深度或宽度可以根据输入的时间步`t`自适应调整。例如，在噪声水平高时使用更深的网络来捕捉全局结构，在噪声水平低时使用更浅的网络来关注细节。
5.  **研究思路**：
    *   查阅有关神经架构搜索（NAS）在生成模型中应用的研究。
    *   从信息论角度分析跳跃连接，将其视为信息瓶颈的旁路。
    *   研究不同归一化层（如`RMSNorm`）与自适应调制结合的可能性。

</details>

## 2.3 从NLP到CV：Vision Transformer的跨界之旅

### 2.3.1 Transformer的视觉革命

Transformer架构由Vaswani等人在2017年的论文《Attention Is All You Need》中为自然语言处理提出。2020年，Dosovitskiy等人的ViT论文证明了纯Transformer架构在图像分类上可以达到甚至超越顶尖的CNN，开启了Transformer在计算机视觉领域的革命。

ViT的核心思想极其简洁：
1.  将输入图像分割成固定大小的patches（例如16×16像素）。
2.  将每个patch线性投影（embedding）为一个向量（token）。
3.  将这些tokens序列以及一个可学习的`[CLS]` token输入到标准的Transformer编码器中。
4.  使用Transformer输出的`[CLS]` token进行分类。

这种设计的优雅之处在于它为视觉问题引入了新的归纳偏置：**世界是由可组合的“部件”构成的**。

### 2.3.2 扩散Transformer (DiT)

2022年，Peebles和Xie在论文《Scalable Diffusion Models with Transformers》中提出了DiT，成功将ViT架构应用于扩散模型。DiT对ViT进行了关键改造以适应去噪任务：

1.  **输入处理**：输入不再是清晰图像，而是带噪声的图像patches。
2.  **无`[CLS]` Token**：生成任务需要对每个patch进行预测，因此去除了分类任务专用的`[CLS]` token。
3.  **条件注入**：时间步`t`和类别标签`c`的嵌入向量被视为额外的条件tokens，通过自适应LayerNorm（AdaLN）或交叉注意力（cross-attention）注入到模型中。
4.  **输出处理**：Transformer的输出tokens被重新排列，并通过一个线性解码器预测每个patch对应的噪声。

DiT的成功，特别是其卓越的可扩展性（scaling law），使其迅速成为SOTA文生图模型（如Sora, Stable Diffusion 3）的首选架构。

<details>
<summary><strong>练习 2.2：比较U-Net和DiT的归纳偏置与复杂度</strong></summary>

1.  **归纳偏置**：对比CNN（U-Net的基础）和Transformer（DiT的基础）的核心归纳偏置。CNN的“局部性”和“平移等变性”与Transformer的“全局关系”和“排列不变性”分别如何影响它们作为去噪网络的性能？
2.  **计算复杂度**：对于一个分辨率为`H x W`的输入，推导U-Net和DiT的主要计算瓶颈。U-Net的复杂度与什么成正比？DiT的复杂度与什么成正比？（提示：考虑卷积操作和自注意力操作的复杂度）
3.  **开放探索**：U-Net和DiT代表了两种不同的架构范式。近年来，出现了许多试图结合两者优点的混合架构（如U-ViT）。分析这种混合设计的动机，并提出一种你自己的混合块（hybrid block）设计。
4.  **研究思路**：
    *   阅读ViT和DiT的原文，关注作者关于模型扩展性（scaling）的实验部分。
    *   探索卷积操作和自注意力在数学上的联系（例如，卷积可以被看作是一种特殊的、带强位置偏置的局部注意力）。
    *   研究最新的SOTA生成模型（如Sora的技术报告），分析其架构选择。

</details>

## 2.4 性能优化与实用技巧

理论架构和实际部署之间往往存在巨大鸿沟。本节分享一些在实践中积累的优化技巧。

### 2.4.1 内存优化：在GPU上塞下更大的模型

训练扩散模型时，内存的最大消耗通常来自**激活值**，特别是U-Net中为跳跃连接而保存的各层特征图。

- **梯度检查点 (Gradient Checkpointing)**：核心思想是“用计算换内存”。通过`torch.utils.checkpoint.checkpoint`包裹模型的一部分（如一个ResBlock），在前向传播时不保存其内部的激活值，而在反向传播时重新计算它们。这可以显著降低内存占用（约30-50%），但会增加训练时间（约20-30%）。

- **混合精度训练 (Mixed Precision)**：使用`torch.cuda.amp`（自动混合精度）可以利用现代GPU的Tensor Cores，将大部分计算从FP32转为FP16或BF16，内存减半，速度翻倍。关键是使用`GradScaler`来防止FP16梯度下溢。

- **注意力优化**：标准自注意力的内存和计算复杂度与序列长度的平方成正比。对于高分辨率图像，这很快会成为瓶颈。FlashAttention等库通过融合内核操作，避免将巨大的注意力矩阵写入和读出GPU内存，从而实现显著的加速和内存节省。

### 2.4.2 训练稳定性：让大模型稳定收敛

- **初始化策略**：一个关键技巧是**将输出层的权重和偏置初始化为零**。这确保模型在训练开始时输出为零，即预测的噪声为零。这是一种“无为而治”的初始化，使得模型在学习初期不会对输入造成巨大扰动，有助于稳定训练。

- **数值稳定性**：
    - **梯度裁剪**：通过`torch.nn.utils.clip_grad_norm_`来防止梯度爆炸，是训练大模型的标配。
    - **学习率调度**：使用预热（warmup）和余弦退火（cosine decay）的学习率调度器通常比固定学习率效果更好。
    - **AdamW优化器**：AdamW通过解耦权重衰减和梯度更新，通常比标准Adam更稳定。

<details>
<summary><strong>综合练习：为特定任务设计去噪网络</strong></summary>

假设你要为以下两种不同的任务设计去噪网络架构，你会如何选择和修改U-Net或DiT？请详细说明理由。

**任务A：移动端实时人像风格化**
- **约束**：模型大小 < 50MB，在手机GPU上推理延迟 < 100ms。
- **数据**：512x512的人像图片。

**任务B：生成具有复杂物理规律的科学模拟数据（如流体动力学）**
- **约束**：追求最高的物理保真度，计算资源几乎无限。
- **数据**：256x256x256的3D体数据，需要尊重物理守恒定律。

**设计分析与研究方向：**
1.  **架构选择**：为每个任务选择基础架构（U-Net, DiT, 或混合架构），并论证你的选择。
2.  **关键修改**：针对每个任务的约束和数据特性，你会对所选架构进行哪些关键修改？（例如，对于任务A，如何修改通道数、深度、注意力机制？对于任务B，如何处理3D数据、如何引入物理约束？）
3.  **理论空白**：在任务B中，如何设计一个能内建物理不变量（如散度为零）的神经网络架构？这被称为物理信息神经网络（PINN）与生成模型的交叉领域，是一个活跃的研究方向。
4.  **研究思路**：
    *   查阅有关模型量化、剪枝和知识蒸馏的文献，以满足任务A的部署要求。
    *   研究傅里叶神经算子（Fourier Neural Operator）等将物理方程求解器与神经网络结合的工作，以获取任务B的灵感。
    *   探索等变神经网络（Equivariant Neural Networks），它们被设计用来尊重数据的内在对称性（如旋转不变性）。

</details>

## 本章小结

本章我们追溯了扩散模型中两种主流架构的历史渊源，并深入分析了它们的设计细节和演进过程。

- **U-Net**：从2015年的医学图像分割任务，到2020年成为DDPM的核心架构，其多尺度特征融合能力是成功的关键。
- **Vision Transformer (DiT)**：从2017年的NLP革命，经2020年的CV突破，到2022年成为可扩展扩散模型的主流选择，其全局关系建模能力和卓越的扩展性是其优势所在。

这两种架构能够成功应用于扩散模型并非偶然，而是经过了精心的改造和适配：
- **共同的改造**：都引入了时间嵌入作为关键的条件信息，并发展出如AdaGN/AdaLN等高效的注入机制。
- **不同的演进**：U-Net在卷积、采样和归一化等模块上不断优化；DiT则专注于如何将Transformer范式更好地应用于像素级的生成任务。

扩散模型的架构演进史启示我们：创新并不总是需要“从零开始”。善于发现和利用已有技术的潜力，通过巧妙的改造和组合，往往能产生意想不到的突破。

下一章，我们将深入DDPM的数学原理，看看这些强大的架构是如何在一个清晰的概率框架下进行训练和优化的。
