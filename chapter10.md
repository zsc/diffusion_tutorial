[â† è¿”å›ç›®å½•](index.md) | ç¬¬10ç«  / å…±14ç«  | [ä¸‹ä¸€ç«  â†’](chapter11.md)

# ç¬¬10ç« ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM)

æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Models, LDMï¼‰æ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªé©å‘½æ€§è¿›å±•ï¼Œå®ƒé€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´è€ŒéåŸå§‹åƒç´ ç©ºé—´è¿›è¡Œæ‰©æ•£ï¼Œæå¤§åœ°æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨LDMçš„æ ¸å¿ƒæ€æƒ³ï¼ŒåŒ…æ‹¬è‡ªç¼–ç å™¨çš„è®¾è®¡ã€æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§ã€ä»¥åŠå¦‚ä½•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°æ•°é‡çº§çš„åŠ é€Ÿã€‚æ‚¨å°†ç†è§£Stable DiffusionèƒŒåçš„æŠ€æœ¯åŸç†ï¼ŒæŒæ¡è®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€å·§ï¼Œå¹¶å­¦ä¹ å¦‚ä½•æƒè¡¡å‹ç¼©ç‡ä¸é‡å»ºè´¨é‡ã€‚

## ç« èŠ‚å¤§çº²

### 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´
- é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜
- æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿
- æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©
- LDMçš„æ•´ä½“æ¶æ„

### 10.2 è‡ªç¼–ç å™¨è®¾è®¡
- VQ-VAE vs KL-VAE
- æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ
- æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

### 10.3 æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£
- æ½œåœ¨æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æè¿°
- å™ªå£°è°ƒåº¦çš„é€‚é…
- æ¡ä»¶æœºåˆ¶åœ¨æ½œåœ¨ç©ºé—´çš„å®ç°
- è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

### 10.4 Stable Diffusionæ¶æ„è¯¦è§£
- æ¨¡å‹ç»„ä»¶åˆ†æ
- CLIPæ–‡æœ¬ç¼–ç å™¨é›†æˆ
- äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
- æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### 10.5 å®è·µè€ƒè™‘ä¸æ‰©å±•
- ä¸åŒåˆ†è¾¨ç‡çš„å¤„ç†
- å¾®è°ƒä¸é€‚é…
- æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²
- æœªæ¥å‘å±•æ–¹å‘

## 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´

### 10.1.1 é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜

åœ¨åƒç´ ç©ºé—´ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹é¢ä¸´ä¸¥é‡çš„è®¡ç®—ç“¶é¢ˆï¼š

**è®¡ç®—å¤æ‚åº¦åˆ†æ**ï¼š
- 512Ã—512 RGBå›¾åƒï¼š786,432ç»´
- 1024Ã—1024 RGBå›¾åƒï¼š3,145,728ç»´
- U-Netçš„è®¡ç®—é‡ï¼š $O(n^2)$ å¯¹äºè‡ªæ³¨æ„åŠ›å±‚

å…·ä½“æ•°å­—ï¼š
```python
# åƒç´ ç©ºé—´æ‰©æ•£çš„å†…å­˜éœ€æ±‚
def compute_memory_requirements(h, w, c=3, batch_size=1):
    # è¾“å…¥å¼ é‡
    input_size = batch_size * c * h * w * 4  # float32
    
    # U-Netä¸­é—´ç‰¹å¾ï¼ˆå‡è®¾æœ€å¤§é€šé“æ•°2048ï¼‰
    feature_size = batch_size * 2048 * (h//8) * (w//8) * 4
    
    # è‡ªæ³¨æ„åŠ›çŸ©é˜µ
    seq_len = (h//8) * (w//8)
    attention_size = batch_size * seq_len * seq_len * 4
    
    total_gb = (input_size + feature_size + attention_size) / (1024**3)
    return total_gb

# 1024x1024å›¾åƒéœ€è¦çº¦48GBå†…å­˜ï¼
```

### 10.1.2 æ½œåœ¨ç©ºé—´çš„æ ¸å¿ƒä¼˜åŠ¿

LDMé€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œè·å¾—å¤šä¸ªä¼˜åŠ¿ï¼š

1. **è®¡ç®—æ•ˆç‡**ï¼š8å€ä¸‹é‡‡æ ·å‡å°‘64å€è®¡ç®—é‡
2. **è¯­ä¹‰å‹ç¼©**ï¼šæ½œåœ¨è¡¨ç¤ºæ›´æ¥è¿‘è¯­ä¹‰ä¿¡æ¯
3. **æ›´å¥½çš„å½’çº³åç½®**ï¼šè‡ªç„¶å›¾åƒçš„ä½ç»´æµå½¢å‡è®¾
4. **æ¨¡å—åŒ–è®¾è®¡**ï¼šåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆä»»åŠ¡

**å‹ç¼©ç‡vsè´¨é‡çš„æƒè¡¡**ï¼š
```
ä¸‹é‡‡æ ·å› å­ | æ½œåœ¨ç»´åº¦ | åŠ é€Ÿæ¯” | é‡å»ºPSNR
    4       |  64Ã—64   |  16x   |  >30dB
    8       |  32Ã—32   |  64x   |  ~27dB
   16       |  16Ã—16   | 256x   |  ~23dB
```

### 10.1.3 æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©

LDMçš„å…³é”®æ´å¯Ÿæ˜¯åŒºåˆ†ä¸¤ç§å‹ç¼©ï¼š

**ä¿¡æ¯å‹ç¼©**ï¼ˆä¼ ç»Ÿå‹ç¼©ï¼‰ï¼š
- ç›®æ ‡ï¼šå®Œç¾é‡å»ºæ¯ä¸ªåƒç´ 
- æ–¹æ³•ï¼šç†µç¼–ç ã€é¢„æµ‹ç¼–ç 
- é—®é¢˜ï¼šä¿ç•™äº†æ„ŸçŸ¥ä¸é‡è¦çš„ç»†èŠ‚

**æ„ŸçŸ¥å‹ç¼©**ï¼ˆLDMä½¿ç”¨ï¼‰ï¼š
- ç›®æ ‡ï¼šä¿ç•™æ„ŸçŸ¥é‡è¦çš„ç‰¹å¾
- æ–¹æ³•ï¼šå­¦ä¹ çš„ç¼–ç å™¨ + æ„ŸçŸ¥æŸå¤±
- ä¼˜åŠ¿ï¼šæ›´é«˜å‹ç¼©ç‡ï¼Œæ›´è¯­ä¹‰åŒ–çš„è¡¨ç¤º

```python
class PerceptualCompression(nn.Module):
    def __init__(self, perceptual_weight=1.0):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg')
        self.perceptual_weight = perceptual_weight
    
    def forward(self, x, x_recon):
        # åƒç´ çº§æŸå¤±
        pixel_loss = F.l1_loss(x, x_recon)
        
        # æ„ŸçŸ¥æŸå¤±
        perceptual_loss = self.perceptual_loss(x, x_recon)
        
        # ç»„åˆ
        return pixel_loss + self.perceptual_weight * perceptual_loss
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜å‹ç¼©ç‡**  
ä»€ä¹ˆå†³å®šäº†æœ€ä¼˜çš„å‹ç¼©ç‡ï¼Ÿæ˜¯å¦å¯ä»¥æ ¹æ®æ•°æ®é›†ç‰¹æ€§è‡ªé€‚åº”é€‰æ‹©ï¼Ÿè¿™æ¶‰åŠåˆ°ç‡å¤±çœŸç†è®ºå’Œæµå½¢å‡è®¾ã€‚

### 10.1.4 LDMçš„æ•´ä½“æ¶æ„

LDMç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ„æˆï¼š

```python
class LatentDiffusionModel(nn.Module):
    def __init__(self, autoencoder, diffusion_model, conditioning_model):
        super().__init__()
        self.autoencoder = autoencoder  # ç¼–ç /è§£ç å›¾åƒ
        self.diffusion = diffusion_model  # æ½œåœ¨ç©ºé—´æ‰©æ•£
        self.cond_model = conditioning_model  # å¤„ç†æ¡ä»¶ä¿¡æ¯
        
        # å†»ç»“è‡ªç¼–ç å™¨ï¼ˆé€šå¸¸é¢„è®­ç»ƒï¼‰
        self.autoencoder.eval()
        for param in self.autoencoder.parameters():
            param.requires_grad = False
    
    def encode(self, x):
        # å›¾åƒ -> æ½œåœ¨è¡¨ç¤º
        with torch.no_grad():
            z = self.autoencoder.encode(x)
            # å¯é€‰ï¼šæ ‡å‡†åŒ–
            z = z * self.scale_factor
        return z
    
    def decode(self, z):
        # æ½œåœ¨è¡¨ç¤º -> å›¾åƒ
        z = z / self.scale_factor
        with torch.no_grad():
            x = self.autoencoder.decode(z)
        return x
```

<details>
<summary>**ç»ƒä¹  10.1ï¼šåˆ†æå‹ç¼©æ•ˆç‡**</summary>

ç ”ç©¶ä¸åŒå‹ç¼©ç­–ç•¥çš„æ•ˆæœã€‚

1. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - å®ç°ä¸åŒä¸‹é‡‡æ ·ç‡çš„è‡ªç¼–ç å™¨
   - æµ‹é‡é‡å»ºè´¨é‡ï¼ˆPSNR, SSIM, LPIPSï¼‰
   - ç»˜åˆ¶ç‡å¤±çœŸæ›²çº¿

2. **è¯­ä¹‰ä¿ç•™åˆ†æ**ï¼š
   - ä½¿ç”¨é¢„è®­ç»ƒåˆ†ç±»å™¨è¯„ä¼°è¯­ä¹‰ä¿ç•™
   - æ¯”è¾ƒåƒç´ MSE vs æ„ŸçŸ¥æŸå¤±
   - åˆ†æå“ªäº›ç‰¹å¾è¢«ä¿ç•™/ä¸¢å¤±

3. **è®¡ç®—æ•ˆç›Šè¯„ä¼°**ï¼š
   - æµ‹é‡ä¸åŒåˆ†è¾¨ç‡çš„æ¨ç†æ—¶é—´
   - è®¡ç®—å†…å­˜ä½¿ç”¨
   - æ‰¾å‡ºæ•ˆç‡ç“¶é¢ˆ

4. **ç†è®ºæ‹“å±•**ï¼š
   - ä»æµå½¢å‡è®¾è§’åº¦åˆ†æå‹ç¼©
   - ç ”ç©¶æœ€ä¼˜ä¼ è¾“ç†è®ºçš„åº”ç”¨
   - æ¢ç´¢è‡ªé€‚åº”å‹ç¼©ç‡

</details>

### 10.1.5 ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

LDMé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆï¼š

**ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒè‡ªç¼–ç å™¨**
```python
def train_autoencoder(model, dataloader, epochs):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        for x in dataloader:
            # ç¼–ç -è§£ç 
            z = model.encode(x)
            x_recon = model.decode(z)
            
            # é‡å»ºæŸå¤±
            recon_loss = F.l1_loss(x, x_recon)
            
            # æ„ŸçŸ¥æŸå¤±
            p_loss = perceptual_loss(x, x_recon)
            
            # KLæ­£åˆ™åŒ–ï¼ˆå¦‚æœä½¿ç”¨VAEï¼‰
            kl_loss = model.kl_loss(z)
            
            loss = recon_loss + 0.1 * p_loss + 0.001 * kl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

**ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹**
```python
def train_diffusion(diffusion_model, autoencoder, dataloader):
    # å†»ç»“è‡ªç¼–ç å™¨
    autoencoder.eval()
    
    for x, c in dataloader:
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            z = autoencoder.encode(x)
        
        # æ ‡å‡†æ‰©æ•£è®­ç»ƒ
        t = torch.randint(0, num_steps, (z.shape[0],))
        noise = torch.randn_like(z)
        z_t = add_noise(z, noise, t)
        
        # é¢„æµ‹å™ªå£°
        pred_noise = diffusion_model(z_t, t, c)
        loss = F.mse_loss(pred_noise, noise)
        
        loss.backward()
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šé¢„è®­ç»ƒç­–ç•¥**  
å¯ä»¥ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒé€šç”¨è‡ªç¼–ç å™¨ï¼Œç„¶ååœ¨ç‰¹å®šé¢†åŸŸå¾®è°ƒã€‚è¿™å¤§å¤§å‡å°‘äº†è®­ç»ƒæˆæœ¬ã€‚

### 10.1.6 æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§

ç†æƒ³çš„æ½œåœ¨ç©ºé—´åº”å…·å¤‡ï¼š

1. **å¹³æ»‘æ€§**ï¼šç›¸è¿‘çš„æ½œåœ¨ç¼–ç å¯¹åº”ç›¸ä¼¼çš„å›¾åƒ
2. **è¯­ä¹‰æ€§**ï¼šæ½œåœ¨ç»´åº¦å¯¹åº”æœ‰æ„ä¹‰çš„å˜åŒ–
3. **ç´§å‡‘æ€§**ï¼šé«˜æ•ˆåˆ©ç”¨æ¯ä¸ªç»´åº¦
4. **æ­£æ€æ€§**ï¼šä¾¿äºæ‰©æ•£æ¨¡å‹å»ºæ¨¡

**åˆ†ææ½œåœ¨ç©ºé—´**ï¼š
```python
def analyze_latent_space(autoencoder, dataloader):
    latents = []
    labels = []
    
    with torch.no_grad():
        for x, y in dataloader:
            z = autoencoder.encode(x)
            latents.append(z.cpu())
            labels.append(y.cpu())
    
    latents = torch.cat(latents)
    labels = torch.cat(labels)
    
    # ç»Ÿè®¡ç‰¹æ€§
    print(f"Mean: {latents.mean():.3f}")
    print(f"Std: {latents.std():.3f}")
    print(f"Kurtosis: {stats.kurtosis(latents.numpy().flatten()):.3f}")
    
    # å¯è§†åŒ–ï¼ˆä½¿ç”¨t-SNEæˆ–UMAPï¼‰
    embedded = TSNE(n_components=2).fit_transform(latents.numpy())
    plt.scatter(embedded[:, 0], embedded[:, 1], c=labels)
```

ğŸŒŸ **å¼€æ”¾é—®é¢˜ï¼šæœ€ä¼˜æ½œåœ¨ç©ºé—´è®¾è®¡**  
å¦‚ä½•è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„æ½œåœ¨ç©ºé—´ï¼Ÿèƒ½å¦å­¦ä¹ è§£è€¦çš„è¡¨ç¤ºï¼Ÿè¿™æ¶‰åŠåˆ°è¡¨ç¤ºå­¦ä¹ å’Œå› æœæ¨æ–­çš„å‰æ²¿ç ”ç©¶ã€‚

## 10.3 æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£

### 10.3.1 æ½œåœ¨æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æè¿°

åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£éœ€è¦é‡æ–°å®šä¹‰å‰å‘å’Œåå‘è¿‡ç¨‹ï¼š

**å‰å‘è¿‡ç¨‹**ï¼š
$$q(\mathbf{z}_t | \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_t; \sqrt{\bar{\alpha}_t}\mathbf{z}_0, (1-\bar{\alpha}_t)\mathbf{I})$$

å…¶ä¸­ $\mathbf{z}_0 = \mathcal{E}(\mathbf{x})$ æ˜¯ç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºã€‚

**å…³é”®å·®å¼‚**ï¼š
1. **ç»´åº¦é™ä½**ï¼šä» $\mathbb{R}^{3 \times H \times W}$ åˆ° $\mathbb{R}^{C \times h \times w}$
2. **åˆ†å¸ƒå˜åŒ–**ï¼šæ½œåœ¨ç©ºé—´å¯èƒ½ä¸å®Œå…¨ç¬¦åˆé«˜æ–¯åˆ†å¸ƒ
3. **å°ºåº¦å·®å¼‚**ï¼šéœ€è¦é€‚å½“çš„å½’ä¸€åŒ–

**åå‘è¿‡ç¨‹**ï¼š
$$p_\theta(\mathbf{z}_{t-1} | \mathbf{z}_t) = \mathcal{N}(\mathbf{z}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{z}_t, t), \sigma_t^2\mathbf{I})$$

æ‰©æ•£æ¨¡å‹å­¦ä¹ é¢„æµ‹å™ªå£° $\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t)$ ï¼Œç”¨äºè®¡ç®—å‡å€¼ï¼š
$$\boldsymbol{\mu}_\theta(\mathbf{z}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{z}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t)\right)$$

### 10.3.2 å™ªå£°è°ƒåº¦çš„é€‚é…

æ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§ä¸åƒç´ ç©ºé—´ä¸åŒï¼Œéœ€è¦è°ƒæ•´å™ªå£°è°ƒåº¦ï¼š

**1. ä¿¡å™ªæ¯”åˆ†æ**ï¼š
```python
def analyze_latent_snr(autoencoder, dataloader):
    latents = []
    with torch.no_grad():
        for x, _ in dataloader:
            z = autoencoder.encode(x)
            latents.append(z)
    
    latents = torch.cat(latents)
    
    # è®¡ç®—ä¿¡å·åŠŸç‡
    signal_power = (latents ** 2).mean()
    
    # åˆ†æä¸åŒå™ªå£°æ°´å¹³çš„SNR
    for t in [0.1, 0.5, 0.9]:
        noise_power = (1 - t) * signal_power
        snr = 10 * torch.log10(signal_power / noise_power)
        print(f"t={t}: SNR={snr:.2f} dB")
```

**2. è‡ªé€‚åº”è°ƒåº¦**ï¼š
```python
class AdaptiveNoiseSchedule:
    def __init__(self, latent_stats):
        self.mean = latent_stats['mean']
        self.std = latent_stats['std']
        
    def get_betas(self, num_steps):
        # æ ¹æ®æ½œåœ¨ç©ºé—´ç»Ÿè®¡è°ƒæ•´beta
        # ç¡®ä¿æœ€ç»ˆSNRæ¥è¿‘0
        target_final_snr = 0.001
        beta_start = 0.0001 * self.std
        beta_end = 0.02 * self.std
        
        return torch.linspace(beta_start, beta_end, num_steps)
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šé¢„è®¡ç®—ç»Ÿè®¡é‡**  
åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®¡ç®—æ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨äºå½’ä¸€åŒ–å’Œå™ªå£°è°ƒåº¦è®¾è®¡ã€‚

### 10.3.3 æ¡ä»¶æœºåˆ¶åœ¨æ½œåœ¨ç©ºé—´çš„å®ç°

LDMä¸­çš„æ¡ä»¶ä¿¡æ¯é€šè¿‡å¤šç§æ–¹å¼æ³¨å…¥ï¼š

**1. äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**ï¼š
```python
class CrossAttentionBlock(nn.Module):
    def __init__(self, dim, context_dim, num_heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            dim, num_heads, kdim=context_dim, vdim=context_dim
        )
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
    def forward(self, x, context):
        # x: [B, H*W, C] æ½œåœ¨ç‰¹å¾
        # context: [B, L, D] æ¡ä»¶ç¼–ç ï¼ˆå¦‚æ–‡æœ¬ï¼‰
        
        x_norm = self.norm1(x)
        attn_out = self.attention(x_norm, context, context)[0]
        x = x + attn_out
        return x
```

**2. ç‰¹å¾è°ƒåˆ¶ï¼ˆFiLMï¼‰**ï¼š
```python
class FiLMLayer(nn.Module):
    def __init__(self, latent_dim, condition_dim):
        super().__init__()
        self.scale_net = nn.Linear(condition_dim, latent_dim)
        self.shift_net = nn.Linear(condition_dim, latent_dim)
        
    def forward(self, x, condition):
        scale = self.scale_net(condition).unsqueeze(2).unsqueeze(3)
        shift = self.shift_net(condition).unsqueeze(2).unsqueeze(3)
        return x * (1 + scale) + shift
```

**3. ç©ºé—´æ¡ä»¶æ§åˆ¶**ï¼š
```python
def add_spatial_conditioning(z_t, spatial_cond, method='concat'):
    if method == 'concat':
        # ç›´æ¥æ‹¼æ¥
        return torch.cat([z_t, spatial_cond], dim=1)
    elif method == 'add':
        # åŠ æ€§èåˆï¼ˆéœ€è¦ç»´åº¦åŒ¹é…ï¼‰
        return z_t + spatial_cond
    elif method == 'gated':
        # é—¨æ§èåˆ
        gate = torch.sigmoid(spatial_cond)
        return z_t * gate + spatial_cond * (1 - gate)
```

ğŸ”¬ **ç ”ç©¶æ–¹å‘ï¼šæ¡ä»¶æ³¨å…¥çš„æœ€ä¼˜ä½ç½®**  
åº”è¯¥åœ¨U-Netçš„å“ªäº›å±‚æ³¨å…¥æ¡ä»¶ä¿¡æ¯ï¼Ÿæ—©æœŸå±‚å½±å“å…¨å±€ç»“æ„ï¼ŒåæœŸå±‚æ§åˆ¶ç»†èŠ‚ã€‚ç³»ç»Ÿç ”ç©¶è¿™ç§æƒè¡¡å¯ä»¥æŒ‡å¯¼æ¶æ„è®¾è®¡ã€‚

### 10.3.4 è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

**1. æ¸è¿›å¼è®­ç»ƒ**ï¼š
```python
class ProgressiveLatentDiffusion:
    def __init__(self, autoencoder, diffusion_model):
        self.autoencoder = autoencoder
        self.diffusion = diffusion_model
        self.current_resolution = 32
        
    def train_step(self, x, epoch):
        # æ¸è¿›æé«˜åˆ†è¾¨ç‡
        if epoch > 100 and self.current_resolution < 64:
            self.current_resolution = 64
            self.update_model_resolution()
        
        # åŠ¨æ€è°ƒæ•´æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            z = self.autoencoder.encode(x)
            if self.current_resolution < z.shape[-1]:
                z = F.interpolate(z, size=self.current_resolution)
        
        # æ ‡å‡†æ‰©æ•£è®­ç»ƒ
        return self.diffusion.training_step(z)
```

**2. æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
```python
# ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒ
scaler = torch.cuda.amp.GradScaler()

def train_with_amp(model, data, optimizer):
    with torch.cuda.amp.autocast():
        # å‰å‘ä¼ æ’­åœ¨åŠç²¾åº¦
        loss = model.compute_loss(data)
    
    # åå‘ä¼ æ’­å’Œä¼˜åŒ–åœ¨å…¨ç²¾åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**3. æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 10.3.5 è´¨é‡ä¸æ•ˆç‡çš„æƒè¡¡

**å‹ç¼©ç‡ vs é‡å»ºè´¨é‡**ï¼š

| ä¸‹é‡‡æ ·å› å­ | å‹ç¼©ç‡ | é€Ÿåº¦æå‡ | FID | é€‚ç”¨åœºæ™¯ |
|-----------|--------|----------|-----|---------|
| 4x | 16x | 10-15x | ~5 | é«˜è´¨é‡ç”Ÿæˆ |
| 8x | 64x | 40-60x | ~10 | å¹³è¡¡é€‰æ‹© |
| 16x | 256x | 150-200x | ~25 | å¿«é€Ÿé¢„è§ˆ |

**åŠ¨æ€è´¨é‡è°ƒæ•´**ï¼š
```python
class AdaptiveQualityLDM:
    def __init__(self, models_dict):
        # models_dict: {4: model_4x, 8: model_8x, 16: model_16x}
        self.models = models_dict
        
    def generate(self, prompt, quality='balanced'):
        if quality == 'draft':
            model = self.models[16]
            steps = 10
        elif quality == 'balanced':
            model = self.models[8]
            steps = 25
        else:  # quality == 'high'
            model = self.models[4]
            steps = 50
            
        return model.sample(prompt, num_steps=steps)
```

<details>
<summary>**ç»ƒä¹  10.3ï¼šæ½œåœ¨ç©ºé—´æ‰©æ•£å®éªŒ**</summary>

æ¢ç´¢æ½œåœ¨ç©ºé—´æ‰©æ•£çš„å„ä¸ªæ–¹é¢ã€‚

1. **å‹ç¼©ç‡å½±å“åˆ†æ**ï¼š
   - è®­ç»ƒä¸åŒå‹ç¼©ç‡çš„LDMï¼ˆ4x, 8x, 16xï¼‰
   - æ¯”è¾ƒç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œé€Ÿåº¦
   - ç»˜åˆ¶å‹ç¼©ç‡-è´¨é‡æ›²çº¿

2. **å™ªå£°è°ƒåº¦ä¼˜åŒ–**ï¼š
   - å®ç°åŸºäºSNRçš„è‡ªé€‚åº”è°ƒåº¦
   - æ¯”è¾ƒçº¿æ€§ã€ä½™å¼¦å’Œå­¦ä¹ çš„è°ƒåº¦
   - åˆ†æå¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“

3. **æ¡ä»¶æ³¨å…¥ç ”ç©¶**ï¼š
   - å®ç°ä¸åŒçš„æ¡ä»¶æ³¨å…¥æ–¹æ³•
   - æµ‹è¯•åœ¨ä¸åŒå±‚æ³¨å…¥çš„æ•ˆæœ
   - è¯„ä¼°å¯¹å¯æ§æ€§çš„å½±å“

4. **åˆ›æ–°æ¢ç´¢**ï¼š
   - è®¾è®¡å¤šå°ºåº¦æ½œåœ¨ç©ºé—´ï¼ˆå±‚æ¬¡åŒ–LDMï¼‰
   - ç ”ç©¶å‘é‡é‡åŒ–çš„æ½œåœ¨æ‰©æ•£
   - æ¢ç´¢è‡ªé€‚åº”å‹ç¼©ç‡é€‰æ‹©

</details>

### 10.3.6 è°ƒè¯•ä¸å¯è§†åŒ–

**ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼š
```python
class LDMMonitor:
    def __init__(self, autoencoder):
        self.autoencoder = autoencoder
        
    def visualize_diffusion_process(self, model, x0, steps=[0, 250, 500, 750, 999]):
        """å¯è§†åŒ–æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹"""
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        z0 = self.autoencoder.encode(x0)
        
        # å‰å‘æ‰©æ•£
        zs_forward = []
        for t in steps:
            zt = add_noise(z0, t)
            zs_forward.append(zt)
        
        # åå‘å»å™ª
        zs_reverse = []
        zt = torch.randn_like(z0)
        for t in reversed(range(1000)):
            zt = denoise_step(model, zt, t)
            if t in steps:
                zs_reverse.append(zt)
        
        # è§£ç å¹¶å¯è§†åŒ–
        imgs_forward = [self.autoencoder.decode(z) for z in zs_forward]
        imgs_reverse = [self.autoencoder.decode(z) for z in zs_reverse]
        
        return imgs_forward, imgs_reverse
```

**è¯Šæ–­å·¥å…·**ï¼š
```python
def diagnose_latent_diffusion(model, autoencoder, test_batch):
    """è¯Šæ–­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜"""
    
    # 1. æ£€æŸ¥æ½œåœ¨ç©ºé—´åˆ†å¸ƒ
    z = autoencoder.encode(test_batch)
    print(f"Latent stats - Mean: {z.mean():.3f}, Std: {z.std():.3f}")
    
    # 2. æ£€æŸ¥é‡å»ºè´¨é‡
    x_recon = autoencoder.decode(z)
    recon_error = F.mse_loss(test_batch, x_recon)
    print(f"Reconstruction error: {recon_error:.4f}")
    
    # 3. æ£€æŸ¥å™ªå£°é¢„æµ‹
    t = torch.randint(0, 1000, (z.shape[0],))
    noise = torch.randn_like(z)
    z_noisy = add_noise(z, noise, t)
    pred_noise = model(z_noisy, t)
    noise_error = F.mse_loss(pred_noise, noise)
    print(f"Noise prediction error: {noise_error:.4f}")
    
    # 4. æ£€æŸ¥ç”Ÿæˆæ ·æœ¬
    z_sample = torch.randn_like(z)
    for t in reversed(range(0, 1000, 100)):
        z_sample = denoise_step(model, z_sample, t)
    x_sample = autoencoder.decode(z_sample)
    
    return {
        'latent_stats': (z.mean().item(), z.std().item()),
        'recon_error': recon_error.item(),
        'noise_error': noise_error.item(),
        'sample': x_sample
    }
```

ğŸŒŸ **æœ€ä½³å®è·µï¼šå¤šé˜¶æ®µè°ƒè¯•**  
å…ˆç¡®ä¿è‡ªç¼–ç å™¨å·¥ä½œæ­£å¸¸ï¼Œå†è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿè¿­ä»£ï¼ŒéªŒè¯æµç¨‹æ­£ç¡®åå†æ‰©å±•åˆ°å¤§è§„æ¨¡è®­ç»ƒã€‚

## 10.2 è‡ªç¼–ç å™¨è®¾è®¡

### 10.2.1 VQ-VAE vs KL-VAE

LDMä¸­å¸¸ç”¨ä¸¤ç§è‡ªç¼–ç å™¨æ¶æ„ï¼Œå„æœ‰ä¼˜åŠ£ï¼š

**VQ-VAEï¼ˆVector Quantized VAEï¼‰**ï¼š
```python
class VQVAE(nn.Module):
    def __init__(self, num_embeddings=8192, embedding_dim=256):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.quantize = VectorQuantizer(num_embeddings, embedding_dim)
    
    def forward(self, x):
        # ç¼–ç 
        z_e = self.encoder(x)
        
        # å‘é‡é‡åŒ–
        z_q, indices, commitment_loss = self.quantize(z_e)
        
        # è§£ç 
        x_recon = self.decoder(z_q)
        
        return x_recon, commitment_loss
```

**KL-VAEï¼ˆKLæ­£åˆ™åŒ–çš„VAEï¼‰**ï¼š
```python
class KLVAE(nn.Module):
    def __init__(self, latent_dim=256, kl_weight=1e-6):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.kl_weight = kl_weight
        
        # ç¼–ç å™¨è¾“å‡ºå‡å€¼å’Œå¯¹æ•°æ–¹å·®
        self.mean_layer = nn.Conv2d(512, latent_dim, 1)
        self.logvar_layer = nn.Conv2d(512, latent_dim, 1)
    
    def encode(self, x):
        h = self.encoder(x)
        mean = self.mean_layer(h)
        logvar = self.logvar_layer(h)
        
        # é‡å‚æ•°åŒ–
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mean + eps * std
        
        return z, mean, logvar
    
    def kl_loss(self, mean, logvar):
        # KL(q(z|x) || p(z))ï¼Œå…¶ä¸­p(z) = N(0, I)
        return -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
```

**æ¯”è¾ƒ**ï¼š
| ç‰¹æ€§ | VQ-VAE | KL-VAE |
|------|--------|---------|
| æ½œåœ¨ç©ºé—´ | ç¦»æ•£ | è¿ç»­ |
| è®­ç»ƒç¨³å®šæ€§ | è¾ƒéš¾ï¼ˆéœ€è¦æŠ€å·§ï¼‰ | è¾ƒå¥½ |
| å‹ç¼©ç‡ | å›ºå®š | çµæ´» |
| åç»­æ‰©æ•£ | éœ€è¦é€‚é… | ç›´æ¥åº”ç”¨ |

ğŸ’¡ **å®è·µé€‰æ‹©ï¼šä¸ºä»€ä¹ˆLDMåå¥½KL-VAE**  
è¿ç»­æ½œåœ¨ç©ºé—´æ›´é€‚åˆæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯å™ªå£°å‡è®¾ã€‚æå°çš„KLæƒé‡ï¼ˆ1e-6ï¼‰ä½¿å…¶æ¥è¿‘ç¡®å®šæ€§ç¼–ç å™¨ã€‚

### 10.2.2 æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ

å•çº¯çš„åƒç´ é‡å»ºæŸå¤±ä¼šå¯¼è‡´æ¨¡ç³Šç»“æœã€‚LDMä½¿ç”¨ç»„åˆæŸå¤±ï¼š

```python
class AutoencoderLoss(nn.Module):
    def __init__(self, disc_start=50000, perceptual_weight=1.0, 
                 disc_weight=0.5, kl_weight=1e-6):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg').eval()
        self.disc_start = disc_start
        self.perceptual_weight = perceptual_weight
        self.disc_weight = disc_weight
        self.kl_weight = kl_weight
    
    def forward(self, x, x_recon, mean, logvar, disc_fake, disc_real, step):
        # 1. é‡å»ºæŸå¤±
        rec_loss = F.l1_loss(x, x_recon)
        
        # 2. æ„ŸçŸ¥æŸå¤±
        if self.perceptual_weight > 0:
            p_loss = self.perceptual_loss(x, x_recon).mean()
        else:
            p_loss = torch.tensor(0.0)
        
        # 3. KLæŸå¤±
        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
        kl_loss = kl_loss / x.shape[0]  # æ‰¹æ¬¡å¹³å‡
        
        # 4. å¯¹æŠ—æŸå¤±ï¼ˆå»¶è¿Ÿå¯åŠ¨ï¼‰
        if step > self.disc_start:
            # ç”Ÿæˆå™¨æŸå¤±ï¼šæ¬ºéª—åˆ¤åˆ«å™¨
            g_loss = -torch.mean(disc_fake)
        else:
            g_loss = torch.tensor(0.0)
        
        # ç»„åˆ
        loss = rec_loss + self.perceptual_weight * p_loss + \
               self.kl_weight * kl_loss + self.disc_weight * g_loss
        
        return loss, {
            'rec': rec_loss.item(),
            'perceptual': p_loss.item(),
            'kl': kl_loss.item(),
            'gen': g_loss.item()
        }
```

**åˆ¤åˆ«å™¨è®¾è®¡**ï¼š
```python
class PatchDiscriminator(nn.Module):
    def __init__(self, in_channels=3, ndf=64, n_layers=3):
        super().__init__()
        layers = [nn.Conv2d(in_channels, ndf, 4, 2, 1), 
                  nn.LeakyReLU(0.2, True)]
        
        for i in range(1, n_layers):
            in_ch = ndf * min(2**(i-1), 8)
            out_ch = ndf * min(2**i, 8)
            layers += [
                nn.Conv2d(in_ch, out_ch, 4, 2, 1),
                nn.BatchNorm2d(out_ch),
                nn.LeakyReLU(0.2, True)
            ]
        
        # æœ€åä¸€å±‚è¾“å‡ºå•é€šé“ç‰¹å¾å›¾
        layers.append(nn.Conv2d(out_ch, 1, 4, 1, 1))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
```

### 10.2.3 æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–

ä¸ºäº†ç¡®ä¿æ½œåœ¨ç©ºé—´é€‚åˆæ‰©æ•£å»ºæ¨¡ï¼Œéœ€è¦é€‚å½“çš„æ­£åˆ™åŒ–ï¼š

**1. KLæ­£åˆ™åŒ–çš„ä½œç”¨**ï¼š
- é˜²æ­¢æ½œåœ¨ç©ºé—´åç¼©
- é¼“åŠ±æ¥è¿‘æ ‡å‡†é«˜æ–¯åˆ†å¸ƒ
- ä½†æƒé‡éœ€è¦å¾ˆå°é¿å…ä¿¡æ¯æŸå¤±

**2. è°±å½’ä¸€åŒ–**ï¼š
```python
def add_spectral_norm(module):
    """é€’å½’åœ°ä¸ºæ‰€æœ‰å·ç§¯å±‚æ·»åŠ è°±å½’ä¸€åŒ–"""
    for name, child in module.named_children():
        if isinstance(child, nn.Conv2d):
            setattr(module, name, nn.utils.spectral_norm(child))
        else:
            add_spectral_norm(child)
```

**3. æ¢¯åº¦æƒ©ç½š**ï¼š
```python
def gradient_penalty(discriminator, real, fake):
    batch_size = real.size(0)
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real.device)
    interpolated = epsilon * real + (1 - epsilon) * fake
    interpolated.requires_grad_(True)
    
    d_interpolated = discriminator(interpolated)
    gradients = torch.autograd.grad(
        outputs=d_interpolated, inputs=interpolated,
        grad_outputs=torch.ones_like(d_interpolated),
        create_graph=True, retain_graph=True
    )[0]
    
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    penalty = ((gradient_norm - 1) ** 2).mean()
    
    return penalty
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜æ­£åˆ™åŒ–ç­–ç•¥**  
å¦‚ä½•å¹³è¡¡é‡å»ºè´¨é‡å’Œæ½œåœ¨ç©ºé—´çš„è§„æ•´æ€§ï¼Ÿæ˜¯å¦å¯ä»¥è®¾è®¡è‡ªé€‚åº”çš„æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Ÿ

### 10.2.4 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

**é«˜æ•ˆçš„ç¼–ç å™¨è®¾è®¡**ï¼š
```python
class Encoder(nn.Module):
    def __init__(self, in_channels=3, ch=128, ch_mult=(1,2,4,8), 
                 num_res_blocks=2, z_channels=4):
        super().__init__()
        self.num_resolutions = len(ch_mult)
        
        # åˆå§‹å·ç§¯
        self.conv_in = nn.Conv2d(in_channels, ch, 3, 1, 1)
        
        # ä¸‹é‡‡æ ·å—
        self.down = nn.ModuleList()
        in_ch = ch
        for i, mult in enumerate(ch_mult):
            out_ch = ch * mult
            for j in range(num_res_blocks):
                self.down.append(ResnetBlock(in_ch, out_ch))
                in_ch = out_ch
            
            if i != self.num_resolutions - 1:
                self.down.append(Downsample(in_ch))
        
        # ä¸­é—´å—
        self.mid = nn.Module()
        self.mid.block_1 = ResnetBlock(in_ch, in_ch)
        self.mid.attn_1 = AttnBlock(in_ch)
        self.mid.block_2 = ResnetBlock(in_ch, in_ch)
        
        # è¾“å‡ºå±‚
        self.norm_out = nn.GroupNorm(32, in_ch)
        self.conv_out = nn.Conv2d(in_ch, 2*z_channels, 3, 1, 1)  # å‡å€¼å’Œæ–¹å·®
    
    def forward(self, x):
        # ç¼–ç 
        h = self.conv_in(x)
        
        for module in self.down:
            h = module(h)
        
        # ä¸­é—´å¤„ç†
        h = self.mid.block_1(h)
        h = self.mid.attn_1(h)
        h = self.mid.block_2(h)
        
        # è¾“å‡º
        h = self.norm_out(h)
        h = F.silu(h)
        h = self.conv_out(h)
        
        return h
```

**æ®‹å·®å—å®ç°**ï¼š
```python
class ResnetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.0):
        super().__init__()
        self.norm1 = nn.GroupNorm(32, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)
        self.norm2 = nn.GroupNorm(32, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        
        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.shortcut = nn.Identity()
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = F.silu(h)
        h = self.conv1(h)
        
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        h = self.conv2(h)
        
        return h + self.shortcut(x)
```

<details>
<summary>**ç»ƒä¹  10.2ï¼šè‡ªç¼–ç å™¨æ¶æ„å®éªŒ**</summary>

æ¢ç´¢ä¸åŒçš„è‡ªç¼–ç å™¨è®¾è®¡é€‰æ‹©ã€‚

1. **æ¶æ„æ¯”è¾ƒ**ï¼š
   - å®ç°VQ-VAEå’ŒKL-VAE
   - æ¯”è¾ƒé‡å»ºè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§
   - åˆ†ææ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§

2. **æŸå¤±å‡½æ•°ç ”ç©¶**ï¼š
   - è°ƒæ•´å„æŸå¤±é¡¹çš„æƒé‡
   - å°è¯•ä¸åŒçš„æ„ŸçŸ¥ç½‘ç»œï¼ˆVGG, ResNetï¼‰
   - ç ”ç©¶å¯¹æŠ—è®­ç»ƒçš„å¯åŠ¨æ—¶æœº

3. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - æµ‹è¯•ä¸åŒçš„æ½œåœ¨ç»´åº¦
   - åˆ†æç‡å¤±çœŸæƒè¡¡
   - æ‰¾å‡ºç‰¹å®šæ•°æ®é›†çš„æœ€ä¼˜è®¾ç½®

4. **åˆ›æ–°è®¾è®¡**ï¼š
   - å°è¯•æ¸è¿›å¼è®­ç»ƒï¼ˆé€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼‰
   - å®ç°æ¡ä»¶è‡ªç¼–ç å™¨
   - æ¢ç´¢å±‚æ¬¡åŒ–æ½œåœ¨è¡¨ç¤º

</details>

### 10.2.5 è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§

**1. å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
def get_lr_scheduler(optimizer, warmup_steps=5000):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 1.0
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**2. EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰**ï¼š
```python
class EMA:
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + \
                                   (1 - self.decay) * param.data
```

**3. æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

ğŸ’¡ **è°ƒè¯•æŠ€å·§ï¼šç›‘æ§æ½œåœ¨ç©ºé—´**  
å®šæœŸå¯è§†åŒ–æ½œåœ¨ç¼–ç çš„åˆ†å¸ƒï¼Œç¡®ä¿æ²¡æœ‰æ¨¡å¼å´©æºƒæˆ–å¼‚å¸¸å€¼ã€‚

### 10.2.6 é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨

ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨å¯ä»¥å¤§å¤§åŠ é€Ÿå¼€å‘ï¼š

```python
def load_pretrained_autoencoder(model_id="stabilityai/sd-vae-ft-mse"):
    from diffusers import AutoencoderKL
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    vae = AutoencoderKL.from_pretrained(model_id)
    
    # é€‚é…æ¥å£
    class PretrainedAutoencoder(nn.Module):
        def __init__(self, vae):
            super().__init__()
            self.vae = vae
            self.scale_factor = 0.18215  # SDçš„æ ‡å‡†ç¼©æ”¾å› å­
        
        def encode(self, x):
            # x: [B, 3, H, W] in [-1, 1]
            latent = self.vae.encode(x).latent_dist.sample()
            return latent * self.scale_factor
        
        def decode(self, z):
            # z: [B, 4, H/8, W/8]
            z = z / self.scale_factor
            return self.vae.decode(z).sample
    
    return PretrainedAutoencoder(vae)
```

ğŸŒŸ **æœ€ä½³å®è·µï¼šè¿ç§»å­¦ä¹ **  
å³ä½¿ç›®æ ‡é¢†åŸŸä¸åŒï¼Œä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é€šå¸¸æ¯”ä»å¤´è®­ç»ƒæ›´å¥½ã€‚è‡ªç„¶å›¾åƒçš„ç¼–ç å™¨å¯ä»¥å¾ˆå¥½åœ°è¿ç§»åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ã€‚