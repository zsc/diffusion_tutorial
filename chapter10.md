[â† è¿”å›ç›®å½•](index.md) | ç¬¬10ç«  / å…±14ç«  | [ä¸‹ä¸€ç«  â†’](chapter11.md)

# ç¬¬10ç« ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM)

æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Models, LDMï¼‰æ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªé©å‘½æ€§è¿›å±•ï¼Œå®ƒé€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´è€ŒéåŸå§‹åƒç´ ç©ºé—´è¿›è¡Œæ‰©æ•£ï¼Œæå¤§åœ°æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨LDMçš„æ ¸å¿ƒæ€æƒ³ï¼ŒåŒ…æ‹¬è‡ªç¼–ç å™¨çš„è®¾è®¡ã€æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§ã€ä»¥åŠå¦‚ä½•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°æ•°é‡çº§çš„åŠ é€Ÿã€‚æ‚¨å°†ç†è§£Stable DiffusionèƒŒåçš„æŠ€æœ¯åŸç†ï¼ŒæŒæ¡è®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€å·§ï¼Œå¹¶å­¦ä¹ å¦‚ä½•æƒè¡¡å‹ç¼©ç‡ä¸é‡å»ºè´¨é‡ã€‚

## ç« èŠ‚å¤§çº²

### 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´
- é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜
- æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿
- æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©
- LDMçš„æ•´ä½“æ¶æ„

### 10.2 è‡ªç¼–ç å™¨è®¾è®¡
- VQ-VAE vs KL-VAE
- æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ
- æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

### 10.3 æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£
- æ½œåœ¨æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æè¿°
- å™ªå£°è°ƒåº¦çš„é€‚é…
- æ¡ä»¶æœºåˆ¶åœ¨æ½œåœ¨ç©ºé—´çš„å®ç°
- è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

### 10.4 Stable Diffusionæ¶æ„è¯¦è§£
- æ¨¡å‹ç»„ä»¶åˆ†æ
- CLIPæ–‡æœ¬ç¼–ç å™¨é›†æˆ
- äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
- æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### 10.5 å®è·µè€ƒè™‘ä¸æ‰©å±•
- ä¸åŒåˆ†è¾¨ç‡çš„å¤„ç†
- å¾®è°ƒä¸é€‚é…
- æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²
- æœªæ¥å‘å±•æ–¹å‘

## 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´

### 10.1.1 é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜

åœ¨åƒç´ ç©ºé—´ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹é¢ä¸´ä¸¥é‡çš„è®¡ç®—ç“¶é¢ˆï¼š

**è®¡ç®—å¤æ‚åº¦åˆ†æ**ï¼š
- 512Ã—512 RGBå›¾åƒï¼š786,432ç»´
- 1024Ã—1024 RGBå›¾åƒï¼š3,145,728ç»´
- U-Netçš„è®¡ç®—é‡ï¼š$O(n^2)$ å¯¹äºè‡ªæ³¨æ„åŠ›å±‚

å…·ä½“æ•°å­—ï¼š
```python
# åƒç´ ç©ºé—´æ‰©æ•£çš„å†…å­˜éœ€æ±‚
def compute_memory_requirements(h, w, c=3, batch_size=1):
    # è¾“å…¥å¼ é‡
    input_size = batch_size * c * h * w * 4  # float32
    
    # U-Netä¸­é—´ç‰¹å¾ï¼ˆå‡è®¾æœ€å¤§é€šé“æ•°2048ï¼‰
    feature_size = batch_size * 2048 * (h//8) * (w//8) * 4
    
    # è‡ªæ³¨æ„åŠ›çŸ©é˜µ
    seq_len = (h//8) * (w//8)
    attention_size = batch_size * seq_len * seq_len * 4
    
    total_gb = (input_size + feature_size + attention_size) / (1024**3)
    return total_gb

# 1024x1024å›¾åƒéœ€è¦çº¦48GBå†…å­˜ï¼
```

### 10.1.2 æ½œåœ¨ç©ºé—´çš„æ ¸å¿ƒä¼˜åŠ¿

LDMé€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œè·å¾—å¤šä¸ªä¼˜åŠ¿ï¼š

1. **è®¡ç®—æ•ˆç‡**ï¼š8å€ä¸‹é‡‡æ ·å‡å°‘64å€è®¡ç®—é‡
2. **è¯­ä¹‰å‹ç¼©**ï¼šæ½œåœ¨è¡¨ç¤ºæ›´æ¥è¿‘è¯­ä¹‰ä¿¡æ¯
3. **æ›´å¥½çš„å½’çº³åç½®**ï¼šè‡ªç„¶å›¾åƒçš„ä½ç»´æµå½¢å‡è®¾
4. **æ¨¡å—åŒ–è®¾è®¡**ï¼šåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆä»»åŠ¡

**å‹ç¼©ç‡vsè´¨é‡çš„æƒè¡¡**ï¼š
```
ä¸‹é‡‡æ ·å› å­ | æ½œåœ¨ç»´åº¦ | åŠ é€Ÿæ¯” | é‡å»ºPSNR
    4       |  64Ã—64   |  16x   |  >30dB
    8       |  32Ã—32   |  64x   |  ~27dB
   16       |  16Ã—16   | 256x   |  ~23dB
```

### 10.1.3 æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©

LDMçš„å…³é”®æ´å¯Ÿæ˜¯åŒºåˆ†ä¸¤ç§å‹ç¼©ï¼š

**ä¿¡æ¯å‹ç¼©**ï¼ˆä¼ ç»Ÿå‹ç¼©ï¼‰ï¼š
- ç›®æ ‡ï¼šå®Œç¾é‡å»ºæ¯ä¸ªåƒç´ 
- æ–¹æ³•ï¼šç†µç¼–ç ã€é¢„æµ‹ç¼–ç 
- é—®é¢˜ï¼šä¿ç•™äº†æ„ŸçŸ¥ä¸é‡è¦çš„ç»†èŠ‚

**æ„ŸçŸ¥å‹ç¼©**ï¼ˆLDMä½¿ç”¨ï¼‰ï¼š
- ç›®æ ‡ï¼šä¿ç•™æ„ŸçŸ¥é‡è¦çš„ç‰¹å¾
- æ–¹æ³•ï¼šå­¦ä¹ çš„ç¼–ç å™¨ + æ„ŸçŸ¥æŸå¤±
- ä¼˜åŠ¿ï¼šæ›´é«˜å‹ç¼©ç‡ï¼Œæ›´è¯­ä¹‰åŒ–çš„è¡¨ç¤º

```python
class PerceptualCompression(nn.Module):
    def __init__(self, perceptual_weight=1.0):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg')
        self.perceptual_weight = perceptual_weight
    
    def forward(self, x, x_recon):
        # åƒç´ çº§æŸå¤±
        pixel_loss = F.l1_loss(x, x_recon)
        
        # æ„ŸçŸ¥æŸå¤±
        perceptual_loss = self.perceptual_loss(x, x_recon)
        
        # ç»„åˆ
        return pixel_loss + self.perceptual_weight * perceptual_loss
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜å‹ç¼©ç‡**  
ä»€ä¹ˆå†³å®šäº†æœ€ä¼˜çš„å‹ç¼©ç‡ï¼Ÿæ˜¯å¦å¯ä»¥æ ¹æ®æ•°æ®é›†ç‰¹æ€§è‡ªé€‚åº”é€‰æ‹©ï¼Ÿè¿™æ¶‰åŠåˆ°ç‡å¤±çœŸç†è®ºå’Œæµå½¢å‡è®¾ã€‚

### 10.1.4 LDMçš„æ•´ä½“æ¶æ„

LDMç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ„æˆï¼š

```python
class LatentDiffusionModel(nn.Module):
    def __init__(self, autoencoder, diffusion_model, conditioning_model):
        super().__init__()
        self.autoencoder = autoencoder  # ç¼–ç /è§£ç å›¾åƒ
        self.diffusion = diffusion_model  # æ½œåœ¨ç©ºé—´æ‰©æ•£
        self.cond_model = conditioning_model  # å¤„ç†æ¡ä»¶ä¿¡æ¯
        
        # å†»ç»“è‡ªç¼–ç å™¨ï¼ˆé€šå¸¸é¢„è®­ç»ƒï¼‰
        self.autoencoder.eval()
        for param in self.autoencoder.parameters():
            param.requires_grad = False
    
    def encode(self, x):
        # å›¾åƒ -> æ½œåœ¨è¡¨ç¤º
        with torch.no_grad():
            z = self.autoencoder.encode(x)
            # å¯é€‰ï¼šæ ‡å‡†åŒ–
            z = z * self.scale_factor
        return z
    
    def decode(self, z):
        # æ½œåœ¨è¡¨ç¤º -> å›¾åƒ
        z = z / self.scale_factor
        with torch.no_grad():
            x = self.autoencoder.decode(z)
        return x
```

<details>
<summary>**ç»ƒä¹  10.1ï¼šåˆ†æå‹ç¼©æ•ˆç‡**</summary>

ç ”ç©¶ä¸åŒå‹ç¼©ç­–ç•¥çš„æ•ˆæœã€‚

1. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - å®ç°ä¸åŒä¸‹é‡‡æ ·ç‡çš„è‡ªç¼–ç å™¨
   - æµ‹é‡é‡å»ºè´¨é‡ï¼ˆPSNR, SSIM, LPIPSï¼‰
   - ç»˜åˆ¶ç‡å¤±çœŸæ›²çº¿

2. **è¯­ä¹‰ä¿ç•™åˆ†æ**ï¼š
   - ä½¿ç”¨é¢„è®­ç»ƒåˆ†ç±»å™¨è¯„ä¼°è¯­ä¹‰ä¿ç•™
   - æ¯”è¾ƒåƒç´ MSE vs æ„ŸçŸ¥æŸå¤±
   - åˆ†æå“ªäº›ç‰¹å¾è¢«ä¿ç•™/ä¸¢å¤±

3. **è®¡ç®—æ•ˆç›Šè¯„ä¼°**ï¼š
   - æµ‹é‡ä¸åŒåˆ†è¾¨ç‡çš„æ¨ç†æ—¶é—´
   - è®¡ç®—å†…å­˜ä½¿ç”¨
   - æ‰¾å‡ºæ•ˆç‡ç“¶é¢ˆ

4. **ç†è®ºæ‹“å±•**ï¼š
   - ä»æµå½¢å‡è®¾è§’åº¦åˆ†æå‹ç¼©
   - ç ”ç©¶æœ€ä¼˜ä¼ è¾“ç†è®ºçš„åº”ç”¨
   - æ¢ç´¢è‡ªé€‚åº”å‹ç¼©ç‡

</details>

### 10.1.5 ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

LDMé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆï¼š

**ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒè‡ªç¼–ç å™¨**
```python
def train_autoencoder(model, dataloader, epochs):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        for x in dataloader:
            # ç¼–ç -è§£ç 
            z = model.encode(x)
            x_recon = model.decode(z)
            
            # é‡å»ºæŸå¤±
            recon_loss = F.l1_loss(x, x_recon)
            
            # æ„ŸçŸ¥æŸå¤±
            p_loss = perceptual_loss(x, x_recon)
            
            # KLæ­£åˆ™åŒ–ï¼ˆå¦‚æœä½¿ç”¨VAEï¼‰
            kl_loss = model.kl_loss(z)
            
            loss = recon_loss + 0.1 * p_loss + 0.001 * kl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

**ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹**
```python
def train_diffusion(diffusion_model, autoencoder, dataloader):
    # å†»ç»“è‡ªç¼–ç å™¨
    autoencoder.eval()
    
    for x, c in dataloader:
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            z = autoencoder.encode(x)
        
        # æ ‡å‡†æ‰©æ•£è®­ç»ƒ
        t = torch.randint(0, num_steps, (z.shape[0],))
        noise = torch.randn_like(z)
        z_t = add_noise(z, noise, t)
        
        # é¢„æµ‹å™ªå£°
        pred_noise = diffusion_model(z_t, t, c)
        loss = F.mse_loss(pred_noise, noise)
        
        loss.backward()
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šé¢„è®­ç»ƒç­–ç•¥**  
å¯ä»¥ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒé€šç”¨è‡ªç¼–ç å™¨ï¼Œç„¶ååœ¨ç‰¹å®šé¢†åŸŸå¾®è°ƒã€‚è¿™å¤§å¤§å‡å°‘äº†è®­ç»ƒæˆæœ¬ã€‚

### 10.1.6 æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§

ç†æƒ³çš„æ½œåœ¨ç©ºé—´åº”å…·å¤‡ï¼š

1. **å¹³æ»‘æ€§**ï¼šç›¸è¿‘çš„æ½œåœ¨ç¼–ç å¯¹åº”ç›¸ä¼¼çš„å›¾åƒ
2. **è¯­ä¹‰æ€§**ï¼šæ½œåœ¨ç»´åº¦å¯¹åº”æœ‰æ„ä¹‰çš„å˜åŒ–
3. **ç´§å‡‘æ€§**ï¼šé«˜æ•ˆåˆ©ç”¨æ¯ä¸ªç»´åº¦
4. **æ­£æ€æ€§**ï¼šä¾¿äºæ‰©æ•£æ¨¡å‹å»ºæ¨¡

**åˆ†ææ½œåœ¨ç©ºé—´**ï¼š
```python
def analyze_latent_space(autoencoder, dataloader):
    latents = []
    labels = []
    
    with torch.no_grad():
        for x, y in dataloader:
            z = autoencoder.encode(x)
            latents.append(z.cpu())
            labels.append(y.cpu())
    
    latents = torch.cat(latents)
    labels = torch.cat(labels)
    
    # ç»Ÿè®¡ç‰¹æ€§
    print(f"Mean: {latents.mean():.3f}")
    print(f"Std: {latents.std():.3f}")
    print(f"Kurtosis: {stats.kurtosis(latents.numpy().flatten()):.3f}")
    
    # å¯è§†åŒ–ï¼ˆä½¿ç”¨t-SNEæˆ–UMAPï¼‰
    embedded = TSNE(n_components=2).fit_transform(latents.numpy())
    plt.scatter(embedded[:, 0], embedded[:, 1], c=labels)
```

ğŸŒŸ **å¼€æ”¾é—®é¢˜ï¼šæœ€ä¼˜æ½œåœ¨ç©ºé—´è®¾è®¡**  
å¦‚ä½•è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„æ½œåœ¨ç©ºé—´ï¼Ÿèƒ½å¦å­¦ä¹ è§£è€¦çš„è¡¨ç¤ºï¼Ÿè¿™æ¶‰åŠåˆ°è¡¨ç¤ºå­¦ä¹ å’Œå› æœæ¨æ–­çš„å‰æ²¿ç ”ç©¶ã€‚

## 10.2 è‡ªç¼–ç å™¨è®¾è®¡

### 10.2.1 VQ-VAE vs KL-VAE

LDMä¸­å¸¸ç”¨ä¸¤ç§è‡ªç¼–ç å™¨æ¶æ„ï¼Œå„æœ‰ä¼˜åŠ£ï¼š

**VQ-VAEï¼ˆVector Quantized VAEï¼‰**ï¼š
```python
class VQVAE(nn.Module):
    def __init__(self, num_embeddings=8192, embedding_dim=256):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.quantize = VectorQuantizer(num_embeddings, embedding_dim)
    
    def forward(self, x):
        # ç¼–ç 
        z_e = self.encoder(x)
        
        # å‘é‡é‡åŒ–
        z_q, indices, commitment_loss = self.quantize(z_e)
        
        # è§£ç 
        x_recon = self.decoder(z_q)
        
        return x_recon, commitment_loss
```

**KL-VAEï¼ˆKLæ­£åˆ™åŒ–çš„VAEï¼‰**ï¼š
```python
class KLVAE(nn.Module):
    def __init__(self, latent_dim=256, kl_weight=1e-6):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.kl_weight = kl_weight
        
        # ç¼–ç å™¨è¾“å‡ºå‡å€¼å’Œå¯¹æ•°æ–¹å·®
        self.mean_layer = nn.Conv2d(512, latent_dim, 1)
        self.logvar_layer = nn.Conv2d(512, latent_dim, 1)
    
    def encode(self, x):
        h = self.encoder(x)
        mean = self.mean_layer(h)
        logvar = self.logvar_layer(h)
        
        # é‡å‚æ•°åŒ–
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mean + eps * std
        
        return z, mean, logvar
    
    def kl_loss(self, mean, logvar):
        # KL(q(z|x) || p(z))ï¼Œå…¶ä¸­p(z) = N(0, I)
        return -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
```

**æ¯”è¾ƒ**ï¼š
| ç‰¹æ€§ | VQ-VAE | KL-VAE |
|------|--------|---------|
| æ½œåœ¨ç©ºé—´ | ç¦»æ•£ | è¿ç»­ |
| è®­ç»ƒç¨³å®šæ€§ | è¾ƒéš¾ï¼ˆéœ€è¦æŠ€å·§ï¼‰ | è¾ƒå¥½ |
| å‹ç¼©ç‡ | å›ºå®š | çµæ´» |
| åç»­æ‰©æ•£ | éœ€è¦é€‚é… | ç›´æ¥åº”ç”¨ |

ğŸ’¡ **å®è·µé€‰æ‹©ï¼šä¸ºä»€ä¹ˆLDMåå¥½KL-VAE**  
è¿ç»­æ½œåœ¨ç©ºé—´æ›´é€‚åˆæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯å™ªå£°å‡è®¾ã€‚æå°çš„KLæƒé‡ï¼ˆ1e-6ï¼‰ä½¿å…¶æ¥è¿‘ç¡®å®šæ€§ç¼–ç å™¨ã€‚

### 10.2.2 æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ

å•çº¯çš„åƒç´ é‡å»ºæŸå¤±ä¼šå¯¼è‡´æ¨¡ç³Šç»“æœã€‚LDMä½¿ç”¨ç»„åˆæŸå¤±ï¼š

```python
class AutoencoderLoss(nn.Module):
    def __init__(self, disc_start=50000, perceptual_weight=1.0, 
                 disc_weight=0.5, kl_weight=1e-6):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg').eval()
        self.disc_start = disc_start
        self.perceptual_weight = perceptual_weight
        self.disc_weight = disc_weight
        self.kl_weight = kl_weight
    
    def forward(self, x, x_recon, mean, logvar, disc_fake, disc_real, step):
        # 1. é‡å»ºæŸå¤±
        rec_loss = F.l1_loss(x, x_recon)
        
        # 2. æ„ŸçŸ¥æŸå¤±
        if self.perceptual_weight > 0:
            p_loss = self.perceptual_loss(x, x_recon).mean()
        else:
            p_loss = torch.tensor(0.0)
        
        # 3. KLæŸå¤±
        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
        kl_loss = kl_loss / x.shape[0]  # æ‰¹æ¬¡å¹³å‡
        
        # 4. å¯¹æŠ—æŸå¤±ï¼ˆå»¶è¿Ÿå¯åŠ¨ï¼‰
        if step > self.disc_start:
            # ç”Ÿæˆå™¨æŸå¤±ï¼šæ¬ºéª—åˆ¤åˆ«å™¨
            g_loss = -torch.mean(disc_fake)
        else:
            g_loss = torch.tensor(0.0)
        
        # ç»„åˆ
        loss = rec_loss + self.perceptual_weight * p_loss + \
               self.kl_weight * kl_loss + self.disc_weight * g_loss
        
        return loss, {
            'rec': rec_loss.item(),
            'perceptual': p_loss.item(),
            'kl': kl_loss.item(),
            'gen': g_loss.item()
        }
```

**åˆ¤åˆ«å™¨è®¾è®¡**ï¼š
```python
class PatchDiscriminator(nn.Module):
    def __init__(self, in_channels=3, ndf=64, n_layers=3):
        super().__init__()
        layers = [nn.Conv2d(in_channels, ndf, 4, 2, 1), 
                  nn.LeakyReLU(0.2, True)]
        
        for i in range(1, n_layers):
            in_ch = ndf * min(2**(i-1), 8)
            out_ch = ndf * min(2**i, 8)
            layers += [
                nn.Conv2d(in_ch, out_ch, 4, 2, 1),
                nn.BatchNorm2d(out_ch),
                nn.LeakyReLU(0.2, True)
            ]
        
        # æœ€åä¸€å±‚è¾“å‡ºå•é€šé“ç‰¹å¾å›¾
        layers.append(nn.Conv2d(out_ch, 1, 4, 1, 1))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
```

### 10.2.3 æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–

ä¸ºäº†ç¡®ä¿æ½œåœ¨ç©ºé—´é€‚åˆæ‰©æ•£å»ºæ¨¡ï¼Œéœ€è¦é€‚å½“çš„æ­£åˆ™åŒ–ï¼š

**1. KLæ­£åˆ™åŒ–çš„ä½œç”¨**ï¼š
- é˜²æ­¢æ½œåœ¨ç©ºé—´åç¼©
- é¼“åŠ±æ¥è¿‘æ ‡å‡†é«˜æ–¯åˆ†å¸ƒ
- ä½†æƒé‡éœ€è¦å¾ˆå°é¿å…ä¿¡æ¯æŸå¤±

**2. è°±å½’ä¸€åŒ–**ï¼š
```python
def add_spectral_norm(module):
    """é€’å½’åœ°ä¸ºæ‰€æœ‰å·ç§¯å±‚æ·»åŠ è°±å½’ä¸€åŒ–"""
    for name, child in module.named_children():
        if isinstance(child, nn.Conv2d):
            setattr(module, name, nn.utils.spectral_norm(child))
        else:
            add_spectral_norm(child)
```

**3. æ¢¯åº¦æƒ©ç½š**ï¼š
```python
def gradient_penalty(discriminator, real, fake):
    batch_size = real.size(0)
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real.device)
    interpolated = epsilon * real + (1 - epsilon) * fake
    interpolated.requires_grad_(True)
    
    d_interpolated = discriminator(interpolated)
    gradients = torch.autograd.grad(
        outputs=d_interpolated, inputs=interpolated,
        grad_outputs=torch.ones_like(d_interpolated),
        create_graph=True, retain_graph=True
    )[0]
    
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    penalty = ((gradient_norm - 1) ** 2).mean()
    
    return penalty
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜æ­£åˆ™åŒ–ç­–ç•¥**  
å¦‚ä½•å¹³è¡¡é‡å»ºè´¨é‡å’Œæ½œåœ¨ç©ºé—´çš„è§„æ•´æ€§ï¼Ÿæ˜¯å¦å¯ä»¥è®¾è®¡è‡ªé€‚åº”çš„æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Ÿ

### 10.2.4 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

**é«˜æ•ˆçš„ç¼–ç å™¨è®¾è®¡**ï¼š
```python
class Encoder(nn.Module):
    def __init__(self, in_channels=3, ch=128, ch_mult=(1,2,4,8), 
                 num_res_blocks=2, z_channels=4):
        super().__init__()
        self.num_resolutions = len(ch_mult)
        
        # åˆå§‹å·ç§¯
        self.conv_in = nn.Conv2d(in_channels, ch, 3, 1, 1)
        
        # ä¸‹é‡‡æ ·å—
        self.down = nn.ModuleList()
        in_ch = ch
        for i, mult in enumerate(ch_mult):
            out_ch = ch * mult
            for j in range(num_res_blocks):
                self.down.append(ResnetBlock(in_ch, out_ch))
                in_ch = out_ch
            
            if i != self.num_resolutions - 1:
                self.down.append(Downsample(in_ch))
        
        # ä¸­é—´å—
        self.mid = nn.Module()
        self.mid.block_1 = ResnetBlock(in_ch, in_ch)
        self.mid.attn_1 = AttnBlock(in_ch)
        self.mid.block_2 = ResnetBlock(in_ch, in_ch)
        
        # è¾“å‡ºå±‚
        self.norm_out = nn.GroupNorm(32, in_ch)
        self.conv_out = nn.Conv2d(in_ch, 2*z_channels, 3, 1, 1)  # å‡å€¼å’Œæ–¹å·®
    
    def forward(self, x):
        # ç¼–ç 
        h = self.conv_in(x)
        
        for module in self.down:
            h = module(h)
        
        # ä¸­é—´å¤„ç†
        h = self.mid.block_1(h)
        h = self.mid.attn_1(h)
        h = self.mid.block_2(h)
        
        # è¾“å‡º
        h = self.norm_out(h)
        h = F.silu(h)
        h = self.conv_out(h)
        
        return h
```

**æ®‹å·®å—å®ç°**ï¼š
```python
class ResnetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.0):
        super().__init__()
        self.norm1 = nn.GroupNorm(32, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)
        self.norm2 = nn.GroupNorm(32, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        
        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.shortcut = nn.Identity()
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = F.silu(h)
        h = self.conv1(h)
        
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        h = self.conv2(h)
        
        return h + self.shortcut(x)
```

<details>
<summary>**ç»ƒä¹  10.2ï¼šè‡ªç¼–ç å™¨æ¶æ„å®éªŒ**</summary>

æ¢ç´¢ä¸åŒçš„è‡ªç¼–ç å™¨è®¾è®¡é€‰æ‹©ã€‚

1. **æ¶æ„æ¯”è¾ƒ**ï¼š
   - å®ç°VQ-VAEå’ŒKL-VAE
   - æ¯”è¾ƒé‡å»ºè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§
   - åˆ†ææ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§

2. **æŸå¤±å‡½æ•°ç ”ç©¶**ï¼š
   - è°ƒæ•´å„æŸå¤±é¡¹çš„æƒé‡
   - å°è¯•ä¸åŒçš„æ„ŸçŸ¥ç½‘ç»œï¼ˆVGG, ResNetï¼‰
   - ç ”ç©¶å¯¹æŠ—è®­ç»ƒçš„å¯åŠ¨æ—¶æœº

3. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - æµ‹è¯•ä¸åŒçš„æ½œåœ¨ç»´åº¦
   - åˆ†æç‡å¤±çœŸæƒè¡¡
   - æ‰¾å‡ºç‰¹å®šæ•°æ®é›†çš„æœ€ä¼˜è®¾ç½®

4. **åˆ›æ–°è®¾è®¡**ï¼š
   - å°è¯•æ¸è¿›å¼è®­ç»ƒï¼ˆé€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼‰
   - å®ç°æ¡ä»¶è‡ªç¼–ç å™¨
   - æ¢ç´¢å±‚æ¬¡åŒ–æ½œåœ¨è¡¨ç¤º

</details>

### 10.2.5 è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§

**1. å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
def get_lr_scheduler(optimizer, warmup_steps=5000):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 1.0
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**2. EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰**ï¼š
```python
class EMA:
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + \
                                   (1 - self.decay) * param.data
```

**3. æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

ğŸ’¡ **è°ƒè¯•æŠ€å·§ï¼šç›‘æ§æ½œåœ¨ç©ºé—´**  
å®šæœŸå¯è§†åŒ–æ½œåœ¨ç¼–ç çš„åˆ†å¸ƒï¼Œç¡®ä¿æ²¡æœ‰æ¨¡å¼å´©æºƒæˆ–å¼‚å¸¸å€¼ã€‚

### 10.2.6 é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨

ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨å¯ä»¥å¤§å¤§åŠ é€Ÿå¼€å‘ï¼š

```python
def load_pretrained_autoencoder(model_id="stabilityai/sd-vae-ft-mse"):
    from diffusers import AutoencoderKL
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    vae = AutoencoderKL.from_pretrained(model_id)
    
    # é€‚é…æ¥å£
    class PretrainedAutoencoder(nn.Module):
        def __init__(self, vae):
            super().__init__()
            self.vae = vae
            self.scale_factor = 0.18215  # SDçš„æ ‡å‡†ç¼©æ”¾å› å­
        
        def encode(self, x):
            # x: [B, 3, H, W] in [-1, 1]
            latent = self.vae.encode(x).latent_dist.sample()
            return latent * self.scale_factor
        
        def decode(self, z):
            # z: [B, 4, H/8, W/8]
            z = z / self.scale_factor
            return self.vae.decode(z).sample
    
    return PretrainedAutoencoder(vae)
```

ğŸŒŸ **æœ€ä½³å®è·µï¼šè¿ç§»å­¦ä¹ **  
å³ä½¿ç›®æ ‡é¢†åŸŸä¸åŒï¼Œä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é€šå¸¸æ¯”ä»å¤´è®­ç»ƒæ›´å¥½ã€‚è‡ªç„¶å›¾åƒçš„ç¼–ç å™¨å¯ä»¥å¾ˆå¥½åœ°è¿ç§»åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ã€‚