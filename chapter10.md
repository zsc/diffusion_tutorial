[â† è¿”å›ç›®å½•](index.md) | ç¬¬10ç«  / å…±14ç«  | [ä¸‹ä¸€ç«  â†’](chapter11.md)

# ç¬¬10ç« ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM)

æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Models, LDMï¼‰æ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªé©å‘½æ€§è¿›å±•ï¼Œå®ƒé€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´è€ŒéåŸå§‹åƒç´ ç©ºé—´è¿›è¡Œæ‰©æ•£ï¼Œæå¤§åœ°æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨LDMçš„æ ¸å¿ƒæ€æƒ³ï¼ŒåŒ…æ‹¬è‡ªç¼–ç å™¨çš„è®¾è®¡ã€æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§ã€ä»¥åŠå¦‚ä½•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°æ•°é‡çº§çš„åŠ é€Ÿã€‚æ‚¨å°†ç†è§£Stable DiffusionèƒŒåçš„æŠ€æœ¯åŸç†ï¼ŒæŒæ¡è®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„å…³é”®æŠ€å·§ï¼Œå¹¶å­¦ä¹ å¦‚ä½•æƒè¡¡å‹ç¼©ç‡ä¸é‡å»ºè´¨é‡ã€‚

## ç« èŠ‚å¤§çº²

### 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´
- é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜
- æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿
- æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©
- LDMçš„æ•´ä½“æ¶æ„

### 10.2 è‡ªç¼–ç å™¨è®¾è®¡
- VQ-VAE vs KL-VAE
- æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ
- æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

### 10.3 æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£
- æ½œåœ¨æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æè¿°
- å™ªå£°è°ƒåº¦çš„é€‚é…
- æ¡ä»¶æœºåˆ¶åœ¨æ½œåœ¨ç©ºé—´çš„å®ç°
- è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

### 10.4 Stable Diffusionæ¶æ„è¯¦è§£
- æ¨¡å‹ç»„ä»¶åˆ†æ
- CLIPæ–‡æœ¬ç¼–ç å™¨é›†æˆ
- äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
- æ¨ç†ä¼˜åŒ–æŠ€æœ¯

### 10.5 å®è·µè€ƒè™‘ä¸æ‰©å±•
- ä¸åŒåˆ†è¾¨ç‡çš„å¤„ç†
- å¾®è°ƒä¸é€‚é…
- æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²
- æœªæ¥å‘å±•æ–¹å‘

## 10.1 ä»åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´

### 10.1.1 é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æŒ‘æˆ˜

åœ¨åƒç´ ç©ºé—´ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹é¢ä¸´ä¸¥é‡çš„è®¡ç®—ç“¶é¢ˆï¼š

**è®¡ç®—å¤æ‚åº¦åˆ†æ**ï¼š
- 512Ã—512 RGBå›¾åƒï¼š786,432ç»´
- 1024Ã—1024 RGBå›¾åƒï¼š3,145,728ç»´
- U-Netçš„è®¡ç®—é‡ï¼š $O(n^2)$ å¯¹äºè‡ªæ³¨æ„åŠ›å±‚

å…·ä½“æ•°å­—ï¼š
```python
# åƒç´ ç©ºé—´æ‰©æ•£çš„å†…å­˜éœ€æ±‚
def compute_memory_requirements(h, w, c=3, batch_size=1):
    # è¾“å…¥å¼ é‡
    input_size = batch_size * c * h * w * 4  # float32
    
    # U-Netä¸­é—´ç‰¹å¾ï¼ˆå‡è®¾æœ€å¤§é€šé“æ•°2048ï¼‰
    feature_size = batch_size * 2048 * (h//8) * (w//8) * 4
    
    # è‡ªæ³¨æ„åŠ›çŸ©é˜µ
    seq_len = (h//8) * (w//8)
    attention_size = batch_size * seq_len * seq_len * 4
    
    total_gb = (input_size + feature_size + attention_size) / (1024**3)
    return total_gb

# 1024x1024å›¾åƒéœ€è¦çº¦48GBå†…å­˜ï¼
```

### 10.1.2 æ½œåœ¨ç©ºé—´çš„æ ¸å¿ƒä¼˜åŠ¿

LDMé€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œè·å¾—å¤šä¸ªä¼˜åŠ¿ï¼š

1. **è®¡ç®—æ•ˆç‡**ï¼š8å€ä¸‹é‡‡æ ·å‡å°‘64å€è®¡ç®—é‡
2. **è¯­ä¹‰å‹ç¼©**ï¼šæ½œåœ¨è¡¨ç¤ºæ›´æ¥è¿‘è¯­ä¹‰ä¿¡æ¯
3. **æ›´å¥½çš„å½’çº³åç½®**ï¼šè‡ªç„¶å›¾åƒçš„ä½ç»´æµå½¢å‡è®¾
4. **æ¨¡å—åŒ–è®¾è®¡**ï¼šåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆä»»åŠ¡

**å‹ç¼©ç‡vsè´¨é‡çš„æƒè¡¡**ï¼š
```
ä¸‹é‡‡æ ·å› å­ | æ½œåœ¨ç»´åº¦ | åŠ é€Ÿæ¯” | é‡å»ºPSNR
    4       |  64Ã—64   |  16x   |  >30dB
    8       |  32Ã—32   |  64x   |  ~27dB
   16       |  16Ã—16   | 256x   |  ~23dB
```

### 10.1.3 æ„ŸçŸ¥å‹ç¼©vsä¿¡æ¯å‹ç¼©

LDMçš„å…³é”®æ´å¯Ÿæ˜¯åŒºåˆ†ä¸¤ç§å‹ç¼©ï¼š

**ä¿¡æ¯å‹ç¼©**ï¼ˆä¼ ç»Ÿå‹ç¼©ï¼‰ï¼š
- ç›®æ ‡ï¼šå®Œç¾é‡å»ºæ¯ä¸ªåƒç´ 
- æ–¹æ³•ï¼šç†µç¼–ç ã€é¢„æµ‹ç¼–ç 
- é—®é¢˜ï¼šä¿ç•™äº†æ„ŸçŸ¥ä¸é‡è¦çš„ç»†èŠ‚

**æ„ŸçŸ¥å‹ç¼©**ï¼ˆLDMä½¿ç”¨ï¼‰ï¼š
- ç›®æ ‡ï¼šä¿ç•™æ„ŸçŸ¥é‡è¦çš„ç‰¹å¾
- æ–¹æ³•ï¼šå­¦ä¹ çš„ç¼–ç å™¨ + æ„ŸçŸ¥æŸå¤±
- ä¼˜åŠ¿ï¼šæ›´é«˜å‹ç¼©ç‡ï¼Œæ›´è¯­ä¹‰åŒ–çš„è¡¨ç¤º

```python
class PerceptualCompression(nn.Module):
    def __init__(self, perceptual_weight=1.0):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg')
        self.perceptual_weight = perceptual_weight
    
    def forward(self, x, x_recon):
        # åƒç´ çº§æŸå¤±
        pixel_loss = F.l1_loss(x, x_recon)
        
        # æ„ŸçŸ¥æŸå¤±
        perceptual_loss = self.perceptual_loss(x, x_recon)
        
        # ç»„åˆ
        return pixel_loss + self.perceptual_weight * perceptual_loss
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜å‹ç¼©ç‡**  
ä»€ä¹ˆå†³å®šäº†æœ€ä¼˜çš„å‹ç¼©ç‡ï¼Ÿæ˜¯å¦å¯ä»¥æ ¹æ®æ•°æ®é›†ç‰¹æ€§è‡ªé€‚åº”é€‰æ‹©ï¼Ÿè¿™æ¶‰åŠåˆ°ç‡å¤±çœŸç†è®ºå’Œæµå½¢å‡è®¾ã€‚

### 10.1.4 LDMçš„æ•´ä½“æ¶æ„

LDMç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ„æˆï¼š

```python
class LatentDiffusionModel(nn.Module):
    def __init__(self, autoencoder, diffusion_model, conditioning_model):
        super().__init__()
        self.autoencoder = autoencoder  # ç¼–ç /è§£ç å›¾åƒ
        self.diffusion = diffusion_model  # æ½œåœ¨ç©ºé—´æ‰©æ•£
        self.cond_model = conditioning_model  # å¤„ç†æ¡ä»¶ä¿¡æ¯
        
        # å†»ç»“è‡ªç¼–ç å™¨ï¼ˆé€šå¸¸é¢„è®­ç»ƒï¼‰
        self.autoencoder.eval()
        for param in self.autoencoder.parameters():
            param.requires_grad = False
    
    def encode(self, x):
        # å›¾åƒ -> æ½œåœ¨è¡¨ç¤º
        with torch.no_grad():
            z = self.autoencoder.encode(x)
            # å¯é€‰ï¼šæ ‡å‡†åŒ–
            z = z * self.scale_factor
        return z
    
    def decode(self, z):
        # æ½œåœ¨è¡¨ç¤º -> å›¾åƒ
        z = z / self.scale_factor
        with torch.no_grad():
            x = self.autoencoder.decode(z)
        return x
```

<details>
<summary>**ç»ƒä¹  10.1ï¼šåˆ†æå‹ç¼©æ•ˆç‡**</summary>

ç ”ç©¶ä¸åŒå‹ç¼©ç­–ç•¥çš„æ•ˆæœã€‚

1. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - å®ç°ä¸åŒä¸‹é‡‡æ ·ç‡çš„è‡ªç¼–ç å™¨
   - æµ‹é‡é‡å»ºè´¨é‡ï¼ˆPSNR, SSIM, LPIPSï¼‰
   - ç»˜åˆ¶ç‡å¤±çœŸæ›²çº¿

2. **è¯­ä¹‰ä¿ç•™åˆ†æ**ï¼š
   - ä½¿ç”¨é¢„è®­ç»ƒåˆ†ç±»å™¨è¯„ä¼°è¯­ä¹‰ä¿ç•™
   - æ¯”è¾ƒåƒç´ MSE vs æ„ŸçŸ¥æŸå¤±
   - åˆ†æå“ªäº›ç‰¹å¾è¢«ä¿ç•™/ä¸¢å¤±

3. **è®¡ç®—æ•ˆç›Šè¯„ä¼°**ï¼š
   - æµ‹é‡ä¸åŒåˆ†è¾¨ç‡çš„æ¨ç†æ—¶é—´
   - è®¡ç®—å†…å­˜ä½¿ç”¨
   - æ‰¾å‡ºæ•ˆç‡ç“¶é¢ˆ

4. **ç†è®ºæ‹“å±•**ï¼š
   - ä»æµå½¢å‡è®¾è§’åº¦åˆ†æå‹ç¼©
   - ç ”ç©¶æœ€ä¼˜ä¼ è¾“ç†è®ºçš„åº”ç”¨
   - æ¢ç´¢è‡ªé€‚åº”å‹ç¼©ç‡

</details>

### 10.1.5 ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

LDMé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåˆ†ç¦»å‹ç¼©å’Œç”Ÿæˆï¼š

**ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒè‡ªç¼–ç å™¨**
```python
def train_autoencoder(model, dataloader, epochs):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        for x in dataloader:
            # ç¼–ç -è§£ç 
            z = model.encode(x)
            x_recon = model.decode(z)
            
            # é‡å»ºæŸå¤±
            recon_loss = F.l1_loss(x, x_recon)
            
            # æ„ŸçŸ¥æŸå¤±
            p_loss = perceptual_loss(x, x_recon)
            
            # KLæ­£åˆ™åŒ–ï¼ˆå¦‚æœä½¿ç”¨VAEï¼‰
            kl_loss = model.kl_loss(z)
            
            loss = recon_loss + 0.1 * p_loss + 0.001 * kl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

**ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ‰©æ•£æ¨¡å‹**
```python
def train_diffusion(diffusion_model, autoencoder, dataloader):
    # å†»ç»“è‡ªç¼–ç å™¨
    autoencoder.eval()
    
    for x, c in dataloader:
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            z = autoencoder.encode(x)
        
        # æ ‡å‡†æ‰©æ•£è®­ç»ƒ
        t = torch.randint(0, num_steps, (z.shape[0],))
        noise = torch.randn_like(z)
        z_t = add_noise(z, noise, t)
        
        # é¢„æµ‹å™ªå£°
        pred_noise = diffusion_model(z_t, t, c)
        loss = F.mse_loss(pred_noise, noise)
        
        loss.backward()
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šé¢„è®­ç»ƒç­–ç•¥**  
å¯ä»¥ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒé€šç”¨è‡ªç¼–ç å™¨ï¼Œç„¶ååœ¨ç‰¹å®šé¢†åŸŸå¾®è°ƒã€‚è¿™å¤§å¤§å‡å°‘äº†è®­ç»ƒæˆæœ¬ã€‚

### 10.1.6 æ½œåœ¨ç©ºé—´çš„ç‰¹æ€§

ç†æƒ³çš„æ½œåœ¨ç©ºé—´åº”å…·å¤‡ï¼š

1. **å¹³æ»‘æ€§**ï¼šç›¸è¿‘çš„æ½œåœ¨ç¼–ç å¯¹åº”ç›¸ä¼¼çš„å›¾åƒ
2. **è¯­ä¹‰æ€§**ï¼šæ½œåœ¨ç»´åº¦å¯¹åº”æœ‰æ„ä¹‰çš„å˜åŒ–
3. **ç´§å‡‘æ€§**ï¼šé«˜æ•ˆåˆ©ç”¨æ¯ä¸ªç»´åº¦
4. **æ­£æ€æ€§**ï¼šä¾¿äºæ‰©æ•£æ¨¡å‹å»ºæ¨¡

**åˆ†ææ½œåœ¨ç©ºé—´**ï¼š
```python
def analyze_latent_space(autoencoder, dataloader):
    latents = []
    labels = []
    
    with torch.no_grad():
        for x, y in dataloader:
            z = autoencoder.encode(x)
            latents.append(z.cpu())
            labels.append(y.cpu())
    
    latents = torch.cat(latents)
    labels = torch.cat(labels)
    
    # ç»Ÿè®¡ç‰¹æ€§
    print(f"Mean: {latents.mean():.3f}")
    print(f"Std: {latents.std():.3f}")
    print(f"Kurtosis: {stats.kurtosis(latents.numpy().flatten()):.3f}")
    
    # å¯è§†åŒ–ï¼ˆä½¿ç”¨t-SNEæˆ–UMAPï¼‰
    embedded = TSNE(n_components=2).fit_transform(latents.numpy())
    plt.scatter(embedded[:, 0], embedded[:, 1], c=labels)
```

ğŸŒŸ **å¼€æ”¾é—®é¢˜ï¼šæœ€ä¼˜æ½œåœ¨ç©ºé—´è®¾è®¡**  
å¦‚ä½•è®¾è®¡å…·æœ‰ç‰¹å®šå±æ€§çš„æ½œåœ¨ç©ºé—´ï¼Ÿèƒ½å¦å­¦ä¹ è§£è€¦çš„è¡¨ç¤ºï¼Ÿè¿™æ¶‰åŠåˆ°è¡¨ç¤ºå­¦ä¹ å’Œå› æœæ¨æ–­çš„å‰æ²¿ç ”ç©¶ã€‚

## 10.3 æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£

### 10.3.1 æ½œåœ¨æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æè¿°

åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£éœ€è¦é‡æ–°å®šä¹‰å‰å‘å’Œåå‘è¿‡ç¨‹ï¼š

**å‰å‘è¿‡ç¨‹**ï¼š
$$q(\mathbf{z}_t | \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_t; \sqrt{\bar{\alpha}_t}\mathbf{z}_0, (1-\bar{\alpha}_t)\mathbf{I})$$

å…¶ä¸­ $\mathbf{z}_0 = \mathcal{E}(\mathbf{x})$ æ˜¯ç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºã€‚

**å…³é”®å·®å¼‚**ï¼š
1. **ç»´åº¦é™ä½**ï¼šä» $\mathbb{R}^{3 \times H \times W}$ åˆ° $\mathbb{R}^{C \times h \times w}$
2. **åˆ†å¸ƒå˜åŒ–**ï¼šæ½œåœ¨ç©ºé—´å¯èƒ½ä¸å®Œå…¨ç¬¦åˆé«˜æ–¯åˆ†å¸ƒ
3. **å°ºåº¦å·®å¼‚**ï¼šéœ€è¦é€‚å½“çš„å½’ä¸€åŒ–

**åå‘è¿‡ç¨‹**ï¼š
$$p_\theta(\mathbf{z}_{t-1} | \mathbf{z}_t) = \mathcal{N}(\mathbf{z}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{z}_t, t), \sigma_t^2\mathbf{I})$$

æ‰©æ•£æ¨¡å‹å­¦ä¹ é¢„æµ‹å™ªå£° $\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t)$ ï¼Œç”¨äºè®¡ç®—å‡å€¼ï¼š
$$\boldsymbol{\mu}_\theta(\mathbf{z}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{z}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t)\right)$$

### 10.3.2 å™ªå£°è°ƒåº¦çš„é€‚é…

æ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§ä¸åƒç´ ç©ºé—´ä¸åŒï¼Œéœ€è¦è°ƒæ•´å™ªå£°è°ƒåº¦ï¼š

**1. ä¿¡å™ªæ¯”åˆ†æ**ï¼š
```python
def analyze_latent_snr(autoencoder, dataloader):
    latents = []
    with torch.no_grad():
        for x, _ in dataloader:
            z = autoencoder.encode(x)
            latents.append(z)
    
    latents = torch.cat(latents)
    
    # è®¡ç®—ä¿¡å·åŠŸç‡
    signal_power = (latents ** 2).mean()
    
    # åˆ†æä¸åŒå™ªå£°æ°´å¹³çš„SNR
    for t in [0.1, 0.5, 0.9]:
        noise_power = (1 - t) * signal_power
        snr = 10 * torch.log10(signal_power / noise_power)
        print(f"t={t}: SNR={snr:.2f} dB")
```

**2. è‡ªé€‚åº”è°ƒåº¦**ï¼š
```python
class AdaptiveNoiseSchedule:
    def __init__(self, latent_stats):
        self.mean = latent_stats['mean']
        self.std = latent_stats['std']
        
    def get_betas(self, num_steps):
        # æ ¹æ®æ½œåœ¨ç©ºé—´ç»Ÿè®¡è°ƒæ•´beta
        # ç¡®ä¿æœ€ç»ˆSNRæ¥è¿‘0
        target_final_snr = 0.001
        beta_start = 0.0001 * self.std
        beta_end = 0.02 * self.std
        
        return torch.linspace(beta_start, beta_end, num_steps)
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šé¢„è®¡ç®—ç»Ÿè®¡é‡**  
åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®¡ç®—æ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨äºå½’ä¸€åŒ–å’Œå™ªå£°è°ƒåº¦è®¾è®¡ã€‚

### 10.3.3 æ¡ä»¶æœºåˆ¶åœ¨æ½œåœ¨ç©ºé—´çš„å®ç°

LDMä¸­çš„æ¡ä»¶ä¿¡æ¯é€šè¿‡å¤šç§æ–¹å¼æ³¨å…¥ï¼š

**1. äº¤å‰æ³¨æ„åŠ›æœºåˆ¶**ï¼š
```python
class CrossAttentionBlock(nn.Module):
    def __init__(self, dim, context_dim, num_heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            dim, num_heads, kdim=context_dim, vdim=context_dim
        )
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
    def forward(self, x, context):
        # x: [B, H*W, C] æ½œåœ¨ç‰¹å¾
        # context: [B, L, D] æ¡ä»¶ç¼–ç ï¼ˆå¦‚æ–‡æœ¬ï¼‰
        
        x_norm = self.norm1(x)
        attn_out = self.attention(x_norm, context, context)[0]
        x = x + attn_out
        return x
```

**2. ç‰¹å¾è°ƒåˆ¶ï¼ˆFiLMï¼‰**ï¼š
```python
class FiLMLayer(nn.Module):
    def __init__(self, latent_dim, condition_dim):
        super().__init__()
        self.scale_net = nn.Linear(condition_dim, latent_dim)
        self.shift_net = nn.Linear(condition_dim, latent_dim)
        
    def forward(self, x, condition):
        scale = self.scale_net(condition).unsqueeze(2).unsqueeze(3)
        shift = self.shift_net(condition).unsqueeze(2).unsqueeze(3)
        return x * (1 + scale) + shift
```

**3. ç©ºé—´æ¡ä»¶æ§åˆ¶**ï¼š
```python
def add_spatial_conditioning(z_t, spatial_cond, method='concat'):
    if method == 'concat':
        # ç›´æ¥æ‹¼æ¥
        return torch.cat([z_t, spatial_cond], dim=1)
    elif method == 'add':
        # åŠ æ€§èåˆï¼ˆéœ€è¦ç»´åº¦åŒ¹é…ï¼‰
        return z_t + spatial_cond
    elif method == 'gated':
        # é—¨æ§èåˆ
        gate = torch.sigmoid(spatial_cond)
        return z_t * gate + spatial_cond * (1 - gate)
```

ğŸ”¬ **ç ”ç©¶æ–¹å‘ï¼šæ¡ä»¶æ³¨å…¥çš„æœ€ä¼˜ä½ç½®**  
åº”è¯¥åœ¨U-Netçš„å“ªäº›å±‚æ³¨å…¥æ¡ä»¶ä¿¡æ¯ï¼Ÿæ—©æœŸå±‚å½±å“å…¨å±€ç»“æ„ï¼ŒåæœŸå±‚æ§åˆ¶ç»†èŠ‚ã€‚ç³»ç»Ÿç ”ç©¶è¿™ç§æƒè¡¡å¯ä»¥æŒ‡å¯¼æ¶æ„è®¾è®¡ã€‚

### 10.3.4 è®­ç»ƒç­–ç•¥ä¸æŠ€å·§

**1. æ¸è¿›å¼è®­ç»ƒ**ï¼š
```python
class ProgressiveLatentDiffusion:
    def __init__(self, autoencoder, diffusion_model):
        self.autoencoder = autoencoder
        self.diffusion = diffusion_model
        self.current_resolution = 32
        
    def train_step(self, x, epoch):
        # æ¸è¿›æé«˜åˆ†è¾¨ç‡
        if epoch > 100 and self.current_resolution < 64:
            self.current_resolution = 64
            self.update_model_resolution()
        
        # åŠ¨æ€è°ƒæ•´æ½œåœ¨ç©ºé—´
        with torch.no_grad():
            z = self.autoencoder.encode(x)
            if self.current_resolution < z.shape[-1]:
                z = F.interpolate(z, size=self.current_resolution)
        
        # æ ‡å‡†æ‰©æ•£è®­ç»ƒ
        return self.diffusion.training_step(z)
```

**2. æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
```python
# ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒ
scaler = torch.cuda.amp.GradScaler()

def train_with_amp(model, data, optimizer):
    with torch.cuda.amp.autocast():
        # å‰å‘ä¼ æ’­åœ¨åŠç²¾åº¦
        loss = model.compute_loss(data)
    
    # åå‘ä¼ æ’­å’Œä¼˜åŒ–åœ¨å…¨ç²¾åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**3. æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 10.3.5 è´¨é‡ä¸æ•ˆç‡çš„æƒè¡¡

**å‹ç¼©ç‡ vs é‡å»ºè´¨é‡**ï¼š

| ä¸‹é‡‡æ ·å› å­ | å‹ç¼©ç‡ | é€Ÿåº¦æå‡ | FID | é€‚ç”¨åœºæ™¯ |
|-----------|--------|----------|-----|---------|
| 4x | 16x | 10-15x | ~5 | é«˜è´¨é‡ç”Ÿæˆ |
| 8x | 64x | 40-60x | ~10 | å¹³è¡¡é€‰æ‹© |
| 16x | 256x | 150-200x | ~25 | å¿«é€Ÿé¢„è§ˆ |

**åŠ¨æ€è´¨é‡è°ƒæ•´**ï¼š
```python
class AdaptiveQualityLDM:
    def __init__(self, models_dict):
        # models_dict: {4: model_4x, 8: model_8x, 16: model_16x}
        self.models = models_dict
        
    def generate(self, prompt, quality='balanced'):
        if quality == 'draft':
            model = self.models[16]
            steps = 10
        elif quality == 'balanced':
            model = self.models[8]
            steps = 25
        else:  # quality == 'high'
            model = self.models[4]
            steps = 50
            
        return model.sample(prompt, num_steps=steps)
```

<details>
<summary>**ç»ƒä¹  10.3ï¼šæ½œåœ¨ç©ºé—´æ‰©æ•£å®éªŒ**</summary>

æ¢ç´¢æ½œåœ¨ç©ºé—´æ‰©æ•£çš„å„ä¸ªæ–¹é¢ã€‚

1. **å‹ç¼©ç‡å½±å“åˆ†æ**ï¼š
   - è®­ç»ƒä¸åŒå‹ç¼©ç‡çš„LDMï¼ˆ4x, 8x, 16xï¼‰
   - æ¯”è¾ƒç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œé€Ÿåº¦
   - ç»˜åˆ¶å‹ç¼©ç‡-è´¨é‡æ›²çº¿

2. **å™ªå£°è°ƒåº¦ä¼˜åŒ–**ï¼š
   - å®ç°åŸºäºSNRçš„è‡ªé€‚åº”è°ƒåº¦
   - æ¯”è¾ƒçº¿æ€§ã€ä½™å¼¦å’Œå­¦ä¹ çš„è°ƒåº¦
   - åˆ†æå¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“

3. **æ¡ä»¶æ³¨å…¥ç ”ç©¶**ï¼š
   - å®ç°ä¸åŒçš„æ¡ä»¶æ³¨å…¥æ–¹æ³•
   - æµ‹è¯•åœ¨ä¸åŒå±‚æ³¨å…¥çš„æ•ˆæœ
   - è¯„ä¼°å¯¹å¯æ§æ€§çš„å½±å“

4. **åˆ›æ–°æ¢ç´¢**ï¼š
   - è®¾è®¡å¤šå°ºåº¦æ½œåœ¨ç©ºé—´ï¼ˆå±‚æ¬¡åŒ–LDMï¼‰
   - ç ”ç©¶å‘é‡é‡åŒ–çš„æ½œåœ¨æ‰©æ•£
   - æ¢ç´¢è‡ªé€‚åº”å‹ç¼©ç‡é€‰æ‹©

</details>

### 10.3.6 è°ƒè¯•ä¸å¯è§†åŒ–

**ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼š
```python
class LDMMonitor:
    def __init__(self, autoencoder):
        self.autoencoder = autoencoder
        
    def visualize_diffusion_process(self, model, x0, steps=[0, 250, 500, 750, 999]):
        """å¯è§†åŒ–æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹"""
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        z0 = self.autoencoder.encode(x0)
        
        # å‰å‘æ‰©æ•£
        zs_forward = []
        for t in steps:
            zt = add_noise(z0, t)
            zs_forward.append(zt)
        
        # åå‘å»å™ª
        zs_reverse = []
        zt = torch.randn_like(z0)
        for t in reversed(range(1000)):
            zt = denoise_step(model, zt, t)
            if t in steps:
                zs_reverse.append(zt)
        
        # è§£ç å¹¶å¯è§†åŒ–
        imgs_forward = [self.autoencoder.decode(z) for z in zs_forward]
        imgs_reverse = [self.autoencoder.decode(z) for z in zs_reverse]
        
        return imgs_forward, imgs_reverse
```

**è¯Šæ–­å·¥å…·**ï¼š
```python
def diagnose_latent_diffusion(model, autoencoder, test_batch):
    """è¯Šæ–­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜"""
    
    # 1. æ£€æŸ¥æ½œåœ¨ç©ºé—´åˆ†å¸ƒ
    z = autoencoder.encode(test_batch)
    print(f"Latent stats - Mean: {z.mean():.3f}, Std: {z.std():.3f}")
    
    # 2. æ£€æŸ¥é‡å»ºè´¨é‡
    x_recon = autoencoder.decode(z)
    recon_error = F.mse_loss(test_batch, x_recon)
    print(f"Reconstruction error: {recon_error:.4f}")
    
    # 3. æ£€æŸ¥å™ªå£°é¢„æµ‹
    t = torch.randint(0, 1000, (z.shape[0],))
    noise = torch.randn_like(z)
    z_noisy = add_noise(z, noise, t)
    pred_noise = model(z_noisy, t)
    noise_error = F.mse_loss(pred_noise, noise)
    print(f"Noise prediction error: {noise_error:.4f}")
    
    # 4. æ£€æŸ¥ç”Ÿæˆæ ·æœ¬
    z_sample = torch.randn_like(z)
    for t in reversed(range(0, 1000, 100)):
        z_sample = denoise_step(model, z_sample, t)
    x_sample = autoencoder.decode(z_sample)
    
    return {
        'latent_stats': (z.mean().item(), z.std().item()),
        'recon_error': recon_error.item(),
        'noise_error': noise_error.item(),
        'sample': x_sample
    }
```

ğŸŒŸ **æœ€ä½³å®è·µï¼šå¤šé˜¶æ®µè°ƒè¯•**  
å…ˆç¡®ä¿è‡ªç¼–ç å™¨å·¥ä½œæ­£å¸¸ï¼Œå†è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿè¿­ä»£ï¼ŒéªŒè¯æµç¨‹æ­£ç¡®åå†æ‰©å±•åˆ°å¤§è§„æ¨¡è®­ç»ƒã€‚

## 10.2 è‡ªç¼–ç å™¨è®¾è®¡

### 10.2.1 VQ-VAE vs KL-VAE

LDMä¸­å¸¸ç”¨ä¸¤ç§è‡ªç¼–ç å™¨æ¶æ„ï¼Œå„æœ‰ä¼˜åŠ£ï¼š

**VQ-VAEï¼ˆVector Quantized VAEï¼‰**ï¼š
```python
class VQVAE(nn.Module):
    def __init__(self, num_embeddings=8192, embedding_dim=256):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.quantize = VectorQuantizer(num_embeddings, embedding_dim)
    
    def forward(self, x):
        # ç¼–ç 
        z_e = self.encoder(x)
        
        # å‘é‡é‡åŒ–
        z_q, indices, commitment_loss = self.quantize(z_e)
        
        # è§£ç 
        x_recon = self.decoder(z_q)
        
        return x_recon, commitment_loss
```

**KL-VAEï¼ˆKLæ­£åˆ™åŒ–çš„VAEï¼‰**ï¼š
```python
class KLVAE(nn.Module):
    def __init__(self, latent_dim=256, kl_weight=1e-6):
        super().__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.kl_weight = kl_weight
        
        # ç¼–ç å™¨è¾“å‡ºå‡å€¼å’Œå¯¹æ•°æ–¹å·®
        self.mean_layer = nn.Conv2d(512, latent_dim, 1)
        self.logvar_layer = nn.Conv2d(512, latent_dim, 1)
    
    def encode(self, x):
        h = self.encoder(x)
        mean = self.mean_layer(h)
        logvar = self.logvar_layer(h)
        
        # é‡å‚æ•°åŒ–
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mean + eps * std
        
        return z, mean, logvar
    
    def kl_loss(self, mean, logvar):
        # KL(q(z|x) || p(z))ï¼Œå…¶ä¸­p(z) = N(0, I)
        return -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
```

**æ¯”è¾ƒ**ï¼š
| ç‰¹æ€§ | VQ-VAE | KL-VAE |
|------|--------|---------|
| æ½œåœ¨ç©ºé—´ | ç¦»æ•£ | è¿ç»­ |
| è®­ç»ƒç¨³å®šæ€§ | è¾ƒéš¾ï¼ˆéœ€è¦æŠ€å·§ï¼‰ | è¾ƒå¥½ |
| å‹ç¼©ç‡ | å›ºå®š | çµæ´» |
| åç»­æ‰©æ•£ | éœ€è¦é€‚é… | ç›´æ¥åº”ç”¨ |

ğŸ’¡ **å®è·µé€‰æ‹©ï¼šä¸ºä»€ä¹ˆLDMåå¥½KL-VAE**  
è¿ç»­æ½œåœ¨ç©ºé—´æ›´é€‚åˆæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯å™ªå£°å‡è®¾ã€‚æå°çš„KLæƒé‡ï¼ˆ1e-6ï¼‰ä½¿å…¶æ¥è¿‘ç¡®å®šæ€§ç¼–ç å™¨ã€‚

### 10.2.2 æ„ŸçŸ¥æŸå¤±ä¸å¯¹æŠ—è®­ç»ƒ

å•çº¯çš„åƒç´ é‡å»ºæŸå¤±ä¼šå¯¼è‡´æ¨¡ç³Šç»“æœã€‚LDMä½¿ç”¨ç»„åˆæŸå¤±ï¼š

```python
class AutoencoderLoss(nn.Module):
    def __init__(self, disc_start=50000, perceptual_weight=1.0, 
                 disc_weight=0.5, kl_weight=1e-6):
        super().__init__()
        self.perceptual_loss = lpips.LPIPS(net='vgg').eval()
        self.disc_start = disc_start
        self.perceptual_weight = perceptual_weight
        self.disc_weight = disc_weight
        self.kl_weight = kl_weight
    
    def forward(self, x, x_recon, mean, logvar, disc_fake, disc_real, step):
        # 1. é‡å»ºæŸå¤±
        rec_loss = F.l1_loss(x, x_recon)
        
        # 2. æ„ŸçŸ¥æŸå¤±
        if self.perceptual_weight > 0:
            p_loss = self.perceptual_loss(x, x_recon).mean()
        else:
            p_loss = torch.tensor(0.0)
        
        # 3. KLæŸå¤±
        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())
        kl_loss = kl_loss / x.shape[0]  # æ‰¹æ¬¡å¹³å‡
        
        # 4. å¯¹æŠ—æŸå¤±ï¼ˆå»¶è¿Ÿå¯åŠ¨ï¼‰
        if step > self.disc_start:
            # ç”Ÿæˆå™¨æŸå¤±ï¼šæ¬ºéª—åˆ¤åˆ«å™¨
            g_loss = -torch.mean(disc_fake)
        else:
            g_loss = torch.tensor(0.0)
        
        # ç»„åˆ
        loss = rec_loss + self.perceptual_weight * p_loss + \
               self.kl_weight * kl_loss + self.disc_weight * g_loss
        
        return loss, {
            'rec': rec_loss.item(),
            'perceptual': p_loss.item(),
            'kl': kl_loss.item(),
            'gen': g_loss.item()
        }
```

**åˆ¤åˆ«å™¨è®¾è®¡**ï¼š
```python
class PatchDiscriminator(nn.Module):
    def __init__(self, in_channels=3, ndf=64, n_layers=3):
        super().__init__()
        layers = [nn.Conv2d(in_channels, ndf, 4, 2, 1), 
                  nn.LeakyReLU(0.2, True)]
        
        for i in range(1, n_layers):
            in_ch = ndf * min(2**(i-1), 8)
            out_ch = ndf * min(2**i, 8)
            layers += [
                nn.Conv2d(in_ch, out_ch, 4, 2, 1),
                nn.BatchNorm2d(out_ch),
                nn.LeakyReLU(0.2, True)
            ]
        
        # æœ€åä¸€å±‚è¾“å‡ºå•é€šé“ç‰¹å¾å›¾
        layers.append(nn.Conv2d(out_ch, 1, 4, 1, 1))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
```

### 10.2.3 æ½œåœ¨ç©ºé—´çš„æ­£åˆ™åŒ–

ä¸ºäº†ç¡®ä¿æ½œåœ¨ç©ºé—´é€‚åˆæ‰©æ•£å»ºæ¨¡ï¼Œéœ€è¦é€‚å½“çš„æ­£åˆ™åŒ–ï¼š

**1. KLæ­£åˆ™åŒ–çš„ä½œç”¨**ï¼š
- é˜²æ­¢æ½œåœ¨ç©ºé—´åç¼©
- é¼“åŠ±æ¥è¿‘æ ‡å‡†é«˜æ–¯åˆ†å¸ƒ
- ä½†æƒé‡éœ€è¦å¾ˆå°é¿å…ä¿¡æ¯æŸå¤±

**2. è°±å½’ä¸€åŒ–**ï¼š
```python
def add_spectral_norm(module):
    """é€’å½’åœ°ä¸ºæ‰€æœ‰å·ç§¯å±‚æ·»åŠ è°±å½’ä¸€åŒ–"""
    for name, child in module.named_children():
        if isinstance(child, nn.Conv2d):
            setattr(module, name, nn.utils.spectral_norm(child))
        else:
            add_spectral_norm(child)
```

**3. æ¢¯åº¦æƒ©ç½š**ï¼š
```python
def gradient_penalty(discriminator, real, fake):
    batch_size = real.size(0)
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real.device)
    interpolated = epsilon * real + (1 - epsilon) * fake
    interpolated.requires_grad_(True)
    
    d_interpolated = discriminator(interpolated)
    gradients = torch.autograd.grad(
        outputs=d_interpolated, inputs=interpolated,
        grad_outputs=torch.ones_like(d_interpolated),
        create_graph=True, retain_graph=True
    )[0]
    
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    penalty = ((gradient_norm - 1) ** 2).mean()
    
    return penalty
```

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæœ€ä¼˜æ­£åˆ™åŒ–ç­–ç•¥**  
å¦‚ä½•å¹³è¡¡é‡å»ºè´¨é‡å’Œæ½œåœ¨ç©ºé—´çš„è§„æ•´æ€§ï¼Ÿæ˜¯å¦å¯ä»¥è®¾è®¡è‡ªé€‚åº”çš„æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Ÿ

### 10.2.4 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ç»†èŠ‚

**é«˜æ•ˆçš„ç¼–ç å™¨è®¾è®¡**ï¼š
```python
class Encoder(nn.Module):
    def __init__(self, in_channels=3, ch=128, ch_mult=(1,2,4,8), 
                 num_res_blocks=2, z_channels=4):
        super().__init__()
        self.num_resolutions = len(ch_mult)
        
        # åˆå§‹å·ç§¯
        self.conv_in = nn.Conv2d(in_channels, ch, 3, 1, 1)
        
        # ä¸‹é‡‡æ ·å—
        self.down = nn.ModuleList()
        in_ch = ch
        for i, mult in enumerate(ch_mult):
            out_ch = ch * mult
            for j in range(num_res_blocks):
                self.down.append(ResnetBlock(in_ch, out_ch))
                in_ch = out_ch
            
            if i != self.num_resolutions - 1:
                self.down.append(Downsample(in_ch))
        
        # ä¸­é—´å—
        self.mid = nn.Module()
        self.mid.block_1 = ResnetBlock(in_ch, in_ch)
        self.mid.attn_1 = AttnBlock(in_ch)
        self.mid.block_2 = ResnetBlock(in_ch, in_ch)
        
        # è¾“å‡ºå±‚
        self.norm_out = nn.GroupNorm(32, in_ch)
        self.conv_out = nn.Conv2d(in_ch, 2*z_channels, 3, 1, 1)  # å‡å€¼å’Œæ–¹å·®
    
    def forward(self, x):
        # ç¼–ç 
        h = self.conv_in(x)
        
        for module in self.down:
            h = module(h)
        
        # ä¸­é—´å¤„ç†
        h = self.mid.block_1(h)
        h = self.mid.attn_1(h)
        h = self.mid.block_2(h)
        
        # è¾“å‡º
        h = self.norm_out(h)
        h = F.silu(h)
        h = self.conv_out(h)
        
        return h
```

**æ®‹å·®å—å®ç°**ï¼š
```python
class ResnetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.0):
        super().__init__()
        self.norm1 = nn.GroupNorm(32, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)
        self.norm2 = nn.GroupNorm(32, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        
        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.shortcut = nn.Identity()
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = F.silu(h)
        h = self.conv1(h)
        
        h = self.norm2(h)
        h = F.silu(h)
        h = self.dropout(h)
        h = self.conv2(h)
        
        return h + self.shortcut(x)
```

<details>
<summary>**ç»ƒä¹  10.2ï¼šè‡ªç¼–ç å™¨æ¶æ„å®éªŒ**</summary>

æ¢ç´¢ä¸åŒçš„è‡ªç¼–ç å™¨è®¾è®¡é€‰æ‹©ã€‚

1. **æ¶æ„æ¯”è¾ƒ**ï¼š
   - å®ç°VQ-VAEå’ŒKL-VAE
   - æ¯”è¾ƒé‡å»ºè´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§
   - åˆ†ææ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§

2. **æŸå¤±å‡½æ•°ç ”ç©¶**ï¼š
   - è°ƒæ•´å„æŸå¤±é¡¹çš„æƒé‡
   - å°è¯•ä¸åŒçš„æ„ŸçŸ¥ç½‘ç»œï¼ˆVGG, ResNetï¼‰
   - ç ”ç©¶å¯¹æŠ—è®­ç»ƒçš„å¯åŠ¨æ—¶æœº

3. **å‹ç¼©ç‡å®éªŒ**ï¼š
   - æµ‹è¯•ä¸åŒçš„æ½œåœ¨ç»´åº¦
   - åˆ†æç‡å¤±çœŸæƒè¡¡
   - æ‰¾å‡ºç‰¹å®šæ•°æ®é›†çš„æœ€ä¼˜è®¾ç½®

4. **åˆ›æ–°è®¾è®¡**ï¼š
   - å°è¯•æ¸è¿›å¼è®­ç»ƒï¼ˆé€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼‰
   - å®ç°æ¡ä»¶è‡ªç¼–ç å™¨
   - æ¢ç´¢å±‚æ¬¡åŒ–æ½œåœ¨è¡¨ç¤º

</details>

### 10.2.5 è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§

**1. å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
def get_lr_scheduler(optimizer, warmup_steps=5000):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 1.0
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**2. EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰**ï¼š
```python
class EMA:
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = self.decay * self.shadow[name] + \
                                   (1 - self.decay) * param.data
```

**3. æ¢¯åº¦ç´¯ç§¯**ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = compute_loss(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

ğŸ’¡ **è°ƒè¯•æŠ€å·§ï¼šç›‘æ§æ½œåœ¨ç©ºé—´**  
å®šæœŸå¯è§†åŒ–æ½œåœ¨ç¼–ç çš„åˆ†å¸ƒï¼Œç¡®ä¿æ²¡æœ‰æ¨¡å¼å´©æºƒæˆ–å¼‚å¸¸å€¼ã€‚

### 10.2.6 é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨

ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨å¯ä»¥å¤§å¤§åŠ é€Ÿå¼€å‘ï¼š

```python
def load_pretrained_autoencoder(model_id="stabilityai/sd-vae-ft-mse"):
    from diffusers import AutoencoderKL
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    vae = AutoencoderKL.from_pretrained(model_id)
    
    # é€‚é…æ¥å£
    class PretrainedAutoencoder(nn.Module):
        def __init__(self, vae):
            super().__init__()
            self.vae = vae
            self.scale_factor = 0.18215  # SDçš„æ ‡å‡†ç¼©æ”¾å› å­
        
        def encode(self, x):
            # x: [B, 3, H, W] in [-1, 1]
            latent = self.vae.encode(x).latent_dist.sample()
            return latent * self.scale_factor
        
        def decode(self, z):
            # z: [B, 4, H/8, W/8]
            z = z / self.scale_factor
            return self.vae.decode(z).sample
    
    return PretrainedAutoencoder(vae)
```

ğŸŒŸ **æœ€ä½³å®è·µï¼šè¿ç§»å­¦ä¹ **  
å³ä½¿ç›®æ ‡é¢†åŸŸä¸åŒï¼Œä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é€šå¸¸æ¯”ä»å¤´è®­ç»ƒæ›´å¥½ã€‚è‡ªç„¶å›¾åƒçš„ç¼–ç å™¨å¯ä»¥å¾ˆå¥½åœ°è¿ç§»åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ã€‚

## 10.4 Stable Diffusionæ¶æ„è¯¦è§£

### 10.4.1 æ•´ä½“æ¶æ„æ¦‚è§ˆ

Stable Diffusionæ˜¯LDMæœ€æˆåŠŸçš„å®ç°ï¼Œå…¶æ¶æ„ç²¾å¿ƒå¹³è¡¡äº†æ•ˆç‡å’Œè´¨é‡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å›¾åƒ      â”‚â”€â”€â”€â”€â–¶â”‚  VAEç¼–ç å™¨   â”‚â”€â”€â”€â”€â–¶â”‚ æ½œåœ¨è¡¨ç¤º z  â”‚
â”‚ 512Ã—512Ã—3   â”‚     â”‚  (ä¸‹é‡‡æ ·8x)  â”‚     â”‚  64Ã—64Ã—4    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–‡æœ¬æç¤º    â”‚â”€â”€â”€â”€â–¶â”‚ CLIPç¼–ç å™¨   â”‚â”€â”€â”€â”€â–¶â”‚  æ–‡æœ¬åµŒå…¥   â”‚
â”‚             â”‚     â”‚              â”‚     â”‚  77Ã—768     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      U-Netå»å™ªç½‘ç»œ           â”‚
                    â”‚   (å¸¦äº¤å‰æ³¨æ„åŠ›æœºåˆ¶)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  VAEè§£ç å™¨   â”‚â”€â”€â”€â”€â–¶â”‚  ç”Ÿæˆå›¾åƒ   â”‚
                    â”‚  (ä¸Šé‡‡æ ·8x)  â”‚     â”‚ 512Ã—512Ã—3   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å‚æ•°**ï¼š
- æ½œåœ¨ç»´åº¦ï¼š4
- ä¸‹é‡‡æ ·å› å­ï¼š8
- U-Neté€šé“æ•°ï¼š320 â†’ 640 â†’ 1280 â†’ 1280
- æ³¨æ„åŠ›åˆ†è¾¨ç‡ï¼š32Ã—32, 16Ã—16, 8Ã—8
- æ€»å‚æ•°é‡ï¼š~860Mï¼ˆU-Netï¼‰+ 83Mï¼ˆVAEï¼‰+ 123Mï¼ˆCLIPï¼‰

### 10.4.2 VAEç»„ä»¶è¯¦è§£

Stable Diffusionä½¿ç”¨KL-æ­£åˆ™åŒ–çš„VAEï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

```python
class StableDiffusionVAE(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¼–ç å™¨é…ç½®
        self.encoder = Encoder(
            in_channels=3,
            out_channels=8,  # å‡å€¼å’Œæ–¹å·®å„4é€šé“
            ch=128,
            ch_mult=(1, 2, 4, 4),  # é€šé“å€å¢å› å­
            num_res_blocks=2,
            z_channels=4
        )
        
        # è§£ç å™¨é…ç½®ï¼ˆé•œåƒç»“æ„ï¼‰
        self.decoder = Decoder(
            in_channels=4,
            out_channels=3,
            ch=128,
            ch_mult=(1, 2, 4, 4),
            num_res_blocks=2
        )
        
        # å…³é”®çš„ç¼©æ”¾å› å­
        self.scale_factor = 0.18215
        
    def encode(self, x):
        # x: [B, 3, H, W] in [-1, 1]
        h = self.encoder(x)
        mean, logvar = torch.chunk(h, 2, dim=1)
        
        # é‡‡æ ·
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mean + eps * std
        
        # åº”ç”¨ç¼©æ”¾å› å­
        z = z * self.scale_factor
        return z
```

ğŸ’¡ **å…³é”®ç»†èŠ‚ï¼šç¼©æ”¾å› å­çš„ä½œç”¨**  
0.18215è¿™ä¸ªé­”æ³•æ•°å­—å°†æ½œåœ¨è¡¨ç¤ºå½’ä¸€åŒ–åˆ°å•ä½æ–¹å·®é™„è¿‘ï¼Œè¿™å¯¹æ‰©æ•£æ¨¡å‹çš„ç¨³å®šè®­ç»ƒè‡³å…³é‡è¦ã€‚å®ƒæ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šç»éªŒç¡®å®šçš„ã€‚

### 10.4.3 CLIPæ–‡æœ¬ç¼–ç å™¨

Stable Diffusionä½¿ç”¨OpenAIçš„CLIP ViT-L/14æ¨¡å‹ç¼–ç æ–‡æœ¬ï¼š

```python
class CLIPTextEncoder:
    def __init__(self, version="openai/clip-vit-large-patch14"):
        self.tokenizer = CLIPTokenizer.from_pretrained(version)
        self.model = CLIPTextModel.from_pretrained(version)
        self.max_length = 77
        
    def encode(self, text):
        # åˆ†è¯
        tokens = self.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )
        
        # ç¼–ç 
        outputs = self.model(**tokens)
        
        # è¿”å›æœ€åéšè—çŠ¶æ€
        # shape: [batch_size, 77, 768]
        return outputs.last_hidden_state
```

**æ–‡æœ¬ç¼–ç ç‰¹æ€§**ï¼š
- æœ€å¤§é•¿åº¦ï¼š77 tokens
- åµŒå…¥ç»´åº¦ï¼š768
- ä½¿ç”¨æ•´ä¸ªåºåˆ—ï¼ˆä¸ä»…æ˜¯[CLS] tokenï¼‰
- ä¿ç•™ä½ç½®ä¿¡æ¯ç”¨äºç»†ç²’åº¦æ§åˆ¶

ğŸ”¬ **ç ”ç©¶çº¿ç´¢ï¼šæ›´å¥½çš„æ–‡æœ¬ç¼–ç å™¨**  
CLIPæ˜¯ä¸ºå›¾åƒ-æ–‡æœ¬å¯¹é½è®­ç»ƒçš„ï¼Œä¸ä¸€å®šæœ€é€‚åˆç”Ÿæˆä»»åŠ¡ã€‚ä¸“é—¨ä¸ºæ‰©æ•£æ¨¡å‹è®¾è®¡çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚T5ï¼‰å¯èƒ½æä¾›æ›´å¥½çš„æ§åˆ¶ã€‚

### 10.4.4 U-Netæ¶æ„ç»†èŠ‚

Stable Diffusionçš„U-Netæ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒï¼š

```python
class StableDiffusionUNet(nn.Module):
    def __init__(
        self,
        in_channels=4,
        out_channels=4,
        model_channels=320,
        attention_resolutions=[4, 2, 1],
        channel_mult=[1, 2, 4, 4],
        num_heads=8,
        context_dim=768,  # CLIP embedding dim
    ):
        super().__init__()
        
        # æ—¶é—´åµŒå…¥
        self.time_embed = nn.Sequential(
            nn.Linear(model_channels, model_channels * 4),
            nn.SiLU(),
            nn.Linear(model_channels * 4, model_channels * 4),
        )
        
        # è¾“å…¥å—
        self.input_blocks = nn.ModuleList([
            TimestepEmbedSequential(
                nn.Conv2d(in_channels, model_channels, 3, padding=1)
            )
        ])
        
        # ä¸‹é‡‡æ ·å—
        ch = model_channels
        for level, mult in enumerate(channel_mult):
            for _ in range(num_res_blocks):
                layers = [
                    ResBlock(ch, model_channels * mult),
                ]
                
                # åœ¨ç‰¹å®šåˆ†è¾¨ç‡æ·»åŠ æ³¨æ„åŠ›
                if level in attention_resolutions:
                    layers.append(
                        SpatialTransformer(
                            model_channels * mult,
                            num_heads=num_heads,
                            context_dim=context_dim
                        )
                    )
                
                self.input_blocks.append(TimestepEmbedSequential(*layers))
                ch = model_channels * mult
            
            # ä¸‹é‡‡æ ·ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if level != len(channel_mult) - 1:
                self.input_blocks.append(
                    TimestepEmbedSequential(Downsample(ch))
                )
```

### 10.4.5 äº¤å‰æ³¨æ„åŠ›æœºåˆ¶

äº¤å‰æ³¨æ„åŠ›æ˜¯æ–‡æœ¬æ§åˆ¶çš„å…³é”®ï¼š

```python
class CrossAttention(nn.Module):
    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = context_dim or query_dim
        
        self.heads = heads
        self.scale = dim_head ** -0.5
        
        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_out = nn.Linear(inner_dim, query_dim)
        
    def forward(self, x, context=None):
        # x: [B, HW, C] - å›¾åƒç‰¹å¾
        # context: [B, L, D] - æ–‡æœ¬åµŒå…¥
        
        h = self.heads
        
        q = self.to_q(x)
        context = context if context is not None else x
        k = self.to_k(context)
        v = self.to_v(context)
        
        # é‡å¡‘ä¸ºå¤šå¤´
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))
        
        # æ³¨æ„åŠ›è®¡ç®—
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = torch.softmax(dots, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        
        return self.to_out(out)
```

<details>
<summary>**ç»ƒä¹  10.4ï¼šç†è§£Stable Diffusionçš„è®¾è®¡é€‰æ‹©**</summary>

æ·±å…¥åˆ†æSDçš„æ¶æ„å†³ç­–ã€‚

1. **åˆ†è¾¨ç‡å®éªŒ**ï¼š
   - ä¿®æ”¹VAEä¸‹é‡‡æ ·å› å­ï¼ˆ4x, 8x, 16xï¼‰
   - æµ‹é‡å¯¹ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦çš„å½±å“
   - æ‰¾å‡ºæœ€ä¼˜çš„è´¨é‡-æ•ˆç‡å¹³è¡¡ç‚¹

2. **æ³¨æ„åŠ›åˆ†æ**ï¼š
   - å¯è§†åŒ–ä¸åŒå±‚çš„äº¤å‰æ³¨æ„åŠ›å›¾
   - åˆ†æå“ªäº›è¯å¯¹åº”å“ªäº›å›¾åƒåŒºåŸŸ
   - ç ”ç©¶æ³¨æ„åŠ›å¤´çš„ä¸“é—¨åŒ–

3. **æ–‡æœ¬ç¼–ç å™¨æ¯”è¾ƒ**ï¼š
   - æ¯”è¾ƒCLIP vs BERT vs T5
   - æµ‹è¯•ä¸åŒçš„poolingç­–ç•¥
   - è¯„ä¼°å¯¹æç¤ºéµå¾ªçš„å½±å“

4. **æ¶æ„æ¶ˆè**ï¼š
   - ç§»é™¤æŸäº›æ³¨æ„åŠ›å±‚
   - æ”¹å˜é€šé“å€å¢å› å­
   - åˆ†æå„ç»„ä»¶çš„è´¡çŒ®

</details>

### 10.4.6 æ¡ä»¶æœºåˆ¶çš„å®ç°ç»†èŠ‚

Stable Diffusionæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼š

**1. æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰**ï¼š
```python
def sample_with_cfg(model, z_t, t, text_emb, uncond_emb, cfg_scale=7.5):
    # åŒæ—¶é¢„æµ‹æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°
    z_combined = torch.cat([z_t, z_t])
    t_combined = torch.cat([t, t])
    c_combined = torch.cat([uncond_emb, text_emb])
    
    noise_pred = model(z_combined, t_combined, c_combined)
    noise_uncond, noise_cond = noise_pred.chunk(2)
    
    # åº”ç”¨CFG
    noise_pred = noise_uncond + cfg_scale * (noise_cond - noise_uncond)
    
    return noise_pred
```

**2. è´Ÿé¢æç¤º**ï¼š
```python
def encode_prompts(text_encoder, prompt, negative_prompt=""):
    # ç¼–ç æ­£é¢å’Œè´Ÿé¢æç¤º
    text_emb = text_encoder.encode(prompt)
    uncond_emb = text_encoder.encode(negative_prompt)
    
    return text_emb, uncond_emb
```

**3. å›¾åƒæ¡ä»¶ï¼ˆimg2imgï¼‰**ï¼š
```python
def img2img_encode(vae, image, strength=0.75, steps=50):
    # ç¼–ç å›¾åƒåˆ°æ½œåœ¨ç©ºé—´
    z_0 = vae.encode(image)
    
    # ç¡®å®šèµ·å§‹æ—¶é—´æ­¥
    start_step = int(steps * (1 - strength))
    
    # æ·»åŠ é€‚é‡å™ªå£°
    noise = torch.randn_like(z_0)
    z_t = scheduler.add_noise(z_0, noise, timesteps[start_step])
    
    return z_t, start_step
```

### 10.4.7 æ¨ç†ä¼˜åŒ–æŠ€æœ¯

**1. åŠç²¾åº¦æ¨ç†**ï¼š
```python
# è½¬æ¢æ¨¡å‹åˆ°fp16
model = model.half()
vae = vae.half()

# å…³é”®å±‚ä¿æŒfp32
model.conv_in = model.conv_in.float()
model.conv_out = model.conv_out.float()
```

**2. æ³¨æ„åŠ›ä¼˜åŒ–**ï¼š
```python
# ä½¿ç”¨xFormersæˆ–Flash Attention
import xformers.ops

def efficient_attention(q, k, v):
    # ä½¿ç”¨memory-efficient attention
    return xformers.ops.memory_efficient_attention(q, k, v)
```

**3. æ‰¹å¤„ç†ä¼˜åŒ–**ï¼š
```python
def batch_denoise(model, z_batch, t, c_batch):
    # åŠ¨æ€æ‰¹å¤§å°é¿å…OOM
    max_batch = estimate_max_batch_size(z_batch.shape)
    
    results = []
    for i in range(0, len(z_batch), max_batch):
        batch = z_batch[i:i+max_batch]
        c = c_batch[i:i+max_batch]
        results.append(model(batch, t, c))
    
    return torch.cat(results)
```

ğŸ’¡ **æ€§èƒ½æç¤ºï¼šVAEè§£ç ç“¶é¢ˆ**  
åœ¨æ‰¹é‡ç”Ÿæˆæ—¶ï¼ŒVAEè§£ç å¾€å¾€æˆä¸ºç“¶é¢ˆã€‚å¯ä»¥å…ˆç”Ÿæˆæ‰€æœ‰æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åæ‰¹é‡è§£ç ï¼Œæˆ–ä½¿ç”¨æ›´è½»é‡çš„è§£ç å™¨ã€‚

### 10.4.8 æ¨¡å‹å˜ä½“ä¸æ”¹è¿›

**Stable Diffusionæ¼”è¿›**ï¼š

| ç‰ˆæœ¬ | åˆ†è¾¨ç‡ | æ”¹è¿› | å‚æ•°é‡ |
|------|--------|------|---------|
| SD 1.4 | 512Ã—512 | åŸºç¡€ç‰ˆæœ¬ | 860M |
| SD 1.5 | 512Ã—512 | æ›´å¥½çš„è®­ç»ƒæ•°æ® | 860M |
| SD 2.0 | 768Ã—768 | æ–°çš„CLIPç¼–ç å™¨ | 865M |
| SD 2.1 | 768Ã—768 | å‡å°‘NSFWè¿‡æ»¤ | 865M |
| SDXL | 1024Ã—1024 | çº§è”U-Netæ¶æ„ | 3.5B |

**SDXLçš„åˆ›æ–°**ï¼š
```python
class SDXLUNet(nn.Module):
    def __init__(self):
        super().__init__()
        # åŸºç¡€U-Netï¼šç”Ÿæˆæ½œåœ¨è¡¨ç¤º
        self.base_unet = UNet(
            in_channels=4,
            model_channels=320,
            channel_mult=[1, 2, 4],
            use_fp16=True
        )
        
        # ç²¾ç‚¼U-Netï¼šæå‡ç»†èŠ‚
        self.refiner_unet = UNet(
            in_channels=4,
            model_channels=384,
            channel_mult=[1, 2, 4, 4],
            use_fp16=True
        )
        
        # æ¡ä»¶å¢å¼º
        self.add_time_condition = True
        self.add_crop_condition = True
        self.add_size_condition = True
```

ğŸŒŸ **æœªæ¥æ–¹å‘ï¼šæ¨¡å—åŒ–è®¾è®¡**  
æœªæ¥çš„æ¶æ„å¯èƒ½é‡‡ç”¨æ›´æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚ç»„åˆä¸åŒçš„ç¼–ç å™¨ã€å»å™ªå™¨å’Œè§£ç å™¨ã€‚è¿™éœ€è¦æ ‡å‡†åŒ–çš„æ¥å£å’Œè®­ç»ƒåè®®ã€‚

### 10.4.9 è®­ç»ƒç»†èŠ‚ä¸æ•°æ®å¤„ç†

**è®­ç»ƒé…ç½®**ï¼š
```python
training_config = {
    'base_learning_rate': 1e-4,
    'batch_size': 2048,  # ç´¯ç§¯æ¢¯åº¦
    'num_epochs': 5,
    'warmup_steps': 10000,
    'use_ema': True,
    'ema_decay': 0.9999,
    'gradient_clip': 1.0,
    'weight_decay': 0.01,
}

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.CenterCrop(512),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5]),  # [-1, 1]
])
```

**è®­ç»ƒç­–ç•¥**ï¼š
1. **å¤šå°ºåº¦è®­ç»ƒ**ï¼šéšæœºè£å‰ªä¸åŒå°ºå¯¸
2. **æ¡ä»¶dropout**ï¼š10%æ¦‚ç‡ä¸¢å¼ƒæ–‡æœ¬æ¡ä»¶
3. **å™ªå£°åç§»**ï¼šå¾®è°ƒå™ªå£°è°ƒåº¦æ”¹å–„æš—éƒ¨ç»†èŠ‚
4. **æ¸è¿›å¼è®­ç»ƒ**ï¼šå…ˆè®­ç»ƒä½åˆ†è¾¨ç‡ï¼Œå†å¾®è°ƒé«˜åˆ†è¾¨ç‡

### 10.4.10 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**1. ç”Ÿæˆè´¨é‡é—®é¢˜**ï¼š
- æ¨¡ç³Šï¼šå¢åŠ CFG scaleæˆ–ä½¿ç”¨æ›´å¤šæ­¥æ•°
- ä¼ªå½±ï¼šæ£€æŸ¥VAEæƒé‡ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨fp32
- é¢œè‰²åç§»ï¼šè°ƒæ•´å™ªå£°åç§»å‚æ•°

**2. æç¤ºéµå¾ªé—®é¢˜**ï¼š
- ä½¿ç”¨æç¤ºæƒé‡ï¼š`(word:1.3)` å¢å¼ºï¼Œ`[word]` å‡å¼±
- è´Ÿé¢æç¤ºï¼šæ˜ç¡®æ’é™¤ä¸æƒ³è¦çš„å…ƒç´ 
- æç¤ºå·¥ç¨‹ï¼šä½¿ç”¨æ›´å…·ä½“çš„æè¿°

**3. å†…å­˜ä¼˜åŒ–**ï¼š
```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.enable_gradient_checkpointing()

# ä½¿ç”¨CPU offload
from accelerate import cpu_offload

model = cpu_offload(model, device='cuda', offload_buffers=True)
```

ğŸ”§ **è°ƒè¯•æŠ€å·§ï¼šé€æ­¥éªŒè¯**  
é‡åˆ°é—®é¢˜æ—¶ï¼Œé€ä¸ªç»„ä»¶éªŒè¯ï¼š(1)VAEé‡å»ºè´¨é‡ (2)æ— æ¡ä»¶ç”Ÿæˆ (3)æ–‡æœ¬æ¡ä»¶å“åº” (4)CFGæ•ˆæœã€‚è¿™æœ‰åŠ©äºå®šä½é—®é¢˜æ ¹æºã€‚

## 10.5 å®è·µè€ƒè™‘ä¸æ‰©å±•

### 10.5.1 ä¸åŒåˆ†è¾¨ç‡çš„å¤„ç†

LDMéœ€è¦çµæ´»å¤„ç†å„ç§åˆ†è¾¨ç‡çš„å›¾åƒï¼š

**1. å¤šåˆ†è¾¨ç‡è®­ç»ƒ**ï¼š
```python
class MultiResolutionDataset(Dataset):
    def __init__(self, base_size=512, sizes=[256, 512, 768, 1024]):
        self.sizes = sizes
        self.base_size = base_size
        
    def __getitem__(self, idx):
        img = self.load_image(idx)
        
        # éšæœºé€‰æ‹©ç›®æ ‡å°ºå¯¸
        target_size = random.choice(self.sizes)
        
        # æ™ºèƒ½è£å‰ªå’Œç¼©æ”¾
        if img.width > img.height:
            # æ¨ªå‘å›¾åƒ
            scale = target_size / img.height
            new_width = int(img.width * scale)
            img = img.resize((new_width, target_size))
            # ä¸­å¿ƒè£å‰ªåˆ°æ­£æ–¹å½¢
            left = (new_width - target_size) // 2
            img = img.crop((left, 0, left + target_size, target_size))
        else:
            # çºµå‘æˆ–æ­£æ–¹å½¢å›¾åƒ
            scale = target_size / img.width
            new_height = int(img.height * scale)
            img = img.resize((target_size, new_height))
            # ä¸­å¿ƒè£å‰ª
            top = (new_height - target_size) // 2
            img = img.crop((0, top, target_size, top + target_size))
            
        return self.transform(img)
```

**2. åˆ†è¾¨ç‡è‡ªé€‚åº”æ¨ç†**ï¼š
```python
class AdaptiveInference:
    def __init__(self, model, vae):
        self.model = model
        self.vae = vae
        self.patch_size = 64  # æ½œåœ¨ç©ºé—´patchå¤§å°
        
    def generate_high_res(self, prompt, height, width):
        # è®¡ç®—éœ€è¦çš„æ½œåœ¨ç©ºé—´å¤§å°
        latent_h = height // 8
        latent_w = width // 8
        
        if latent_h * latent_w > self.patch_size ** 2:
            # ä½¿ç”¨åˆ†å—ç”Ÿæˆ
            return self.tiled_generation(prompt, latent_h, latent_w)
        else:
            # ç›´æ¥ç”Ÿæˆ
            return self.direct_generation(prompt, latent_h, latent_w)
    
    def tiled_generation(self, prompt, h, w):
        """åˆ†å—ç”Ÿæˆå¤§å›¾åƒ"""
        overlap = 8  # é‡å åŒºåŸŸ
        tiles = []
        
        for i in range(0, h, self.patch_size - overlap):
            for j in range(0, w, self.patch_size - overlap):
                # ç”Ÿæˆæ¯ä¸ªå—
                tile = self.generate_tile(prompt, i, j)
                tiles.append((i, j, tile))
        
        # æ··åˆæ‹¼æ¥
        return self.blend_tiles(tiles, h, w)
```

ğŸ’¡ **å®è·µæŠ€å·§ï¼šå®½é«˜æ¯”ä¿æŒ**  
è®­ç»ƒæ—¶è®°å½•å›¾åƒçš„åŸå§‹å®½é«˜æ¯”ï¼Œæ¨ç†æ—¶å¯ä»¥ç”Ÿæˆç›¸åŒæ¯”ä¾‹çš„å›¾åƒï¼Œé¿å…å˜å½¢ã€‚

### 10.5.2 å¾®è°ƒä¸é€‚é…

**1. LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒ**ï¼š
```python
class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # ä½ç§©çŸ©é˜µ
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank
        
    def forward(self, x, orig_weight):
        # åŸå§‹çº¿æ€§å˜æ¢
        out = F.linear(x, orig_weight)
        
        # æ·»åŠ LoRA
        lora_out = F.linear(F.linear(x, self.lora_A), self.lora_B)
        
        return out + lora_out * self.scaling
```

**2. Textual Inversion**ï¼š
```python
class TextualInversion:
    def __init__(self, text_encoder, token_dim=768):
        self.text_encoder = text_encoder
        self.token_dim = token_dim
        
        # å­¦ä¹ çš„tokenåµŒå…¥
        self.learned_embeds = nn.ParameterDict()
        
    def add_concept(self, concept_name, init_text="object"):
        """æ·»åŠ æ–°æ¦‚å¿µ"""
        # è·å–åˆå§‹åŒ–åµŒå…¥
        init_ids = self.text_encoder.tokenize(init_text)
        init_embed = self.text_encoder.get_embeddings(init_ids)
        
        # åˆ›å»ºå¯å­¦ä¹ å‚æ•°
        self.learned_embeds[concept_name] = nn.Parameter(
            init_embed.clone().detach()
        )
        
    def forward(self, text):
        # æ›¿æ¢ç‰¹æ®Štokenä¸ºå­¦ä¹ çš„åµŒå…¥
        for concept, embed in self.learned_embeds.items():
            if f"<{concept}>" in text:
                text = text.replace(f"<{concept}>", "")
                # æ³¨å…¥å­¦ä¹ çš„åµŒå…¥
                return self.inject_embedding(text, embed)
```

**3. DreamBoothå¾®è°ƒ**ï¼š
```python
def dreambooth_loss(model, images, prompts, prior_preservation=True):
    """DreamBoothè®­ç»ƒæŸå¤±"""
    # ä¸»è¦æŸå¤±ï¼šé‡å»ºç‰¹å®šå®ä¾‹
    instance_loss = diffusion_loss(model, images, prompts)
    
    if prior_preservation:
        # å…ˆéªŒä¿æŒæŸå¤±ï¼šé˜²æ­¢è¯­è¨€æ¼‚ç§»
        class_images = generate_class_images(prompts)
        prior_loss = diffusion_loss(model, class_images, prompts)
        
        total_loss = instance_loss + 0.5 * prior_loss
    else:
        total_loss = instance_loss
        
    return total_loss
```

ğŸ”¬ **ç ”ç©¶æ–¹å‘ï¼šé«˜æ•ˆå¾®è°ƒæ–¹æ³•**  
å¦‚ä½•ç”¨æœ€å°‘çš„å‚æ•°å’Œæ•°æ®å®ç°æœ‰æ•ˆçš„æ¨¡å‹é€‚é…ï¼Ÿè¿™æ¶‰åŠåˆ°å…ƒå­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒçš„å‰æ²¿ç ”ç©¶ã€‚

### 10.5.3 æ¨¡å‹å‹ç¼©ä¸éƒ¨ç½²

**1. é‡åŒ–æŠ€æœ¯**ï¼š
```python
class QuantizedLDM:
    def __init__(self, model, bits=8):
        self.model = model
        self.bits = bits
        
    def quantize_model(self):
        """åŠ¨æ€é‡åŒ–"""
        # INT8é‡åŒ–
        self.model = torch.quantization.quantize_dynamic(
            self.model,
            {nn.Linear, nn.Conv2d},
            dtype=torch.qint8
        )
        
    def calibrate_quantization(self, calibration_data):
        """é™æ€é‡åŒ–æ ¡å‡†"""
        backend = "fbgemm"  # x86 CPU
        self.model.qconfig = torch.quantization.get_default_qconfig(backend)
        
        # å‡†å¤‡é‡åŒ–
        torch.quantization.prepare(self.model, inplace=True)
        
        # æ ¡å‡†
        with torch.no_grad():
            for batch in calibration_data:
                self.model(batch)
        
        # è½¬æ¢
        torch.quantization.convert(self.model, inplace=True)
```

**2. æ¨¡å‹å‰ªæ**ï¼š
```python
def prune_ldm(model, amount=0.3):
    """ç»“æ„åŒ–å‰ªæ"""
    import torch.nn.utils.prune as prune
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            # L1ç»“æ„åŒ–å‰ªæ
            prune.ln_structured(
                module, 
                name='weight',
                amount=amount,
                n=1,
                dim=0  # è¾“å‡ºé€šé“
            )
        elif isinstance(module, nn.Linear):
            # éç»“æ„åŒ–å‰ªæ
            prune.l1_unstructured(
                module,
                name='weight',
                amount=amount
            )
    
    # ç§»é™¤å‰ªæå‚æ•°åŒ–
    for name, module in model.named_modules():
        if hasattr(module, 'weight_mask'):
            prune.remove(module, 'weight')
```

**3. ONNXå¯¼å‡ºä¸ä¼˜åŒ–**ï¼š
```python
def export_to_onnx(model, dummy_input, output_path):
    """å¯¼å‡ºæ¨¡å‹åˆ°ONNXæ ¼å¼"""
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        input_names=['latent', 'timestep', 'condition'],
        output_names=['noise_pred'],
        dynamic_axes={
            'latent': {0: 'batch', 2: 'height', 3: 'width'},
            'condition': {0: 'batch', 1: 'seq_len'}
        },
        opset_version=14,
        do_constant_folding=True
    )
    
    # ä¼˜åŒ–ONNXæ¨¡å‹
    import onnx
    from onnxruntime.transformers import optimizer
    
    model_onnx = onnx.load(output_path)
    optimized_model = optimizer.optimize_model(
        model_onnx,
        model_type='bert',  # ä½¿ç”¨BERTä¼˜åŒ–å™¨å¤„ç†æ³¨æ„åŠ›
        num_heads=8,
        hidden_size=768
    )
    
    onnx.save(optimized_model, output_path.replace('.onnx', '_opt.onnx'))
```

### 10.5.4 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

**1. æ‰¹é‡å¤„ç†ä¼˜åŒ–**ï¼š
```python
class BatchOptimizer:
    def __init__(self, model, max_batch_size=8):
        self.model = model
        self.max_batch_size = max_batch_size
        
    def adaptive_batch_size(self, resolution):
        """æ ¹æ®åˆ†è¾¨ç‡è‡ªé€‚åº”è°ƒæ•´æ‰¹å¤§å°"""
        base_pixels = 512 * 512
        current_pixels = resolution[0] * resolution[1]
        
        # æŒ‰åƒç´ æ•°åæ¯”ä¾‹è°ƒæ•´
        adapted_batch = int(self.max_batch_size * base_pixels / current_pixels)
        
        return max(1, adapted_batch)
    
    def process_batch_with_gradient_checkpointing(self, batch):
        """ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨"""
        def create_custom_forward(module):
            def custom_forward(*inputs):
                return module(*inputs)
            return custom_forward
        
        # å¯¹U-Netçš„æ¯ä¸ªå—ä½¿ç”¨æ£€æŸ¥ç‚¹
        for block in self.model.unet_blocks:
            block = torch.utils.checkpoint.checkpoint(
                create_custom_forward(block),
                batch,
                use_reentrant=False
            )
```

**2. ç¼“å­˜ä¼˜åŒ–**ï¼š
```python
class CachedLDM:
    def __init__(self, model):
        self.model = model
        self.cache = {}
        
    def encode_with_cache(self, text, cache_key=None):
        """ç¼“å­˜æ–‡æœ¬ç¼–ç ç»“æœ"""
        if cache_key and cache_key in self.cache:
            return self.cache[cache_key]
        
        encoding = self.model.encode_text(text)
        
        if cache_key:
            self.cache[cache_key] = encoding
            
        return encoding
    
    def clear_cache(self, max_size=100):
        """LRUç¼“å­˜æ¸…ç†"""
        if len(self.cache) > max_size:
            # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„é¡¹
            items = sorted(self.cache.items(), 
                         key=lambda x: x[1].last_access, 
                         reverse=True)
            self.cache = dict(items[:max_size])
```

<details>
<summary>**ç»¼åˆç»ƒä¹ ï¼šæ„å»ºç”Ÿäº§çº§LDMç³»ç»Ÿ**</summary>

è®¾è®¡å¹¶å®ç°ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„LDMç³»ç»Ÿã€‚

1. **ç³»ç»Ÿæ¶æ„è®¾è®¡**ï¼š
   - è®¾è®¡å¾®æœåŠ¡æ¶æ„
   - å®ç°è¯·æ±‚é˜Ÿåˆ—å’Œè´Ÿè½½å‡è¡¡
   - æ·»åŠ ç›‘æ§å’Œæ—¥å¿—
   - å¤„ç†æ•…éšœæ¢å¤

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å®ç°å¤šGPUæ¨ç†
   - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
   - æ·»åŠ ç»“æœç¼“å­˜
   - æ”¯æŒæµå¼ç”Ÿæˆ

3. **åŠŸèƒ½æ‰©å±•**ï¼š
   - æ”¯æŒå¤šç§é‡‡æ ·å™¨
   - å®ç°å›¾åƒç¼–è¾‘åŠŸèƒ½
   - æ·»åŠ å®‰å…¨è¿‡æ»¤
   - æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹

4. **éƒ¨ç½²æ–¹æ¡ˆ**ï¼š
   - å®¹å™¨åŒ–ï¼ˆDockerï¼‰
   - Kubernetesç¼–æ’
   - APIç½‘å…³è®¾è®¡
   - CDNé›†æˆ

</details>

### 10.5.5 æœªæ¥å‘å±•æ–¹å‘

**1. æ¶æ„åˆ›æ–°**ï¼š
- **ç¨€ç–æ³¨æ„åŠ›**ï¼šå‡å°‘è®¡ç®—å¤æ‚åº¦
- **åŠ¨æ€åˆ†è¾¨ç‡**ï¼šè‡ªé€‚åº”å¤„ç†ä¸åŒå°ºå¯¸
- **ç¥ç»æ¶æ„æœç´¢**ï¼šè‡ªåŠ¨ä¼˜åŒ–ç»“æ„

**2. è®­ç»ƒæ–¹æ³•æ”¹è¿›**ï¼š
- **è‡ªç›‘ç£é¢„è®­ç»ƒ**ï¼šåˆ©ç”¨æ— æ ‡æ³¨æ•°æ®
- **å¤šæ¨¡æ€è”åˆè®­ç»ƒ**ï¼šå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç»Ÿä¸€
- **è¿ç»­å­¦ä¹ **ï¼šä¸æ–­é€‚åº”æ–°æ•°æ®

**3. åº”ç”¨æ‰©å±•**ï¼š
- **3Dç”Ÿæˆ**ï¼šä»2Dæ‰©å±•åˆ°3D
- **è§†é¢‘ç”Ÿæˆ**ï¼šæ—¶åºä¸€è‡´æ€§
- **äº¤äº’å¼ç¼–è¾‘**ï¼šå®æ—¶å“åº”ç”¨æˆ·è¾“å…¥

**4. æ•ˆç‡æå‡**ï¼š
```python
# æœªæ¥å¯èƒ½çš„ä¼˜åŒ–æ–¹å‘ç¤ºä¾‹
class FutureLDM:
    def __init__(self):
        # 1. åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›
        self.sparse_attention = DynamicSparseAttention()
        
        # 2. ç¥ç»ODEæ±‚è§£å™¨
        self.neural_ode_solver = NeuralODESolver()
        
        # 3. å¯å¾®åˆ†é‡åŒ–
        self.differentiable_quantization = LearnedQuantization()
        
        # 4. è‡ªé€‚åº”è®¡ç®—
        self.adaptive_compute = EarlyExitMechanism()
```

ğŸŒŸ **å¼€æ”¾æŒ‘æˆ˜ï¼šä¸‹ä¸€ä»£LDM**  
å¦‚ä½•è®¾è®¡èƒ½å¤Ÿå¤„ç†ä»»æ„æ¨¡æ€ã€ä»»æ„åˆ†è¾¨ç‡ã€å®æ—¶äº¤äº’çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Ÿè¿™éœ€è¦ç®—æ³•ã€æ¶æ„å’Œç¡¬ä»¶çš„ååŒåˆ›æ–°ã€‚

### 10.5.6 å®è·µå»ºè®®æ€»ç»“

1. **å¼€å§‹åŸå‹**ï¼š
   - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¿«é€ŸéªŒè¯æƒ³æ³•
   - ä»å°æ•°æ®é›†å’Œä½åˆ†è¾¨ç‡å¼€å§‹
   - é€æ­¥å¢åŠ å¤æ‚åº¦

2. **ä¼˜åŒ–ç­–ç•¥**ï¼š
   - å…ˆä¼˜åŒ–ç®—æ³•ï¼Œå†ä¼˜åŒ–å®ç°
   - ä½¿ç”¨profileræ‰¾å‡ºç“¶é¢ˆ
   - å¹³è¡¡è´¨é‡ã€é€Ÿåº¦å’Œå†…å­˜

3. **éƒ¨ç½²è€ƒè™‘**ï¼š
   - é€‰æ‹©åˆé€‚çš„é‡åŒ–ç­–ç•¥
   - å®ç°é²æ£’çš„é”™è¯¯å¤„ç†
   - è€ƒè™‘è¾¹ç¼˜è®¾å¤‡é™åˆ¶

4. **æŒç»­æ”¹è¿›**ï¼š
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - A/Bæµ‹è¯•ä¸åŒç‰ˆæœ¬
   - è·Ÿè¸ªæœ€æ–°ç ”ç©¶è¿›å±•

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨å·²ç»æŒæ¡äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒåŸç†å’Œå®è·µæŠ€å·§ã€‚LDMé€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ‰©æ•£ï¼Œå®ç°äº†æ•ˆç‡å’Œè´¨é‡çš„ä¼˜ç§€å¹³è¡¡ï¼Œæˆä¸ºå½“å‰æœ€æµè¡Œçš„ç”Ÿæˆæ¨¡å‹æ¶æ„ä¹‹ä¸€ã€‚ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚

[â† è¿”å›ç›®å½•](index.md) | ç¬¬10ç«  / å…±14ç«  | [ä¸‹ä¸€ç«  â†’](chapter11.md)