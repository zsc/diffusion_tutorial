[â† ä¸Šä¸€ç« ](chapter2.md)
 ç¬¬3ç«  / å…±14ç« 
 [ä¸‹ä¸€ç«  â†’](chapter4.md)



# ç¬¬3ç« ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM)



 2020å¹´ï¼ŒHoç­‰äººçš„è®ºæ–‡"Denoising Diffusion Probabilistic Models"è®©æ‰©æ•£æ¨¡å‹çœŸæ­£è¿›å…¥äº†å®ç”¨é˜¶æ®µã€‚DDPMä¸ä»…ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜è¾¾åˆ°äº†ä¸GANç›¸åª²ç¾çš„ç”Ÿæˆè´¨é‡ã€‚æœ¬ç« å°†æ·±å…¥å‰–æDDPMçš„æ•°å­¦åŸç†ã€è®­ç»ƒç®—æ³•å’Œå®ç°ç»†èŠ‚ã€‚é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†æŒæ¡å¦‚ä½•ä»é›¶å®ç°ä¸€ä¸ªå®Œæ•´çš„DDPMï¼Œå¹¶ç†è§£å…¶èƒŒåçš„æ¦‚ç‡è®ºåŸºç¡€ã€‚



## 3.1 DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼šç®€åŒ–ä¸ç»Ÿä¸€



åœ¨DDPMä¹‹å‰ï¼Œæ‰©æ•£æ¨¡å‹è™½ç„¶ç†è®ºä¼˜é›…ï¼Œä½†å®è·µå›°éš¾ã€‚2015å¹´Sohl-Dicksteinç­‰äººçš„å¼€åˆ›æ€§å·¥ä½œéœ€è¦ä¼°è®¡æ•´ä¸ªåå‘è¿‡ç¨‹çš„ç†µï¼Œè®­ç»ƒæå…¶å¤æ‚ã€‚DDPMçš„é©å‘½æ€§è´¡çŒ®åœ¨äºï¼š**å°†å¤æ‚çš„å˜åˆ†æ¨æ–­ç®€åŒ–ä¸ºç®€å•çš„å»å™ªä»»åŠ¡**ã€‚



> **å®šä¹‰**
> DDPMçš„ä¸‰ä¸ªå…³é”®ç®€åŒ–



 - **å›ºå®šæ–¹å·®è°ƒåº¦**ï¼šå‰å‘è¿‡ç¨‹ä½¿ç”¨é¢„å®šä¹‰çš„ $\beta_t$ åºåˆ—ï¼Œæ— éœ€å­¦ä¹ 
 - **ç®€åŒ–åå‘è¿‡ç¨‹**ï¼šå‡è®¾åå‘è¿‡ç¨‹ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œåªéœ€å­¦ä¹ å‡å€¼ï¼ˆå®é™…ä¸Šæ˜¯å­¦ä¹ å™ªå£°ï¼‰
 - **é‡å‚æ•°åŒ–ç›®æ ‡**ï¼šå°†é¢„æµ‹å‡å€¼è½¬æ¢ä¸ºé¢„æµ‹å™ªå£°ï¼Œå¤§å¹…æå‡è®­ç»ƒç¨³å®šæ€§





### 3.1.1 ä»å¤æ‚åˆ°ç®€å•ï¼šDDPMçš„æ´å¯Ÿ



è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç±»æ¯”æ¥ç†è§£DDPMçš„æ ¸å¿ƒæ€æƒ³ï¼š




#### å¢¨æ°´æ‰©æ•£çš„ç±»æ¯”


æƒ³è±¡ä¸€æ»´å¢¨æ°´åœ¨æ°´ä¸­æ‰©æ•£ï¼š



 - **å‰å‘è¿‡ç¨‹**ï¼šå¢¨æ°´é€æ¸æ‰©æ•£ï¼Œæœ€ç»ˆå‡åŒ€åˆ†å¸ƒï¼ˆç‰©ç†è¿‡ç¨‹ï¼Œç¡®å®šçš„ï¼‰
 - **åå‘è¿‡ç¨‹**ï¼šå¦‚ä½•è®©æ‰©æ•£çš„å¢¨æ°´é‡æ–°èšé›†ï¼Ÿï¼ˆéœ€è¦å­¦ä¹ çš„ï¼‰



DDPMçš„å…³é”®æ´å¯Ÿï¼š**åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“"å¢¨æ°´åº”è¯¥å‘å“ªä¸ªæ–¹å‘èšé›†"**ï¼Œè€Œè¿™ä¸ªæ–¹å‘æ°å¥½ä¸æ·»åŠ çš„å™ªå£°æ–¹å‘ç›¸åï¼




### 3.1.2 æ•°å­¦æ¡†æ¶æ¦‚è§ˆ



DDPMå®šä¹‰äº†ä¸¤ä¸ªè¿‡ç¨‹ï¼š




 **å‰å‘è¿‡ç¨‹ï¼ˆå›ºå®šï¼‰**ï¼š

 $q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$


 **åå‘è¿‡ç¨‹ï¼ˆå­¦ä¹ ï¼‰**ï¼š

 $p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2\mathbf{I})$




å…³é”®åˆ›æ–°åœ¨äºå¦‚ä½•å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ï¼š




# æ—©æœŸæ–¹æ³•ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆä¸ç¨³å®šï¼‰
mean = model(x_t, t)

# DDPMåˆ›æ–°ï¼šé¢„æµ‹å™ªå£°ï¼ˆç¨³å®šä¸”æœ‰æ•ˆï¼‰
noise_pred = model(x_t, t)
mean = (x_t - beta_t / sqrt(1 - alpha_bar_t) * noise_pred) / sqrt(alpha_t)



### 3.1.3 ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿ


 è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„æ”¹å˜å¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼š



> **å®šä¹‰**
> é¢„æµ‹å™ªå£°çš„ä¼˜åŠ¿

 
 
 æ–¹é¢
 é¢„æµ‹å‡å€¼
 é¢„æµ‹å™ªå£°
 
 
 è¾“å‡ºèŒƒå›´
 éœ€è¦åŒ¹é…æ•°æ®åˆ†å¸ƒ
 æ ‡å‡†é«˜æ–¯ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
 
 
 è®­ç»ƒä¿¡å·
 éštå˜åŒ–å‰§çƒˆ
 å„æ—¶é—´æ­¥ç›¸å¯¹ä¸€è‡´
 
 
 æ¢¯åº¦æµ
 å¯èƒ½æ¢¯åº¦æ¶ˆå¤±
 æ¢¯åº¦ä¼ æ’­è‰¯å¥½
 
 
 ç‰©ç†æ„ä¹‰
 é¢„æµ‹å»å™ªåçš„å›¾åƒ
 é¢„æµ‹æ·»åŠ çš„å™ªå£°
 
 



### 3.1.4 DDPM vs æ—©æœŸæ‰©æ•£æ¨¡å‹



è®©æˆ‘ä»¬å¯¹æ¯”DDPMä¸2015å¹´çš„åŸå§‹æ‰©æ•£æ¨¡å‹ï¼š




# 2015å¹´çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¤æ‚ï¼‰
# éœ€è¦ä¼°è®¡ï¼š
# 1. å‰å‘è¿‡ç¨‹çš„ç†µ
# 2. åå‘è¿‡ç¨‹çš„å®Œæ•´åˆ†å¸ƒ
# 3. å˜åˆ†å‚æ•°çš„ä¼˜åŒ–
# è®­ç»ƒæå…¶ä¸ç¨³å®šï¼Œç”Ÿæˆè´¨é‡å·®

# DDPMï¼ˆ2020å¹´ï¼‰çš„è®­ç»ƒï¼ˆæç®€ï¼‰
for x_0, _ in dataloader:
 t = torch.randint(0, num_timesteps, (batch_size,))
 noise = torch.randn_like(x_0)
 x_t = sqrt_alpha_bar[t] * x_0 + sqrt_one_minus_alpha_bar[t] * noise

 noise_pred = model(x_t, t)
 loss = F.mse_loss(noise_pred, noise)
 loss.backward()


 è¿™ç§ç®€åŒ–ä¸æ˜¯ä»¥ç‰ºç‰²æ€§èƒ½ä¸ºä»£ä»·çš„â€”â€”ç›¸åï¼ŒDDPMé¦–æ¬¡è®©æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸GANç«äº‰ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚




æ€è€ƒé¢˜ 3.1ï¼šç›´è§‰ç†è§£

ä¸ºä»€ä¹ˆåœ¨é«˜å™ªå£°æƒ…å†µä¸‹ï¼ˆå¤§çš„tï¼‰ï¼Œé¢„æµ‹å™ªå£°æ¯”é¢„æµ‹åŸå§‹å›¾åƒæ›´å®¹æ˜“ï¼Ÿæç¤ºï¼šè€ƒè™‘ä¿¡å™ªæ¯”ã€‚

**ç­”æ¡ˆï¼š**


å½“tå¾ˆå¤§æ—¶ï¼Œ$\mathbf{x}_t \approx \mathcal{N}(0, \mathbf{I})$ï¼Œå‡ ä¹æ˜¯çº¯å™ªå£°ã€‚æ­¤æ—¶ï¼š



 - åŸå§‹å›¾åƒ $\mathbf{x}_0$ çš„ä¿¡æ¯å‡ ä¹å®Œå…¨ä¸¢å¤±ï¼Œé¢„æµ‹å®ƒéœ€è¦"å‡­ç©ºæƒ³è±¡"
 - ä½†æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ æ˜¯å·²çŸ¥çš„ï¼Œä¸”å ä¸»å¯¼åœ°ä½
 - ç½‘ç»œåªéœ€è¦è¯†åˆ«å™ªå£°æ¨¡å¼ï¼Œè€Œä¸æ˜¯é‡å»ºå¤æ‚çš„å›¾åƒç»“æ„



ç±»æ¯”ï¼šåœ¨é›ªèŠ±å™ªå£°çš„ç”µè§†å±å¹•ä¸Šï¼Œè¯†åˆ«å™ªå£°æ¨¡å¼æ¯”é‡å»ºåŸå§‹èŠ‚ç›®å®¹æ˜“å¾—å¤šã€‚





## 3.2 å‰å‘è¿‡ç¨‹ï¼šæ•°å­¦æ¨å¯¼ä¸æ€§è´¨



å‰å‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œå®ƒå®šä¹‰äº†å¦‚ä½•å°†æ•°æ®é€æ­¥è½¬æ¢ä¸ºå™ªå£°ã€‚è™½ç„¶è¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä¸éœ€è¦å®é™…æ‰§è¡Œå®Œæ•´çš„é©¬å°”å¯å¤«é“¾ï¼Œä½†ç†è§£å…¶æ•°å­¦æ€§è´¨å¯¹æŒæ¡DDPMè‡³å…³é‡è¦ã€‚



### 3.2.1 é©¬å°”å¯å¤«é“¾çš„æ„å»º



å‰å‘è¿‡ç¨‹å®šä¹‰ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼š




 $$\mathbf{x}_0 \to \mathbf{x}_1 \to \mathbf{x}_2 \to \cdots \to \mathbf{x}_T$$




å…¶ä¸­æ¯ä¸€æ­¥çš„è½¬ç§»æ¦‚ç‡ä¸ºï¼š




 $$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$$




> **å®šä¹‰**
> å…³é”®æ€§è´¨1ï¼šæ–¹å·®è°ƒåº¦çš„çº¦æŸ


ä¸ºä»€ä¹ˆæ˜¯ $\sqrt{1-\beta_t}$ è€Œä¸æ˜¯å…¶ä»–ç³»æ•°ï¼Ÿè¿™æ˜¯ä¸ºäº†ä¿æŒä¿¡å·çš„æœŸæœ›èƒ½é‡ï¼š



 $$\mathbb{E}[\|\mathbf{x}_t\|^2 | \mathbf{x}_{t-1}] = (1-\beta_t)\|\mathbf{x}_{t-1}\|^2 + \beta_t \cdot d$$



å…¶ä¸­ $d$ æ˜¯æ•°æ®ç»´åº¦ã€‚å½“ $\beta_t$ å¾ˆå°æ—¶ï¼Œä¿¡å·èƒ½é‡è¿‘ä¼¼ä¿æŒä¸å˜ã€‚




è®©æˆ‘ä»¬éªŒè¯è¿™ä¸ªæ€§è´¨ï¼š




import torch
import matplotlib.pyplot as plt

# éªŒè¯èƒ½é‡ä¿æŒæ€§è´¨
x_0 = torch.randn(1000, 3, 32, 32) # 1000ä¸ª32x32çš„RGBå›¾åƒ
beta = 0.02 # å…¸å‹çš„betaå€¼

# ä¸€æ­¥å‰å‘è¿‡ç¨‹
noise = torch.randn_like(x_0)
x_1 = torch.sqrt(1 - beta) * x_0 + torch.sqrt(beta) * noise

print(f"åŸå§‹ä¿¡å·èƒ½é‡: {x_0.pow(2).mean():.4f}")
print(f"æ‰©æ•£åä¿¡å·èƒ½é‡: {x_1.pow(2).mean():.4f}")
print(f"ç†è®ºé¢„æœŸ: {(1-beta)*x_0.pow(2).mean() + beta*3*32*32:.4f}")



### 3.2.2 é‡å‚æ•°åŒ–æŠ€å·§


 DDPMçš„ä¸€ä¸ªå…³é”®æŠ€å·§æ˜¯ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥ä» $\mathbf{x}_0$ é‡‡æ ·ä»»æ„æ—¶åˆ»çš„ $\mathbf{x}_t$ï¼Œè€Œä¸éœ€è¦é€æ­¥æ¨¡æ‹Ÿæ•´ä¸ªé©¬å°”å¯å¤«é“¾ã€‚



> **å®šä¹‰**
> å®šç†ï¼šé—­å¼é‡‡æ ·å…¬å¼


å®šä¹‰ $\alpha_t = 1 - \beta_t$ å’Œ $\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s$ï¼Œåˆ™ï¼š



 $$q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$$





**è¯æ˜**ï¼ˆè¿™ä¸ªè¯æ˜å¾ˆé‡è¦ï¼Œå€¼å¾—ä»”ç»†ç†è§£ï¼‰ï¼š





æˆ‘ä»¬ç”¨å½’çº³æ³•è¯æ˜ã€‚


**åŸºç¡€æƒ…å†µ**ï¼ˆ$t=1$ï¼‰ï¼šæ˜¾ç„¶æˆç«‹ï¼Œå› ä¸º $\bar{\alpha}_1 = \alpha_1 = 1 - \beta_1$ã€‚



**å½’çº³æ­¥éª¤**ï¼šå‡è®¾å¯¹ $t-1$ æˆç«‹ï¼Œå³ï¼š

 $$\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}$$


å…¶ä¸­ $\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})$ã€‚æ ¹æ®å‰å‘è¿‡ç¨‹å®šä¹‰ï¼š

 $$\mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$


ä»£å…¥ $\mathbf{x}_{t-1}$ çš„è¡¨è¾¾å¼ï¼š

 $$\mathbf{x}_t = \sqrt{\alpha_t}(\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}}\boldsymbol{\epsilon}_{t-1}) + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$

 $$= \sqrt{\alpha_t\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t$$


æ³¨æ„åˆ° $\alpha_t\bar{\alpha}_{t-1} = \bar{\alpha}_t$ï¼Œä¸”ä¸¤ä¸ªç‹¬ç«‹é«˜æ–¯å™ªå£°çš„çº¿æ€§ç»„åˆä»æ˜¯é«˜æ–¯å™ªå£°ï¼š

 $$\text{Var}[\sqrt{\alpha_t(1-\bar{\alpha}_{t-1})}\boldsymbol{\epsilon}_{t-1} + \sqrt{1-\alpha_t}\boldsymbol{\epsilon}_t] = \alpha_t(1-\bar{\alpha}_{t-1}) + (1-\alpha_t) = 1-\bar{\alpha}_t$$


å› æ­¤ï¼š

 $$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$$


å…¶ä¸­ $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ã€‚è¯æ¯•ã€‚





### 3.2.3 å™ªå£°è°ƒåº¦çš„è®¾è®¡



å™ªå£°è°ƒåº¦ $\{\beta_t\}_{t=1}^T$ çš„é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚DDPMåŸæ–‡ä½¿ç”¨çº¿æ€§è°ƒåº¦ï¼Œä½†åç»­ç ”ç©¶å‘ç°å…¶ä»–è°ƒåº¦å¯èƒ½æ›´ä¼˜ã€‚




import numpy as np
import matplotlib.pyplot as plt

def linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
 """DDPMåŸå§‹çš„çº¿æ€§è°ƒåº¦"""
 return np.linspace(beta_start, beta_end, timesteps)

def cosine_beta_schedule(timesteps, s=0.008):
 """Improved DDPMçš„ä½™å¼¦è°ƒåº¦"""
 steps = timesteps + 1
 t = np.linspace(0, timesteps, steps)
 alphas_cumprod = np.cos(((t / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2
 alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
 betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
 return np.clip(betas, 0.0001, 0.9999)

def quadratic_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
 """äºŒæ¬¡è°ƒåº¦ï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰"""
 t = np.linspace(0, 1, timesteps)
 return beta_start + (beta_end - beta_start) * t ** 2

# å¯è§†åŒ–ä¸åŒè°ƒåº¦
timesteps = 1000
linear_betas = linear_beta_schedule(timesteps)
cosine_betas = cosine_beta_schedule(timesteps)
quadratic_betas = quadratic_beta_schedule(timesteps)

# è®¡ç®—ä¿¡å™ªæ¯”ï¼ˆæ›´ç›´è§‚çš„æŒ‡æ ‡ï¼‰
# æ¯”è¾ƒä¸åŒå™ªå£°è°ƒåº¦
linear_betas = linear_beta_schedule(1000)
cosine_betas = cosine_beta_schedule(1000)
quadratic_betas = quadratic_beta_schedule(1000)

# è®¡ç®—ä¿¡å™ªæ¯”
def compute_snr(betas):
 alphas = 1 - betas
 alphas_cumprod = np.cumprod(alphas)
 return alphas_cumprod / (1 - alphas_cumprod)

# å±•ç¤ºä¸åŒè°ƒåº¦ä¸‹çš„å…³é”®ç»Ÿè®¡æ•°æ®
alphas_cumprod_linear = np.cumprod(1 - linear_betas)
alphas_cumprod_cosine = np.cumprod(1 - cosine_betas)

t_vis = [0, 250, 500, 750, 999]
print("Signal preservation (âˆšá¾±_t) at key timesteps:")
print("Timestep | Linear | Cosine")
for t in t_vis:
 print(f"{t:8d} | {np.sqrt(alphas_cumprod_linear[t]):.4f} | {np.sqrt(alphas_cumprod_cosine[t]):.4f}")

print("\nSNR at key timesteps:")
snr_linear = compute_snr(linear_betas)
snr_cosine = compute_snr(cosine_betas)
for t in t_vis:
 print(f"{t:8d} | {snr_linear[t]:.4f} | {snr_cosine[t]:.4f}")



> **å®šä¹‰**
> è°ƒåº¦ç­–ç•¥å¯¹æ¯”

 
 
 è°ƒåº¦ç±»å‹
 ç‰¹ç‚¹
 ä¼˜åŠ¿
 åŠ£åŠ¿
 
 
 çº¿æ€§ (Linear)
 Î²çº¿æ€§å¢é•¿
 ç®€å•ç›´è§‚
 å‰æœŸç ´åè¿‡å¿«
 
 
 ä½™å¼¦ (Cosine)
 åŸºäºSNRè®¾è®¡
 æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡
 æœ«æœŸå¯èƒ½è¿‡æ…¢
 
 
 äºŒæ¬¡ (Quadratic)
 Î²äºŒæ¬¡å¢é•¿
 å‰æœŸä¿ç•™æ›´å¤šä¿¡æ¯
 åæœŸå¯èƒ½å¤ªæ¿€è¿›
 
 




ç»ƒä¹  3.2ï¼šå®ç°è‡ªå®šä¹‰å™ªå£°è°ƒåº¦
 è®¾è®¡ä¸€ä¸ª"Så½¢"å™ªå£°è°ƒåº¦ï¼Œä½¿å¾—ï¼š



 - å‰æœŸï¼ˆt 800ï¼‰ï¼šå†æ¬¡æ”¾ç¼“ï¼Œç¡®ä¿æ”¶æ•›åˆ°çº¯å™ªå£°



å®ç°è¿™ä¸ªè°ƒåº¦å¹¶ä¸æ ‡å‡†è°ƒåº¦å¯¹æ¯”SNRæ›²çº¿ã€‚

def sigmoid_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
 """Så½¢å™ªå£°è°ƒåº¦"""
 t = np.linspace(-6, 6, timesteps)
 sigmoid = 1 / (1 + np.exp(-t))
 betas = beta_start + (beta_end - beta_start) * sigmoid
 return betas

# ä¹Ÿå¯ä»¥åˆ†æ®µè®¾è®¡
def piecewise_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):
 """åˆ†æ®µå™ªå£°è°ƒåº¦"""
 betas = np.zeros(timesteps)

 # å‰æœŸï¼šç¼“æ…¢å¢é•¿
 t1 = int(0.2 * timesteps)
 betas[:t1] = np.linspace(beta_start, beta_start * 5, t1)

 # ä¸­æœŸï¼šå¿«é€Ÿå¢é•¿
 t2 = int(0.8 * timesteps)
 betas[t1:t2] = np.linspace(beta_start * 5, beta_end * 0.8, t2 - t1)

 # åæœŸï¼šç¼“æ…¢å¢é•¿åˆ°beta_end
 betas[t2:] = np.linspace(beta_end * 0.8, beta_end, timesteps - t2)

 return betas
 **å…³é”®æ´å¯Ÿ**ï¼šå¥½çš„å™ªå£°è°ƒåº¦åº”è¯¥åœ¨ä¿ç•™è¶³å¤Ÿä¿¡æ¯å’Œå……åˆ†æ¢ç´¢å™ªå£°ç©ºé—´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä½™å¼¦è°ƒåº¦ä¹‹æ‰€ä»¥ä¼˜äºçº¿æ€§è°ƒåº¦ï¼Œæ­£æ˜¯å› ä¸ºå®ƒæ›´å¥½åœ°å¹³è¡¡äº†è¿™ä¸¤ä¸ªéœ€æ±‚ã€‚





## 3.3 åå‘è¿‡ç¨‹ï¼šä»å™ªå£°åˆ°å›¾åƒ



åå‘è¿‡ç¨‹æ˜¯æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒâ€”â€”å¦‚ä½•ä»çº¯å™ªå£°é€æ­¥æ¢å¤å‡ºæ¸…æ™°çš„æ•°æ®ã€‚DDPMçš„å…³é”®è´¡çŒ®ä¹‹ä¸€æ˜¯æ¨å¯¼å‡ºäº†åœ¨å·²çŸ¥ $\mathbf{x}_0$ æ—¶çš„åå‘æ¡ä»¶åˆ†å¸ƒçš„é—­å¼è§£ã€‚



### 3.3.1 åå‘æ¡ä»¶æ¦‚ç‡çš„æ¨å¯¼



è¿™æ˜¯DDPMä¸­æœ€é‡è¦çš„æ•°å­¦æ¨å¯¼ä¹‹ä¸€ã€‚æˆ‘ä»¬æƒ³è¦è®¡ç®— $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ã€‚



> **å®šä¹‰**
> å®šç†ï¼šåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒ


ç»™å®š $\mathbf{x}_t$ å’Œ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒä¸ºï¼š



 $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathbf{I})$$



å…¶ä¸­ï¼š



 $$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t$$

 $$\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$





**è¯æ˜**ï¼šä½¿ç”¨è´å¶æ–¯å®šç†ï¼š




 $$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}$$




ç”±äºå‰å‘è¿‡ç¨‹çš„é©¬å°”å¯å¤«æ€§è´¨ï¼Œ$q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t|\mathbf{x}_{t-1})$ã€‚ç°åœ¨æˆ‘ä»¬çŸ¥é“ï¼š




 - $q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$
 - $q(\mathbf{x}_{t-1}|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0, (1-\bar{\alpha}_{t-1})\mathbf{I})$
 - $q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$




å°†ä¸‰ä¸ªé«˜æ–¯åˆ†å¸ƒä»£å…¥è´å¶æ–¯å…¬å¼ï¼Œç»è¿‡ç¹çä½†ç›´æ¥çš„ä»£æ•°è¿ç®—ï¼ˆä¸»è¦æ˜¯é…æ–¹ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ä¸Šè¿°ç»“æœã€‚




#### ğŸ’¡ å…³é”®æ´å¯Ÿ


æ³¨æ„ $\tilde{\boldsymbol{\mu}}_t$ æ˜¯ $\mathbf{x}_0$ å’Œ $\mathbf{x}_t$ çš„**çº¿æ€§ç»„åˆ**ï¼è¿™æ„å‘³ç€ï¼š



 - å¦‚æœæˆ‘ä»¬çŸ¥é“ $\mathbf{x}_0$ï¼Œåå‘è¿‡ç¨‹å°±æ˜¯ç¡®å®šçš„ï¼ˆé™¤äº†å°çš„é«˜æ–¯å™ªå£°ï¼‰
 - å®è·µä¸­æˆ‘ä»¬ä¸çŸ¥é“ $\mathbf{x}_0$ï¼Œæ‰€ä»¥éœ€è¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å®ƒ
 - è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯åœ¨å­¦ä¹ "å»å™ª"





### 3.3.2 å‚æ•°åŒ–é€‰æ‹©ï¼šé¢„æµ‹å™ªå£° vs é¢„æµ‹å‡å€¼



æ—¢ç„¶ $\tilde{\boldsymbol{\mu}}_t$ ä¾èµ–äºæœªçŸ¥çš„ $\mathbf{x}_0$ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼å®ƒã€‚DDPMæä¾›äº†å‡ ç§å‚æ•°åŒ–æ–¹å¼ï¼š




# æ–¹å¼1ï¼šç›´æ¥é¢„æµ‹å‡å€¼ï¼ˆæœ€ç›´æ¥ä½†ä¸ç¨³å®šï¼‰
mu_theta = model(x_t, t)

# æ–¹å¼2ï¼šé¢„æµ‹x_0ï¼ˆéœ€è¦clipåˆ°åˆç†èŒƒå›´ï¼‰
x_0_pred = model(x_t, t)
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred +
 sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé¢„æµ‹å™ªå£°ï¼ˆDDPMçš„é€‰æ‹©ï¼Œæœ€ç¨³å®šï¼‰
epsilon_pred = model(x_t, t)
x_0_pred = (x_t - sqrt_one_minus_alpha_bar_t * epsilon_pred) / sqrt_alpha_bar_t
mu_theta = (sqrt_alpha_bar_prev * beta_t * x_0_pred +
 sqrt_alpha_t * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)


 ä¸ºä»€ä¹ˆé¢„æµ‹å™ªå£°æ›´å¥½ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡é‡å‚æ•°åŒ–æ¥ç†è§£ï¼š





ç”±äº $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºï¼š

 $$\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}}$$


ä»£å…¥ $\tilde{\boldsymbol{\mu}}_t$ çš„è¡¨è¾¾å¼ï¼Œç»è¿‡åŒ–ç®€å¯å¾—ï¼š

 $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$




è¿™ä¸ªè¡¨è¾¾å¼æ­ç¤ºäº†ä¸€ä¸ªä¼˜é›…çš„äº‹å®ï¼š**åå‘è¿‡ç¨‹çš„å‡å€¼åªéœ€è¦çŸ¥é“æ·»åŠ çš„å™ªå£° $\boldsymbol{\epsilon}$ï¼**



> **å®šä¹‰**
> ä¸‰ç§å‚æ•°åŒ–çš„å¯¹æ¯”

 
 
 å‚æ•°åŒ–
 ä¼˜ç‚¹
 ç¼ºç‚¹
 ä½¿ç”¨åœºæ™¯
 
 
 é¢„æµ‹ $\boldsymbol{\mu}_\theta$
 ç›´æ¥ï¼Œæ— éœ€è½¬æ¢
 ä¸åŒtçš„è¾“å‡ºå°ºåº¦å·®å¼‚å¤§
 å‡ ä¹ä¸ç”¨
 
 
 é¢„æµ‹ $\mathbf{x}_0$
 è¯­ä¹‰æ¸…æ™°
 é«˜å™ªå£°æ—¶é¢„æµ‹å›°éš¾
 æŸäº›æ¡ä»¶ç”Ÿæˆä»»åŠ¡
 
 
 é¢„æµ‹ $\boldsymbol{\epsilon}$
 è¾“å‡ºæ ‡å‡†åŒ–ï¼Œè®­ç»ƒç¨³å®š
 é—´æ¥ï¼Œéœ€è¦è½¬æ¢
 æ ‡å‡†é€‰æ‹©
 
 



### 3.3.3 æ–¹å·®çš„å¤„ç†ï¼šå›ºå®š vs å¯å­¦ä¹ 



DDPMçš„å¦ä¸€ä¸ªç®€åŒ–æ˜¯ä½¿ç”¨å›ºå®šçš„æ–¹å·® $\tilde{\beta}_t$ã€‚ä½†è¿™æ˜¯æœ€ä¼˜çš„å—ï¼Ÿ




# DDPMï¼šå›ºå®šæ–¹å·®ï¼ˆä¸¤ç§é€‰æ‹©ï¼‰
# é€‰æ‹©1ï¼šä½¿ç”¨åéªŒæ–¹å·®
variance = (1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t

# é€‰æ‹©2ï¼šä½¿ç”¨Î²_tï¼ˆDDPMè®ºæ–‡çš„é€‰æ‹©ï¼‰
variance = beta_t

# æ”¹è¿›çš„DDPMï¼šå­¦ä¹ æ–¹å·®
# ç½‘ç»œåŒæ—¶é¢„æµ‹å™ªå£°å’Œæ–¹å·®
epsilon_pred, v_pred = model(x_t, t).chunk(2, dim=1)

# å‚æ•°åŒ–æ–¹å·®ï¼ˆåœ¨å¯¹æ•°ç©ºé—´æ’å€¼ï¼‰
min_log = torch.log(beta_t)
max_log = torch.log((1 - alpha_bar_prev) / (1 - alpha_bar_t) * beta_t)
log_variance = v_pred * max_log + (1 - v_pred) * min_log
variance = torch.exp(log_variance)




#### âš ï¸ å®è·µç»éªŒ

 å°½ç®¡å­¦ä¹ æ–¹å·®ç†è®ºä¸Šæ›´ä¼˜ï¼ˆå¯ä»¥è·å¾—æ›´å¥½çš„ä¼¼ç„¶ï¼‰ï¼Œä½†åœ¨å®è·µä¸­ï¼š



 - å›ºå®šæ–¹å·®çš„DDPMå·²ç»èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒ
 - å­¦ä¹ æ–¹å·®å¢åŠ äº†è®­ç»ƒçš„å¤æ‚åº¦
 - å¯¹äºå¤§å¤šæ•°åº”ç”¨ï¼Œå›ºå®šæ–¹å·®æ˜¯è¶³å¤Ÿçš„
 - å¦‚æœè¿½æ±‚æœ€ä¼˜ä¼¼ç„¶ï¼ˆå¦‚å‹ç¼©ä»»åŠ¡ï¼‰ï¼Œæ‰è€ƒè™‘å­¦ä¹ æ–¹å·®






ç»ƒä¹  3.3ï¼šéªŒè¯ä¸åŒå‚æ•°åŒ–çš„ç­‰ä»·æ€§

å®ç°ä¸‰ç§å‚æ•°åŒ–æ–¹å¼ï¼ŒéªŒè¯å®ƒä»¬åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ï¼š



 - ç»™å®šç›¸åŒçš„ $\mathbf{x}_t$ã€$\mathbf{x}_0$ å’Œ $t$
 - è®¡ç®—çœŸå®çš„å™ªå£° $\boldsymbol{\epsilon}$
 - ç”¨ä¸‰ç§æ–¹å¼è®¡ç®— $\tilde{\boldsymbol{\mu}}_t$
 - éªŒè¯ç»“æœç›¸åŒï¼ˆåœ¨æ•°å€¼ç²¾åº¦å†…ï¼‰

import torch

# è®¾ç½®
batch_size = 4
channels = 3
size = 32
t = 500
T = 1000

# åˆå§‹åŒ–
x_0 = torch.randn(batch_size, channels, size, size)
epsilon = torch.randn_like(x_0)

# è®¡ç®—alphaç›¸å…³å€¼
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alphas_bar = torch.cumprod(alphas, dim=0)
alpha_t = alphas[t]
alpha_bar_t = alphas_bar[t]
alpha_bar_prev = alphas_bar[t-1]
beta_t = betas[t]

# å‰å‘è¿‡ç¨‹
x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * epsilon

# æ–¹å¼1ï¼šç›´æ¥è®¡ç®—çœŸå®çš„åéªŒå‡å€¼
mu_true = (torch.sqrt(alpha_bar_prev) * beta_t * x_0 +
 torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼2ï¼šé€šè¿‡é¢„æµ‹x_0
x_0_pred = x_0 # å‡è®¾å®Œç¾é¢„æµ‹
mu_x0 = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_pred +
 torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æ–¹å¼3ï¼šé€šè¿‡é¢„æµ‹å™ªå£°
epsilon_pred = epsilon # å‡è®¾å®Œç¾é¢„æµ‹
x_0_from_eps = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
mu_eps = (torch.sqrt(alpha_bar_prev) * beta_t * x_0_from_eps +
 torch.sqrt(alpha_t) * (1 - alpha_bar_prev) * x_t) / (1 - alpha_bar_t)

# æˆ–è€…ç›´æ¥ç”¨ç®€åŒ–å…¬å¼
mu_eps_direct = (x_t - beta_t / torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_t)

# éªŒè¯
print(f"æ–¹å¼1å’Œæ–¹å¼2çš„å·®å¼‚: {(mu_true - mu_x0).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3çš„å·®å¼‚: {(mu_true - mu_eps).abs().max():.6f}")
print(f"æ–¹å¼1å’Œæ–¹å¼3(ç›´æ¥)çš„å·®å¼‚: {(mu_true - mu_eps_direct).abs().max():.6f}")

# è¾“å‡ºåº”è¯¥éƒ½æ¥è¿‘0ï¼ˆåœ¨æµ®ç‚¹ç²¾åº¦èŒƒå›´å†…ï¼‰
 **å…³é”®æ´å¯Ÿ**ï¼šä¸‰ç§å‚æ•°åŒ–åœ¨æ•°å­¦ä¸Šç­‰ä»·ï¼Œä½†è®­ç»ƒåŠ¨æ€ä¸åŒã€‚é¢„æµ‹å™ªå£°ä¹‹æ‰€ä»¥æ›´ç¨³å®šï¼Œæ˜¯å› ä¸ºå™ªå£° $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ å§‹ç»ˆæ˜¯æ ‡å‡†åŒ–çš„ï¼Œè€Œ $\mathbf{x}_0$ çš„åˆ†å¸ƒå¯èƒ½å¾ˆå¤æ‚ã€‚





## 3.4 è®­ç»ƒç›®æ ‡ï¼šå˜åˆ†ä¸‹ç•Œçš„ç®€åŒ–



DDPMçš„å¦ä¸€ä¸ªé‡è¦è´¡çŒ®æ˜¯å°†å¤æ‚çš„å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰ç®€åŒ–ä¸ºä¸€ä¸ªç®€å•çš„å»å™ªç›®æ ‡ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†æ¨å¯¼è¿™ä¸ªè¿‡ç¨‹ã€‚



### 3.4.1 å®Œæ•´çš„å˜åˆ†ä¸‹ç•Œ



æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ $\log p_\theta(\mathbf{x}_0)$ã€‚ç”±äºç›´æ¥è®¡ç®—å›°éš¾ï¼Œæˆ‘ä»¬ä¼˜åŒ–å…¶å˜åˆ†ä¸‹ç•Œï¼š




 $$\log p_\theta(\mathbf{x}_0) \geq \mathbb{E}_q\left[\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)}\right] = -L_{\text{VLB}}$$




å…¶ä¸­ $L_{\text{VLB}}$ æ˜¯å˜åˆ†ä¸‹ç•ŒæŸå¤±ã€‚ç»è¿‡å±•å¼€ï¼ˆä½¿ç”¨é©¬å°”å¯å¤«æ€§è´¨ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ï¼š




 $$L_{\text{VLB}} = L_T + \sum_{t=2}^{T} L_{t-1} + L_0$$




å…¶ä¸­å„é¡¹å®šä¹‰ä¸ºï¼š



> **å®šä¹‰**
> å˜åˆ†ä¸‹ç•Œçš„ä¸‰ä¸ªç»„æˆéƒ¨åˆ†



 $$L_T = D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0) \| p(\mathbf{x}_T))$$
 $$L_{t-1} = \mathbb{E}_q\left[D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))\right]$$
 $$L_0 = \mathbb{E}_q\left[-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\right]$$




 - $L_T$ï¼šå…ˆéªŒåŒ¹é…é¡¹ï¼Œé€šå¸¸å¾ˆå°å¯ä»¥å¿½ç•¥ï¼ˆå› ä¸º $q(\mathbf{x}_T|\mathbf{x}_0) \approx \mathcal{N}(0, \mathbf{I})$ï¼‰
 - $L_{t-1}$ï¼šå»å™ªåŒ¹é…é¡¹ï¼Œè¿™æ˜¯ä¸»è¦çš„ä¼˜åŒ–ç›®æ ‡
 - $L_0$ï¼šé‡å»ºé¡¹ï¼Œå†³å®šæœ€ç»ˆè¾“å‡ºè´¨é‡





å…³é”®åœ¨äºå¦‚ä½•å¤„ç† $L_{t-1}$ é¡¹ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“ $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$ çš„é—­å¼è§£ï¼ˆè§3.3.1èŠ‚ï¼‰ï¼Œä¸”å‡è®¾ $p_\theta$ ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼ŒKLæ•£åº¦å¯ä»¥ç®€åŒ–ä¸ºï¼š




 $$L_{t-1} = \mathbb{E}_q\left[\frac{1}{2\sigma_t^2}\|\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\|^2\right] + C$$




å…¶ä¸­ $C$ æ˜¯ä¸ $\theta$ æ— å…³çš„å¸¸æ•°ã€‚



### 3.4.2 ç®€åŒ–çš„å»å™ªç›®æ ‡



DDPMçš„å…³é”®æ´å¯Ÿæ˜¯ï¼šé€šè¿‡é€‰æ‹©å™ªå£°é¢„æµ‹å‚æ•°åŒ–ï¼Œå¯ä»¥å°†ä¸Šè¿°ç›®æ ‡è¿›ä¸€æ­¥ç®€åŒ–ã€‚å›å¿†3.3.2èŠ‚çš„ç»“æœï¼š




 $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$$




å¦‚æœæˆ‘ä»¬å‚æ•°åŒ– $\boldsymbol{\mu}_\theta$ ä¸ºï¼š




 $$\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right)$$




é‚£ä¹ˆ $L_{t-1}$ å¯ä»¥ç®€åŒ–ä¸ºï¼š




 $$L_{t-1} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}}\left[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$




å…¶ä¸­ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ã€‚




#### ğŸ¯ DDPMçš„ç®€åŒ–è®­ç»ƒç›®æ ‡


Hoç­‰äººå‘ç°ï¼Œå¿½ç•¥æƒé‡ç³»æ•°å¹¶å¯¹æ‰€æœ‰æ—¶é—´æ­¥æ±‚å’Œï¼Œå¾—åˆ°çš„ç®€åŒ–ç›®æ ‡æ•ˆæœæ›´å¥½ï¼š



 $$L_{\text{simple}} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$$



è¿™å°±æ˜¯è‘—åçš„"ç®€å•æŸå¤±"â€”â€”åªéœ€è¦é¢„æµ‹å™ªå£°ï¼




### 3.4.3 æŸå¤±å‡½æ•°çš„åŠ æƒç­–ç•¥



è™½ç„¶ç®€å•æŸå¤±æ•ˆæœå¾ˆå¥½ï¼Œä½†ä¸åŒæ—¶é—´æ­¥çš„é‡è¦æ€§ç¡®å®ä¸åŒã€‚åç»­ç ”ç©¶æå‡ºäº†å„ç§åŠ æƒç­–ç•¥ï¼š




import torch
import matplotlib.pyplot as plt

# ä¸åŒçš„æŸå¤±åŠ æƒç­–ç•¥
def get_loss_weight(t, strategy='simple', snr_gamma=5.0):
 """
 è®¡ç®—æ—¶é—´æ­¥tçš„æŸå¤±æƒé‡

 ç­–ç•¥:
 - simple: æ‰€æœ‰æ—¶é—´æ­¥æƒé‡ç›¸åŒï¼ˆDDPMåŸå§‹ï¼‰
 - snr: åŸºäºä¿¡å™ªæ¯”çš„åŠ æƒ
 - truncated_snr: æˆªæ–­çš„SNRåŠ æƒï¼ˆé˜²æ­¢æç«¯å€¼ï¼‰
 - importance: åŸºäºé‡è¦æ€§é‡‡æ ·
 """
 if strategy == 'simple':
 return 1.0

 elif strategy == 'snr':
 # æƒé‡ä¸ä¿¡å™ªæ¯”æˆåæ¯”
 snr = alpha_bar[t] / (1 - alpha_bar[t])
 return 1.0 / (1.0 + snr)

 elif strategy == 'truncated_snr':
 # Min-SNR-Î³ åŠ æƒï¼ˆHang et al., 2023ï¼‰
 snr = alpha_bar[t] / (1 - alpha_bar[t])
 return torch.minimum(snr, torch.tensor(snr_gamma)) / snr

 elif strategy == 'importance':
 # åŸºäºL_tç³»æ•°çš„é‡è¦æ€§åŠ æƒ
 return beta[t]**2 / (2 * sigma[t]**2 * alpha[t] * (1 - alpha_bar[t]))

# åˆ†æä¸åŒåŠ æƒç­–ç•¥
T = 1000
t = torch.arange(T)
beta = torch.linspace(0.0001, 0.02, T)
alpha = 1 - beta
alpha_bar = torch.cumprod(alpha, dim=0)
sigma = beta # DDPMçš„é€‰æ‹©

# è®¡ç®—ä¸åŒç­–ç•¥çš„æƒé‡
strategies = ['simple', 'snr', 'truncated_snr', 'importance']
weight_stats = {}

for strategy in strategies:
 weights = torch.tensor([get_loss_weight(i, strategy) for i in range(T)])
 weight_stats[strategy] = {
 'min': weights.min().item(),
 'max': weights.max().item(),
 'mean': weights.mean().item(),
 'std': weights.std().item()
 }

# æ‰“å°æƒé‡ç»Ÿè®¡
print("Loss weight statistics for different strategies:")
for strategy, stats in weight_stats.items():
 print(f"\n{strategy}:")
 print(f" Min: {stats['min']:.6f}")
 print(f" Max: {stats['max']:.6f}")
 print(f" Mean: {stats['mean']:.6f}")
 print(f" Std: {stats['std']:.6f}")



> **å®šä¹‰**
> åŠ æƒç­–ç•¥å¯¹æ¯”

 
 
 ç­–ç•¥
 åŠ¨æœº
 æ•ˆæœ
 è®¡ç®—å¼€é”€
 
 
 ç®€å• (Simple)
 ç®€åŒ–è®­ç»ƒ
 åŸºå‡†ï¼Œæ•ˆæœå·²ç»ä¸é”™
 æœ€ä½
 
 
 SNRåŠ æƒ
 å¹³è¡¡ä¸åŒå™ªå£°æ°´å¹³
 æ”¹å–„é«˜å™ªå£°åŒºåŸŸ
 ä½
 
 
 Min-SNR-Î³
 é¿å…æç«¯æƒé‡
 ç›®å‰æœ€ä¼˜
 ä½
 
 
 é‡è¦æ€§é‡‡æ ·
 ç†è®ºæœ€ä¼˜
 å®è·µä¸­ä¸ç¨³å®š
 ä¸­ç­‰
 
 



### 3.4.4 è®­ç»ƒç®—æ³•æ€»ç»“


 ç»¼åˆä»¥ä¸Šæ¨å¯¼ï¼ŒDDPMçš„è®­ç»ƒç®—æ³•æå…¶ç®€æ´ï¼š




def train_ddpm(model, dataloader, num_epochs, T=1000):
 """DDPMè®­ç»ƒå¾ªç¯"""
 optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)

 # é¢„è®¡ç®—å™ªå£°è°ƒåº¦ç›¸å…³å€¼
 betas = linear_beta_schedule(T)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)
 sqrt_alphas_bar = torch.sqrt(alphas_bar)
 sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)

 for epoch in range(num_epochs):
 for batch_idx, (x_0, _) in enumerate(dataloader):
 batch_size = x_0.shape[0]

 # éšæœºé‡‡æ ·æ—¶é—´æ­¥
 t = torch.randint(0, T, (batch_size,), device=x_0.device)

 # é‡‡æ ·å™ªå£°
 epsilon = torch.randn_like(x_0)

 # å‰å‘æ‰©æ•£ï¼šè®¡ç®—x_t
 x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 +
 sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)

 # é¢„æµ‹å™ªå£°
 epsilon_pred = model(x_t, t)

 # è®¡ç®—æŸå¤±
 loss = F.mse_loss(epsilon_pred, epsilon)

 # åå‘ä¼ æ’­
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()

 if batch_idx % 100 == 0:
 print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')




ç»ƒä¹  3.4ï¼šå®ç°åŠ æƒæŸå¤±
 ä¿®æ”¹ä¸Šè¿°è®­ç»ƒä»£ç ï¼Œå®ç°Min-SNR-Î³åŠ æƒç­–ç•¥ï¼š



 - è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„SNR
 - åº”ç”¨Min-SNR-Î³åŠ æƒï¼ˆå»ºè®®Î³=5ï¼‰
 - æ¯”è¾ƒåŠ æƒå‰åçš„è®­ç»ƒæ›²çº¿

def train_ddpm_weighted(model, dataloader, num_epochs, T=1000, snr_gamma=5.0):
 """å¸¦Min-SNRåŠ æƒçš„DDPMè®­ç»ƒ"""
 optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)

 # é¢„è®¡ç®—
 betas = linear_beta_schedule(T)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)
 sqrt_alphas_bar = torch.sqrt(alphas_bar)
 sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)

 # é¢„è®¡ç®—SNRå’Œæƒé‡
 snr = alphas_bar / (1 - alphas_bar)
 snr_clipped = torch.minimum(snr, torch.tensor(snr_gamma))
 loss_weights = snr_clipped / snr

 for epoch in range(num_epochs):
 for batch_idx, (x_0, _) in enumerate(dataloader):
 batch_size = x_0.shape[0]

 # é‡‡æ ·æ—¶é—´æ­¥
 t = torch.randint(0, T, (batch_size,), device=x_0.device)

 # å‰å‘æ‰©æ•£
 epsilon = torch.randn_like(x_0)
 x_t = (sqrt_alphas_bar[t, None, None, None] * x_0 +
 sqrt_one_minus_alphas_bar[t, None, None, None] * epsilon)

 # é¢„æµ‹å™ªå£°
 epsilon_pred = model(x_t, t)

 # è®¡ç®—åŠ æƒæŸå¤±
 mse_loss = (epsilon_pred - epsilon).pow(2).mean(dim=[1,2,3])
 weights = loss_weights[t]
 loss = (weights * mse_loss).mean()

 # åå‘ä¼ æ’­
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()

# å…³é”®æ”¹è¿›ï¼š
# 1. é«˜SNRï¼ˆä½å™ªå£°ï¼‰åŒºåŸŸçš„æƒé‡è¢«é™ä½ï¼Œé¿å…è¿‡æ‹Ÿåˆç»†èŠ‚
# 2. ä½SNRï¼ˆé«˜å™ªå£°ï¼‰åŒºåŸŸä¿æŒè¾ƒé«˜æƒé‡ï¼Œç¡®ä¿ç»“æ„å­¦ä¹ 
# 3. Î³å‚æ•°æ§åˆ¶æˆªæ–­ç¨‹åº¦ï¼Œé€šå¸¸5-10æ•ˆæœè¾ƒå¥½
 **å®è·µå»ºè®®**ï¼šMin-SNR-Î³åŠ æƒåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚ä½†å¯¹äºä½åˆ†è¾¨ç‡æˆ–ç®€å•æ•°æ®é›†ï¼Œç®€å•æŸå¤±å¯èƒ½å·²ç»è¶³å¤Ÿã€‚





## 3.5 é‡‡æ ·ç®—æ³•ï¼šä»ç†è®ºåˆ°å®è·µ



è®­ç»ƒå¥½DDPMåï¼Œå¦‚ä½•ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Ÿè¿™ä¸€èŠ‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»DDPMçš„é‡‡æ ·ç®—æ³•ï¼Œä»æ ‡å‡†çš„1000æ­¥é‡‡æ ·åˆ°å„ç§å®ç”¨æŠ€å·§ã€‚



### 3.5.1 æ ‡å‡†DDPMé‡‡æ ·



DDPMçš„é‡‡æ ·è¿‡ç¨‹æ˜¯ä»çº¯å™ªå£° $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ å¼€å§‹ï¼Œé€æ­¥å»å™ªç›´åˆ°å¾—åˆ°æ¸…æ™°çš„å›¾åƒ $\mathbf{x}_0$ã€‚



> **å®šä¹‰**
> DDPMé‡‡æ ·ç®—æ³•


å¯¹äºæ¯ä¸€æ­¥ $t = T, T-1, ..., 1$ï¼š



 $$\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right) + \sigma_t \mathbf{z}$$



å…¶ä¸­ï¼š



 - $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ æ˜¯è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹ç½‘ç»œ
 - $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$ æ˜¯é‡‡æ ·å™ªå£°ï¼ˆå½“ $t > 1$ æ—¶ï¼‰
 - $\sigma_t$ æ˜¯æ–¹å·®ï¼ŒDDPMä½¿ç”¨ $\sigma_t = \beta_t$





å®Œæ•´çš„å®ç°ä»£ç ï¼š




@torch.no_grad()
def ddpm_sample(model, shape, num_timesteps=1000, device='cuda'):
 """
 DDPMæ ‡å‡†é‡‡æ ·ç®—æ³•

 Args:
 model: è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹æ¨¡å‹
 shape: ç”Ÿæˆå›¾åƒçš„å½¢çŠ¶ï¼Œå¦‚ (batch_size, 3, 32, 32)
 num_timesteps: æ€»æ—¶é—´æ­¥æ•°
 device: è®¡ç®—è®¾å¤‡

 Returns:
 ç”Ÿæˆçš„å›¾åƒ x_0
 """
 # é¢„è®¡ç®—å™ªå£°è°ƒåº¦
 betas = linear_beta_schedule(num_timesteps).to(device)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)
 sqrt_alphas = torch.sqrt(alphas)
 sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)

 # ä»çº¯å™ªå£°å¼€å§‹
 x_t = torch.randn(shape, device=device)

 # é€æ­¥å»å™ª
 for t in reversed(range(num_timesteps)):
 # åˆ›å»ºæ—¶é—´æ­¥å¼ é‡
 t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)

 # é¢„æµ‹å™ªå£°
 epsilon_pred = model(x_t, t_tensor)

 # è®¡ç®—å‡å€¼
 mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]

 # æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
 if t > 0:
 noise = torch.randn_like(x_t)
 std = torch.sqrt(betas[t]) # DDPMä½¿ç”¨Î²_tä½œä¸ºæ–¹å·®
 x_t = mean + std * noise
 else:
 x_t = mean

 return x_t



#### é‡‡æ ·è¿‡ç¨‹çš„å¯è§†åŒ–


 ä¸ºäº†æ›´å¥½åœ°ç†è§£é‡‡æ ·è¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–ä¸åŒæ—¶é—´æ­¥çš„ä¸­é—´ç»“æœï¼š




def get_sampling_trajectory(model, num_steps_to_show=10):
 """è·å–DDPMé‡‡æ ·è¿‡ç¨‹çš„ä¸­é—´ç»“æœ"""
 # é‡‡æ ·å¹¶ä¿å­˜ä¸­é—´ç»“æœ
 shape = (1, 3, 32, 32)
 T = 1000

 # é€‰æ‹©è¦å±•ç¤ºçš„æ—¶é—´æ­¥
 steps_to_show = torch.linspace(T-1, 0, num_steps_to_show, dtype=torch.long)
 intermediate_results = []

 # åˆå§‹åŒ–
 x_t = torch.randn(shape, device='cuda')
 betas = linear_beta_schedule(T).to('cuda')
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)

 # é‡‡æ ·è¿‡ç¨‹
 for t in reversed(range(T)):
 t_tensor = torch.full((1,), t, device='cuda', dtype=torch.long)

 # é¢„æµ‹å¹¶æ›´æ–°
 epsilon_pred = model(x_t, t_tensor)
 # ... (é‡‡æ ·æ­¥éª¤åŒä¸Š)

 # ä¿å­˜ä¸­é—´ç»“æœ
 if t in steps_to_show:
 # å°†x_tæ˜ å°„åˆ°[0, 1]èŒƒå›´ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
 img = (x_t.clamp(-1, 1) + 1) / 2
 intermediate_results.append({
 'timestep': t,
 'image': img.cpu()
 })

 return intermediate_results

# ä½¿ç”¨ç¤ºä¾‹
trajectory = get_sampling_trajectory(model, num_steps_to_show=10)
print(f"Saved {len(trajectory)} intermediate results")
for i, result in enumerate(trajectory):
 print(f"Step {i}: t={result['timestep']}, shape={result['image'].shape}")




#### é‡‡æ ·è¿‡ç¨‹çš„ç‰¹ç‚¹



 - **å‰æœŸï¼ˆt â‰ˆ 1000ï¼‰**ï¼šä¸»è¦æ¢å¤å…¨å±€ç»“æ„å’Œå¤§è‡´å½¢çŠ¶
 - **ä¸­æœŸï¼ˆt â‰ˆ 500ï¼‰**ï¼šç»†åŒ–å¯¹è±¡è½®å»“å’Œä¸»è¦ç‰¹å¾
 - **åæœŸï¼ˆt â‰ˆ 0ï¼‰**ï¼šæ·»åŠ çº¹ç†ç»†èŠ‚å’Œé«˜é¢‘ä¿¡æ¯


 è¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºè‰ºæœ¯å®¶ä½œç”»ï¼šå…ˆå‹¾å‹’è½®å»“ï¼Œå†å¡«å……é¢œè‰²ï¼Œæœ€åæ·»åŠ ç»†èŠ‚ã€‚




#### è®¡ç®—æ•ˆç‡åˆ†æ



æ ‡å‡†DDPMé‡‡æ ·çš„ä¸»è¦é—®é¢˜æ˜¯é€Ÿåº¦æ…¢ã€‚è®©æˆ‘ä»¬åˆ†æä¸€ä¸‹è®¡ç®—æˆæœ¬ï¼š




def analyze_sampling_cost(model, batch_size=16, image_size=256):
 """åˆ†æDDPMé‡‡æ ·çš„è®¡ç®—æˆæœ¬"""
 import time

 shape = (batch_size, 3, image_size, image_size)
 T = 1000

 # æµ‹é‡å•æ¬¡å‰å‘ä¼ æ’­æ—¶é—´
 x = torch.randn(shape, device='cuda')
 t = torch.randint(0, T, (batch_size,), device='cuda')

 # é¢„çƒ­GPU
 for _ in range(10):
 _ = model(x, t)
 torch.cuda.synchronize()

 # è®¡æ—¶
 start = time.time()
 num_runs = 50
 for _ in range(num_runs):
 _ = model(x, t)
 torch.cuda.synchronize()
 end = time.time()

 time_per_forward = (end - start) / num_runs
 total_time = time_per_forward * T

 print(f"å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size}")
 print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
 print(f"å•æ¬¡å‰å‘ä¼ æ’­: {time_per_forward*1000:.2f} ms")
 print(f"å®Œæ•´é‡‡æ · (1000æ­¥): {total_time:.2f} ç§’")
 print(f"æ¯ç§’ç”Ÿæˆå›¾åƒæ•°: {batch_size/total_time:.3f}")

 # å†…å­˜ä½¿ç”¨ä¼°è®¡
 model_params = sum(p.numel() for p in model.parameters()) * 4 / 1024**3 # GB
 activation_memory = batch_size * 3 * image_size**2 * 4 * 50 / 1024**3 # ç²—ç•¥ä¼°è®¡
 print(f"\nå†…å­˜ä½¿ç”¨:")
 print(f"æ¨¡å‹å‚æ•°: {model_params:.2f} GB")
 print(f"æ¿€æ´»å€¼ (ä¼°è®¡): {activation_memory:.2f} GB")



> **å®šä¹‰**
> å…¸å‹æ€§èƒ½æ•°æ®

 
 
 é…ç½®
 å•æ­¥æ—¶é—´
 æ€»é‡‡æ ·æ—¶é—´
 ååé‡
 
 
 32Ã—32, batch=64
 ~5ms
 5ç§’
 12.8 å›¾åƒ/ç§’
 
 
 256Ã—256, batch=8
 ~50ms
 50ç§’
 0.16 å›¾åƒ/ç§’
 
 
 512Ã—512, batch=4
 ~200ms
 200ç§’
 0.02 å›¾åƒ/ç§’
 
 
 *åŸºäºRTX 3090ï¼Œå®é™…æ€§èƒ½å› æ¨¡å‹æ¶æ„è€Œå¼‚





ç»ƒä¹  3.5.1ï¼šå®ç°é‡‡æ ·è¿›åº¦æ¡

ä¿®æ”¹DDPMé‡‡æ ·å‡½æ•°ï¼Œæ·»åŠ ï¼š



 - tqdmè¿›åº¦æ¡æ˜¾ç¤ºé‡‡æ ·è¿›åº¦
 - å¯é€‰çš„ä¸­é—´ç»“æœä¿å­˜
 - EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰æ¨¡å‹æ”¯æŒ

from tqdm import tqdm

@torch.no_grad()
def ddpm_sample_with_progress(
 model,
 shape,
 num_timesteps=1000,
 device='cuda',
 use_ema=True,
 ema_model=None,
 save_intermediate=False,
 save_steps=None
):
 """å¢å¼ºç‰ˆDDPMé‡‡æ ·"""
 # é€‰æ‹©æ¨¡å‹
 if use_ema and ema_model is not None:
 sample_model = ema_model
 else:
 sample_model = model

 sample_model.eval()

 # é¢„è®¡ç®—
 betas = linear_beta_schedule(num_timesteps).to(device)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)
 sqrt_alphas = torch.sqrt(alphas)
 sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)

 # åˆå§‹åŒ–
 x_t = torch.randn(shape, device=device)
 intermediates = []

 # é‡‡æ ·å¾ªç¯
 for t in tqdm(reversed(range(num_timesteps)), desc='Sampling', total=num_timesteps):
 t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)

 # é¢„æµ‹å™ªå£°
 epsilon_pred = sample_model(x_t, t_tensor)

 # æ›´æ–°x_t
 mean = (x_t - betas[t] / sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas[t]

 if t > 0:
 noise = torch.randn_like(x_t)
 std = torch.sqrt(betas[t])
 x_t = mean + std * noise
 else:
 x_t = mean

 # ä¿å­˜ä¸­é—´ç»“æœ
 if save_intermediate and save_steps is not None and t in save_steps:
 intermediates.append({
 't': t,
 'x_t': x_t.cpu().clone(),
 'pred_x_0': self._predict_x0_from_eps(x_t, t, epsilon_pred)
 })

 if save_intermediate:
 return x_t, intermediates
 else:
 return x_t

def _predict_x0_from_eps(x_t, t, epsilon_pred):
 """ä»å™ªå£°é¢„æµ‹æ¢å¤x_0ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
 return (x_t - sqrt_one_minus_alphas_bar[t] * epsilon_pred) / sqrt_alphas_bar[t]
 **ä½¿ç”¨æŠ€å·§**ï¼š



 - EMAæ¨¡å‹é€šå¸¸ç”Ÿæˆè´¨é‡æ›´å¥½ï¼Œè®­ç»ƒæ—¶åº”åŒæ—¶ç»´æŠ¤
 - ä¿å­˜ä¸­é—´ç»“æœæœ‰åŠ©äºè°ƒè¯•å’Œç†è§£æ¨¡å‹è¡Œä¸º
 - å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œè€ƒè™‘ä½¿ç”¨DataLoaderé£æ ¼çš„ç”Ÿæˆå™¨ä»¥èŠ‚çœå†…å­˜





### 3.5.2 é‡‡æ ·çš„éšæœºæ€§æ§åˆ¶



DDPMé‡‡æ ·è¿‡ç¨‹ä¸­çš„éšæœºæ€§æ¥æºäºä¸¤ä¸ªåœ°æ–¹ï¼šåˆå§‹å™ªå£° $\mathbf{x}_T$ å’Œæ¯æ­¥æ·»åŠ çš„å™ªå£° $\mathbf{z}_t$ã€‚é€šè¿‡æ§åˆ¶è¿™äº›éšæœºæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å½±å“ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚



#### æ¸©åº¦å‚æ•°çš„å¼•å…¥



ç±»ä¼¼äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥æ¸©åº¦å‚æ•°æ¥æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§ï¼š




def ddpm_sample_with_temperature(
 model,
 shape,
 temperature=1.0,
 noise_temperature=1.0,
 num_timesteps=1000,
 device='cuda'
):
 """
 å¸¦æ¸©åº¦æ§åˆ¶çš„DDPMé‡‡æ ·

 Args:
 temperature: æ§åˆ¶åˆå§‹å™ªå£°çš„æ¸©åº¦
 noise_temperature: æ§åˆ¶æ¯æ­¥å™ªå£°çš„æ¸©åº¦
 """
 # é¢„è®¡ç®—ï¼ˆåŒå‰ï¼‰
 betas = linear_beta_schedule(num_timesteps).to(device)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)

 # æ¸©åº¦è°ƒæ•´çš„åˆå§‹å™ªå£°
 x_t = torch.randn(shape, device=device) * temperature

 for t in reversed(range(num_timesteps)):
 t_tensor = torch.full((shape[0],), t, device=device, dtype=torch.long)

 # é¢„æµ‹å™ªå£°
 epsilon_pred = model(x_t, t_tensor)

 # è®¡ç®—å‡å€¼
 mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])

 if t > 0:
 # æ¸©åº¦è°ƒæ•´çš„æ­¥è¿›å™ªå£°
 noise = torch.randn_like(x_t) * noise_temperature
 std = torch.sqrt(betas[t])
 x_t = mean + std * noise
 else:
 x_t = mean

 return x_t



> **å®šä¹‰**
> æ¸©åº¦å‚æ•°çš„æ•ˆæœ



 - **temperature  1.0**ï¼šå¢åŠ åˆå§‹éšæœºæ€§ï¼Œç”Ÿæˆæ›´å¤šæ ·ä½†å¯èƒ½è´¨é‡è¾ƒä½çš„æ ·æœ¬
 - **noise_temperature  1.0**ï¼šå¢åŠ å»å™ªéšæœºæ€§ï¼Œå¯èƒ½äº§ç”Ÿæ›´å¤šç»†èŠ‚ä½†ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±





#### ç¡®å®šæ€§é‡‡æ ·ï¼šDDIMé¢„è§ˆ


 ä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿæ˜¯ï¼šå¦‚æœæˆ‘ä»¬å®Œå…¨å»é™¤æ­¥è¿›å™ªå£°ï¼ˆè®¾ç½® $\sigma_t = 0$ï¼‰ï¼Œé‡‡æ ·è¿‡ç¨‹å˜æˆç¡®å®šæ€§çš„ã€‚è¿™å°±æ˜¯DDIMçš„æ ¸å¿ƒæ€æƒ³ï¼š




def ddpm_deterministic_sample(model, shape, num_timesteps=1000, eta=0.0):
 """
 ç¡®å®šæ€§æˆ–éƒ¨åˆ†ç¡®å®šæ€§é‡‡æ ·
 eta=0: å®Œå…¨ç¡®å®šæ€§ï¼ˆDDIMï¼‰
 eta=1: æ ‡å‡†DDPMï¼ˆå®Œå…¨éšæœºï¼‰
 """
 x_t = torch.randn(shape, device='cuda')

 for t in reversed(range(num_timesteps)):
 # é¢„æµ‹å™ªå£°
 epsilon_pred = model(x_t, t)

 # é¢„æµ‹x_0
 x_0_pred = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])

 if t > 0:
 # è®¡ç®—æ–¹å‘æŒ‡å‘x_{t-1}
 direction = torch.sqrt(1 - alphas_bar[t-1]) * epsilon_pred

 # ç¡®å®šæ€§éƒ¨åˆ†
 x_t = torch.sqrt(alphas_bar[t-1]) * x_0_pred + direction

 # éšæœºéƒ¨åˆ†ï¼ˆç”±etaæ§åˆ¶ï¼‰
 if eta > 0:
 noise = torch.randn_like(x_t)
 variance = eta * betas[t] * (1 - alphas_bar[t-1]) / (1 - alphas_bar[t])
 x_t = x_t + torch.sqrt(variance) * noise
 else:
 x_t = x_0_pred

 return x_t



#### é‡‡æ ·ç§å­ä¸å¯é‡å¤æ€§


 å¯¹äºéœ€è¦å¯é‡å¤ç»“æœçš„åº”ç”¨ï¼Œæ§åˆ¶éšæœºç§å­è‡³å…³é‡è¦ï¼š




class SeededSampler:
 """å¯é‡å¤çš„é‡‡æ ·å™¨"""
 def __init__(self, model, device='cuda'):
 self.model = model
 self.device = device

 def sample_with_seed(self, seed, shape, **kwargs):
 """ä½¿ç”¨æŒ‡å®šç§å­é‡‡æ ·"""
 # ä¿å­˜å½“å‰éšæœºçŠ¶æ€
 cpu_state = torch.get_rng_state()
 cuda_state = torch.cuda.get_rng_state(self.device)

 # è®¾ç½®ç§å­
 torch.manual_seed(seed)
 torch.cuda.manual_seed(seed)

 # é‡‡æ ·
 result = ddpm_sample(self.model, shape, device=self.device, **kwargs)

 # æ¢å¤éšæœºçŠ¶æ€
 torch.set_rng_state(cpu_state)
 torch.cuda.set_rng_state(cuda_state, self.device)

 return result

 def sample_variations(self, base_seed, num_variations, shape, temperature_range=(0.8, 1.2)):
 """ç”ŸæˆåŒä¸€ç§å­çš„å¤šä¸ªå˜ä½“"""
 variations = []

 for i in range(num_variations):
 # ä½¿ç”¨ç›¸åŒçš„åŸºç¡€ç§å­ä½†ä¸åŒçš„æ¸©åº¦
 temp = np.linspace(temperature_range[0], temperature_range[1], num_variations)[i]

 torch.manual_seed(base_seed)
 torch.cuda.manual_seed(base_seed)

 sample = ddpm_sample_with_temperature(
 self.model, shape,
 temperature=temp,
 device=self.device
 )
 variations.append(sample)

 return torch.stack(variations)



#### é«˜çº§æŠ€å·§ï¼šå¼•å¯¼é‡‡æ ·ï¼ˆGuided Samplingï¼‰


 æˆ‘ä»¬å¯ä»¥åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥é¢å¤–çš„å¼•å¯¼ä¿¡å·ï¼Œè¿™æ˜¯æ¡ä»¶ç”Ÿæˆçš„åŸºç¡€ï¼š




def guided_sample(model, shape, guidance_fn=None, guidance_scale=1.0):
 """
 å¸¦å¼•å¯¼çš„é‡‡æ ·
 guidance_fn: è®¡ç®—å¼•å¯¼æ¢¯åº¦çš„å‡½æ•°
 guidance_scale: å¼•å¯¼å¼ºåº¦
 """
 x_t = torch.randn(shape, device='cuda')
 x_t.requires_grad = True

 for t in reversed(range(num_timesteps)):
 # æ ‡å‡†DDPMæ›´æ–°
 with torch.no_grad():
 epsilon_pred = model(x_t, t)
 mean = compute_mean(x_t, epsilon_pred, t)
 std = torch.sqrt(betas[t])

 # è®¡ç®—å¼•å¯¼æ¢¯åº¦
 if guidance_fn is not None and t > 0:
 # è®¡ç®—å¼•å¯¼æŸå¤±
 guidance_loss = guidance_fn(x_t, t)

 # è®¡ç®—æ¢¯åº¦
 grad = torch.autograd.grad(guidance_loss, x_t)[0]

 # åº”ç”¨å¼•å¯¼ï¼ˆæ³¨æ„ç¬¦å·ï¼šæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±ï¼‰
 mean = mean - guidance_scale * std**2 * grad

 # æ›´æ–°x_t
 if t > 0:
 noise = torch.randn_like(x_t)
 x_t = mean + std * noise
 else:
 x_t = mean

 x_t = x_t.detach().requires_grad_(True)

 return x_t.detach()

# ç¤ºä¾‹ï¼šç±»åˆ«å¼•å¯¼
def classifier_guidance(x_t, t, classifier, target_class):
 """ä½¿ç”¨åˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆç‰¹å®šç±»åˆ«"""
 logits = classifier(x_t, t)
 log_prob = F.log_softmax(logits, dim=1)
 return -log_prob[:, target_class].sum() # è´Ÿå¯¹æ•°æ¦‚ç‡ä½œä¸ºæŸå¤±




ç»ƒä¹  3.5.2ï¼šæ¢ç´¢æ¸©åº¦å‚æ•°çš„å½±å“
 å®ç°ä¸€ä¸ªå®éªŒï¼Œç³»ç»Ÿåœ°æ¢ç´¢ä¸åŒæ¸©åº¦å‚æ•°å¯¹ç”Ÿæˆç»“æœçš„å½±å“ï¼š



 - å›ºå®šç§å­ï¼Œæ”¹å˜temperatureï¼ˆ0.5, 0.7, 1.0, 1.3, 1.5ï¼‰
 - å›ºå®šç§å­ï¼Œæ”¹å˜noise_temperatureï¼ˆ0, 0.5, 1.0, 1.5ï¼‰
 - å¯è§†åŒ–ç»“æœå¹¶è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡ï¼ˆå¦‚å¹³å‡åƒç´ æ–¹å·®ï¼‰

def temperature_ablation_study(model, seed=42):
 """æ¸©åº¦å‚æ•°æ¶ˆèå®éªŒ"""
 shape = (1, 3, 32, 32)

 # å®éªŒ1ï¼šåˆå§‹æ¸©åº¦çš„å½±å“
 init_temps = [0.5, 0.7, 1.0, 1.3, 1.5]
 init_results = []

 for temp in init_temps:
 torch.manual_seed(seed)
 torch.cuda.manual_seed(seed)

 sample = ddpm_sample_with_temperature(
 model, shape,
 temperature=temp,
 noise_temperature=1.0
 )
 init_results.append(sample)

 # å®éªŒ2ï¼šå™ªå£°æ¸©åº¦çš„å½±å“
 noise_temps = [0.0, 0.5, 1.0, 1.5]
 noise_results = []

 for noise_temp in noise_temps:
 torch.manual_seed(seed)
 torch.cuda.manual_seed(seed)

 sample = ddpm_sample_with_temperature(
 model, shape,
 temperature=1.0,
 noise_temperature=noise_temp
 )
 noise_results.append(sample)

 # åˆ†æç»“æœ
 results = {
 'init_temperature': {},
 'noise_temperature': {}
 }

 # è®¡ç®—åˆå§‹æ¸©åº¦çš„å½±å“
 print("åˆå§‹æ¸©åº¦å¯¹å›¾åƒç»Ÿè®¡ç‰¹æ€§çš„å½±å“:")
 for temp, img in zip(init_temps, init_results):
 stats = {
 'mean': img.mean().item(),
 'std': img.std().item(),
 'min': img.min().item(),
 'max': img.max().item()
 }
 results['init_temperature'][temp] = stats
 print(f" T_init={temp}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")

 # è®¡ç®—å™ªå£°æ¸©åº¦çš„å½±å“
 print("\nå™ªå£°æ¸©åº¦å¯¹å›¾åƒç»Ÿè®¡ç‰¹æ€§çš„å½±å“:")
 for temp, img in zip(noise_temps, noise_results):
 stats = {
 'mean': img.mean().item(),
 'std': img.std().item(),
 'min': img.min().item(),
 'max': img.max().item()
 }
 results['noise_temperature'][temp] = stats
 print(f" T_noise={temp}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")

 return results

# é¢å¤–åˆ†æï¼šå¤šæ¬¡é‡‡æ ·çš„å¤šæ ·æ€§
def diversity_analysis(model, num_samples=100):
 """åˆ†æä¸åŒæ¸©åº¦è®¾ç½®ä¸‹çš„æ ·æœ¬å¤šæ ·æ€§"""
 shape = (num_samples, 3, 32, 32)

 # æ ‡å‡†é‡‡æ ·
 samples_standard = ddpm_sample(model, shape)

 # ä½æ¸©é‡‡æ ·
 samples_low_temp = ddpm_sample_with_temperature(
 model, shape, temperature=0.7, noise_temperature=0.7
 )

 # è®¡ç®—æˆå¯¹è·ç¦»
 def pairwise_l2_distance(samples):
 # å±•å¹³æ ·æœ¬
 flat = samples.view(num_samples, -1)
 # è®¡ç®—æˆå¯¹L2è·ç¦»
 distances = torch.cdist(flat, flat, p=2)
 # å–ä¸Šä¸‰è§’éƒ¨åˆ†ï¼ˆé¿å…é‡å¤ï¼‰
 mask = torch.triu(torch.ones_like(distances), diagonal=1).bool()
 return distances[mask].mean().item()

 div_standard = pairwise_l2_distance(samples_standard)
 div_low_temp = pairwise_l2_distance(samples_low_temp)

 print(f"æ ‡å‡†é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_standard:.4f}")
 print(f"ä½æ¸©é‡‡æ ·çš„å¹³å‡æˆå¯¹è·ç¦»: {div_low_temp:.4f}")
 print(f"å¤šæ ·æ€§é™ä½æ¯”ä¾‹: {(1 - div_low_temp/div_standard)*100:.1f}%")
 **å…³é”®å‘ç°**ï¼š



 - é™ä½åˆå§‹æ¸©åº¦ä¼šä½¿ç”Ÿæˆç»“æœæ›´æ¥è¿‘"å¹³å‡"å›¾åƒï¼Œå‡å°‘æç«¯æƒ…å†µ
 - noise_temperature=0 ä¼šäº§ç”Ÿè¿‡åº¦å¹³æ»‘çš„ç»“æœï¼Œä¸¢å¤±çº¹ç†ç»†èŠ‚
 - é€‚åº¦é™ä½æ¸©åº¦ï¼ˆ0.7-0.9ï¼‰é€šå¸¸èƒ½æé«˜æ„ŸçŸ¥è´¨é‡ï¼Œä½†ä¼šç‰ºç‰²å¤šæ ·æ€§
 - å¯¹äºç‰¹å®šåº”ç”¨ï¼Œéœ€è¦åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡





### 3.5.3 å¸¸è§é—®é¢˜ä¸è°ƒè¯•æŠ€å·§



DDPMé‡‡æ ·è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°å„ç§é—®é¢˜ã€‚æœ¬èŠ‚æ€»ç»“å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œä¿®å¤é—®é¢˜ã€‚



#### é—®é¢˜1ï¼šç”Ÿæˆç»“æœå…¨æ˜¯å™ªå£°



> **å®šä¹‰**
> ç—‡çŠ¶ä¸åŸå› 



 - **ç—‡çŠ¶**ï¼šé‡‡æ ·ç»“æœçœ‹èµ·æ¥åƒéšæœºå™ªå£°ï¼Œæ²¡æœ‰ä»»ä½•ç»“æ„
 - **å¯èƒ½åŸå› **ï¼š


 æ¨¡å‹æœªæ­£ç¡®åŠ è½½æˆ–æƒé‡æŸå
 - å™ªå£°è°ƒåº¦è®¡ç®—é”™è¯¯
 - æ—¶é—´æ­¥ç¼–ç é”™è¯¯
 - è¾“å…¥å½’ä¸€åŒ–ä¸åŒ¹é…


 






# è°ƒè¯•æ­¥éª¤1ï¼šéªŒè¯æ¨¡å‹é¢„æµ‹
def debug_model_predictions(model, device='cuda'):
 """æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ—¶é—´æ­¥çš„é¢„æµ‹"""
 # åˆ›å»ºæµ‹è¯•è¾“å…¥
 x = torch.randn(1, 3, 32, 32, device=device)

 # æµ‹è¯•å‡ ä¸ªå…³é”®æ—¶é—´æ­¥
 test_timesteps = [0, 250, 500, 750, 999]

 for t in test_timesteps:
 t_tensor = torch.tensor([t], device=device)
 with torch.no_grad():
 pred = model(x, t_tensor)

 print(f"t={t}:")
 print(f" Input stats: mean={x.mean():.4f}, std={x.std():.4f}")
 print(f" Pred stats: mean={pred.mean():.4f}, std={pred.std():.4f}")

 # é¢„æµ‹åº”è¯¥æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
 if abs(pred.mean()) > 0.5 or abs(pred.std() - 1.0) > 0.5:
 print(" âš ï¸ è­¦å‘Šï¼šé¢„æµ‹ç»Ÿè®¡é‡å¼‚å¸¸ï¼")

# è°ƒè¯•æ­¥éª¤2ï¼šéªŒè¯å™ªå£°è°ƒåº¦
def debug_noise_schedule(num_timesteps=1000):
 """æ£€æŸ¥å™ªå£°è°ƒåº¦çš„åˆç†æ€§"""
 betas = linear_beta_schedule(num_timesteps)
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)

 print("å™ªå£°è°ƒåº¦æ£€æŸ¥:")
 print(f"Î²_0 = {betas[0]:.6f}, Î²_T = {betas[-1]:.6f}")
 print(f"á¾±_0 = {alphas_bar[0]:.6f}, á¾±_T = {alphas_bar[-1]:.6f}")

 # æ£€æŸ¥å…³é”®å±æ€§
 if alphas_bar[-1] > 0.01:
 print("âš ï¸ è­¦å‘Šï¼šá¾±_T å¤ªå¤§ï¼Œæœ€ç»ˆå™ªå£°æ°´å¹³ä¸å¤Ÿ")
 if betas[0] > 0.01:
 print("âš ï¸ è­¦å‘Šï¼šÎ²_0 å¤ªå¤§ï¼Œåˆå§‹ç ´åå¤ªä¸¥é‡")

 # æ£€æŸ¥å•è°ƒæ€§
 if not torch.all(alphas_bar[1:] 



#### é—®é¢˜2ï¼šç”Ÿæˆç»“æœæ¨¡ç³Šæˆ–è¿‡åº¦å¹³æ»‘




##### å¸¸è§åŸå› åŠè§£å†³æ–¹æ¡ˆ



 - **æ–¹å·®è®¾ç½®è¿‡å°**ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¿‡å°çš„ $\sigma_t$
 - **æå‰åœæ­¢é‡‡æ ·**ï¼šç¡®ä¿å®Œæˆæ‰€æœ‰1000æ­¥ï¼ˆæˆ–è®¾å®šçš„æ­¥æ•°ï¼‰
 - **æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°å‡å€¼**ï¼šå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒæ—¶çš„å™ªå£°è°ƒåº¦
 - **æ•°å€¼ç²¾åº¦é—®é¢˜**ï¼šä½¿ç”¨FP16æ—¶æŸäº›æ“ä½œå¯èƒ½æŸå¤±ç²¾åº¦






```python
# è¯Šæ–­è¿‡åº¦å¹³æ»‘é—®é¢˜
def diagnose_smoothness(model, num_samples=10):
 """è¯Šæ–­ç”Ÿæˆç»“æœçš„å¹³æ»‘åº¦é—®é¢˜"""
 samples = []

 # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
 for _ in range(num_samples):
 sample = ddpm_sample(model, (1, 3, 32, 32))
 samples.append(sample)

 samples = torch.cat(samples, dim=0)

 # è®¡ç®—é«˜é¢‘ä¿¡æ¯
 def compute_high_freq_energy(images):
 # ä½¿ç”¨Sobelæ»¤æ³¢å™¨æ£€æµ‹è¾¹ç¼˜
 sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
 dtype=torch.float32).view(1, 1, 3, 3)
 sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
 dtype=torch.float32).view(1, 1, 3, 3)

 # è½¬æ¢ä¸ºç°åº¦
 gray = images.mean(dim=1, keepdim=True)

 # è®¡ç®—æ¢¯åº¦
 edges_x = F.conv2d(gray, sobel_x, padding=1)
 edges_y = F.conv2d(gray, sobel_y, padding=1)
 edges = torch.sqrt(edges_x**2 + edges_y**2)

 return edges.mean().item()

 # ä¸çœŸå®æ•°æ®å¯¹æ¯”
 real_data = next(iter(train_loader))[0][:num_samples]

 gen_hf = compute_high_freq_energy(samples)
 real_hf = compute_high_freq_energy(real_data)

 print(f"ç”Ÿæˆæ ·æœ¬çš„é«˜é¢‘èƒ½é‡: {gen_hf:.4f}")
 print(f"çœŸå®æ•°æ®çš„é«˜é¢‘èƒ½é‡: {real_hf:.4f}")
 print(f"æ¯”ç‡: {gen_hf/real_hf:.2f}")

 if gen_hf/real_hf DDPMçš„ä¸»è¦æ€§èƒ½ç“¶é¢ˆåœ¨äºéœ€è¦1000æ­¥è¿­ä»£ï¼Œæ¯æ­¥éƒ½éœ€è¦é€šè¿‡U-Netè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ã€‚å…¸å‹çš„æ€§èƒ½ç‰¹å¾ï¼š



 - **é¢„è®¡ç®—é˜¶æ®µ**ï¼šçº¦0.1ç§’ï¼ŒåŒ…æ‹¬å™ªå£°è°ƒåº¦çš„è®¡ç®—
 - **æ¨¡å‹æ¨ç†**ï¼šæ¯æ­¥15-50msï¼ˆå–å†³äºæ¨¡å‹å¤§å°å’ŒGPUï¼‰ï¼Œæ€»è®¡15-50ç§’
 - **æ›´æ–°è®¡ç®—**ï¼šæ¯æ­¥1-2msï¼Œç›¸å¯¹å¯å¿½ç•¥




**ä¼˜åŒ–å»ºè®®**ï¼š



 - ä½¿ç”¨æ›´å°çš„æ¨¡å‹æ¶æ„ï¼ˆå‡å°‘é€šé“æ•°æˆ–å±‚æ•°ï¼‰
 - å¯ç”¨æ··åˆç²¾åº¦æ¨ç†ï¼ˆtorch.cuda.ampï¼‰
 - ä½¿ç”¨torch.compile()è¿›è¡Œå›¾ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
 - é‡‡ç”¨DDIMç­‰å¿«é€Ÿé‡‡æ ·æ–¹æ³•ï¼ˆå¯å‡å°‘åˆ°50æ­¥ä»¥ä¸‹ï¼‰
 - æ‰¹é‡ç”Ÿæˆä»¥æé«˜GPUåˆ©ç”¨ç‡





#### é—®é¢˜4ï¼šå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰




# å†…å­˜å‹å¥½çš„æ‰¹é‡é‡‡æ ·
def memory_efficient_batch_sampling(model, total_samples, batch_size=16,
 image_shape=(3, 32, 32)):
 """å†…å­˜é«˜æ•ˆçš„æ‰¹é‡é‡‡æ ·"""
 all_samples = []

 # åˆ†æ‰¹ç”Ÿæˆ
 num_batches = (total_samples + batch_size - 1) // batch_size

 for i in tqdm(range(num_batches), desc="Batch sampling"):
 current_batch_size = min(batch_size, total_samples - i * batch_size)
 shape = (current_batch_size,) + image_shape

 # ç”Ÿæˆå½“å‰æ‰¹æ¬¡
 with torch.cuda.amp.autocast(): # ä½¿ç”¨æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
 samples = ddpm_sample(model, shape)

 # ç«‹å³ç§»åˆ°CPUä»¥é‡Šæ”¾GPUå†…å­˜
 all_samples.append(samples.cpu())

 # æ¸…ç†GPUç¼“å­˜
 if i % 10 == 0:
 torch.cuda.empty_cache()

 return torch.cat(all_samples, dim=0)

# è¯Šæ–­å†…å­˜ä½¿ç”¨
def diagnose_memory_usage(model, batch_sizes=[1, 2, 4, 8, 16]):
 """è¯Šæ–­ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨"""
 import gc

 for bs in batch_sizes:
 torch.cuda.empty_cache()
 gc.collect()

 try:
 # è®°å½•åˆå§‹å†…å­˜
 init_mem = torch.cuda.memory_allocated() / 1024**3

 # å°è¯•é‡‡æ ·
 shape = (bs, 3, 256, 256) # ä½¿ç”¨è¾ƒå¤§å°ºå¯¸æµ‹è¯•
 _ = ddpm_sample(model, shape, num_timesteps=50) # åªæµ‹è¯•50æ­¥

 # è®°å½•å³°å€¼å†…å­˜
 peak_mem = torch.cuda.max_memory_allocated() / 1024**3

 print(f"Batch size {bs}: å³°å€¼å†…å­˜ {peak_mem:.2f}GB "
 f"(å¢åŠ  {peak_mem - init_mem:.2f}GB)")

 except torch.cuda.OutOfMemoryError:
 print(f"Batch size {bs}: OOM!")
 break
 finally:
 torch.cuda.empty_cache()



#### å¯è§†åŒ–è°ƒè¯•å·¥å…·




```python
def analyze_sampling_debug(model):
 """åˆ†æé‡‡æ ·è¿‡ç¨‹ç”¨äºè°ƒè¯•"""
 # è®¾ç½®
 shape = (1, 3, 32, 32)
 checkpoints = [999, 800, 600, 400, 200, 100, 50, 20, 10, 0]

 # æ”¶é›†æ•°æ®
 x_t = torch.randn(shape, device='cuda')
 debug_data = {
 'x_t_history': [x_t.cpu()],
 'pred_x0_history': [],
 'noise_pred_history': [],
 'noise_stats': []
 }

 # é‡‡æ ·å¹¶è®°å½•
 betas = linear_beta_schedule(1000).cuda()
 alphas = 1 - betas
 alphas_bar = torch.cumprod(alphas, dim=0)

 for t in reversed(range(1000)):
 t_tensor = torch.tensor([t], device='cuda')

 # é¢„æµ‹
 epsilon_pred = model(x_t, t_tensor)

 # é¢„æµ‹çš„x_0
 pred_x0 = (x_t - torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas_bar[t])

 # æ›´æ–°
 mean = (x_t - betas[t] / torch.sqrt(1 - alphas_bar[t]) * epsilon_pred) / torch.sqrt(alphas[t])
 if t > 0:
 noise = torch.randn_like(x_t)
 x_t = mean + torch.sqrt(betas[t]) * noise
 else:
 x_t = mean

 # è®°å½•æ£€æŸ¥ç‚¹
 if t in checkpoints:
 debug_data['x_t_history'].append(x_t.cpu())
 debug_data['pred_x0_history'].append(pred_x0.cpu())
 debug_data['noise_pred_history'].append(epsilon_pred.cpu())
 debug_data['noise_stats'].append({
 't': t,
 'mean': epsilon_pred.mean().item(),
 'std': epsilon_pred.std().item(),
 'min': epsilon_pred.min().item(),
 'max': epsilon_pred.max().item()
 })

 # æ‰“å°åˆ†æç»“æœ
 print("é‡‡æ ·è¿‡ç¨‹è°ƒè¯•åˆ†æ:")
 print("==================")
 print("\nå™ªå£°é¢„æµ‹ç»Ÿè®¡:")
 print("Timestep | Mean | Std | Min | Max")
 print("-" * 55)
 for stats in debug_data['noise_stats']:
 print(f"{stats['t']:8d} | {stats['mean']:9.6f} | {stats['std']:9.6f} | {stats['min']:9.6f} | {stats['max']:9.6f}")

 # æ£€æŸ¥x_0é¢„æµ‹çš„ç¨³å®šæ€§
 print("\nx_0é¢„æµ‹ç¨³å®šæ€§åˆ†æ:")
 for i, (t, x0) in enumerate(zip(checkpoints[:-1], debug_data['pred_x0_history'])):
 x0_range = x0.max().item() - x0.min().item()
 x0_clipped = (x0  1).sum().item()
 total_pixels = x0.numel()
 clip_ratio = x0_clipped / total_pixels
 print(f"t={t:3d}: range={x0_range:.3f}, clipped pixels={clip_ratio:.1%}")

 return debug_data
```





ç»ƒä¹  3.5.3ï¼šå®ç°é‡‡æ ·è´¨é‡è¯Šæ–­å·¥å…·
 åˆ›å»ºä¸€ä¸ªç»¼åˆè¯Šæ–­å·¥å…·ï¼Œèƒ½å¤Ÿï¼š



 - è‡ªåŠ¨æ£€æµ‹å¸¸è§çš„é‡‡æ ·é—®é¢˜
 - ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
 - æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®

class DDPMSamplingDiagnostics:
 """DDPMé‡‡æ ·ç»¼åˆè¯Šæ–­å·¥å…·"""

 def __init__(self, model, device='cuda'):
 self.model = model
 self.device = device
 self.diagnostics = {}

 def run_full_diagnostics(self, num_samples=5):
 """è¿è¡Œå®Œæ•´è¯Šæ–­"""
 print("=== DDPMé‡‡æ ·è¯Šæ–­å¼€å§‹ ===\n")

 # 1. æ¨¡å‹åŸºç¡€æ£€æŸ¥
 self._check_model_basics()

 # 2. å™ªå£°è°ƒåº¦æ£€æŸ¥
 self._check_noise_schedule()

 # 3. é‡‡æ ·è´¨é‡æ£€æŸ¥
 self._check_sampling_quality(num_samples)

 # 4. æ€§èƒ½æ£€æŸ¥
 self._check_performance()

 # 5. ç”ŸæˆæŠ¥å‘Š
 self._generate_report()

 def _check_model_basics(self):
 """æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®"""
 print("1. æ£€æŸ¥æ¨¡å‹åŸºç¡€è®¾ç½®...")

 # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨evalæ¨¡å¼
 if self.model.training:
 self.diagnostics['model_mode'] = 'WARNING: æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼'
 else:
 self.diagnostics['model_mode'] = 'OK: æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼'

 # æ£€æŸ¥å‚æ•°ç»Ÿè®¡
 params = []
 for p in self.model.parameters():
 params.append(p.data.flatten())
 params = torch.cat(params)

 param_mean = params.mean().item()
 param_std = params.std().item()

 if abs(param_mean) > 1.0 or param_std > 10.0:
 self.diagnostics['param_stats'] = f'WARNING: å‚æ•°ç»Ÿè®¡å¼‚å¸¸ (mean={param_mean:.3f}, std={param_std:.3f})'
 else:
 self.diagnostics['param_stats'] = 'OK: å‚æ•°ç»Ÿè®¡æ­£å¸¸'

 def _check_noise_schedule(self):
 """æ£€æŸ¥å™ªå£°è°ƒåº¦"""
 print("2. æ£€æŸ¥å™ªå£°è°ƒåº¦...")

 betas = linear_beta_schedule(1000)
 alphas_bar = torch.cumprod(1 - betas, dim=0)

 # æ£€æŸ¥ç«¯ç‚¹
 if alphas_bar[0] 0.01:
 self.diagnostics['schedule_end'] = f'WARNING: Î±Ì…_T={alphas_bar[-1]:.4f} å¤ªå¤§'
 else:
 self.diagnostics['schedule_end'] = 'OK: ç»ˆç‚¹æ­£å¸¸'

 def _check_sampling_quality(self, num_samples):
 """æ£€æŸ¥é‡‡æ ·è´¨é‡"""
 print(f"3. æ£€æŸ¥é‡‡æ ·è´¨é‡ (ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬)...")

 samples = []
 for _ in range(num_samples):
 sample = ddpm_sample(self.model, (1, 3, 32, 32), device=self.device)
 samples.append(sample)
 samples = torch.cat(samples)

 # æ£€æŸ¥è¾“å‡ºèŒƒå›´
 sample_min = samples.min().item()
 sample_max = samples.max().item()

 if sample_min 3:
 self.diagnostics['output_range'] = f'WARNING: è¾“å‡ºèŒƒå›´å¼‚å¸¸ [{sample_min:.2f}, {sample_max:.2f}]'
 else:
 self.diagnostics['output_range'] = 'OK: è¾“å‡ºèŒƒå›´æ­£å¸¸'

 # æ£€æŸ¥å¤šæ ·æ€§
 if num_samples > 1:
 diversity = samples.std(dim=0).mean().item()
 if diversity 60:
 self.diagnostics['performance'] = f'WARNING: é¢„è®¡é‡‡æ ·æ—¶é—´è¿‡é•¿ ({total_time:.1f}ç§’)'
 else:
 self.diagnostics['performance'] = f'OK: é¢„è®¡é‡‡æ ·æ—¶é—´ {total_time:.1f}ç§’'

 def _generate_report(self):
 """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
 print("\n=== è¯Šæ–­æŠ¥å‘Š ===")

 warnings = 0
 for key, value in self.diagnostics.items():
 if value.startswith('WARNING'):
 print(f"âŒ {value}")
 warnings += 1
 else:
 print(f"âœ… {value}")

 print(f"\næ€»ç»“: {len(self.diagnostics)}é¡¹æ£€æŸ¥, {warnings}ä¸ªè­¦å‘Š")

 if warnings > 0:
 print("\nå»ºè®®çš„ä¿®å¤æ­¥éª¤:")
 if 'model_mode' in self.diagnostics and 'WARNING' in self.diagnostics['model_mode']:
 print("- è°ƒç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼")
 if 'schedule_end' in self.diagnostics and 'WARNING' in self.diagnostics['schedule_end']:
 print("- å¢åŠ æ€»æ—¶é—´æ­¥æ•°æˆ–è°ƒæ•´beta_end")
 if 'diversity' in self.diagnostics and 'WARNING' in self.diagnostics['diversity']:
 print("- æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¨¡å¼å´©å¡Œ")
 if 'performance' in self.diagnostics and 'WARNING' in self.diagnostics['performance']:
 print("- è€ƒè™‘ä½¿ç”¨DDIMæˆ–å…¶ä»–å¿«é€Ÿé‡‡æ ·æ–¹æ³•")

# ä½¿ç”¨ç¤ºä¾‹
diagnostics = DDPMSamplingDiagnostics(model)
diagnostics.run_full_diagnostics()
 **è¯Šæ–­å·¥å…·çš„æ‰©å±•**ï¼šå¯ä»¥æ·»åŠ æ›´å¤šæ£€æŸ¥é¡¹ï¼Œå¦‚ï¼š



 - æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†EMAæ¨¡å‹
 - éªŒè¯æ¡ä»¶ç”Ÿæˆçš„æ­£ç¡®æ€§
 - æ£€æµ‹ç‰¹å®šçš„è§†è§‰ä¼ªå½±ï¼ˆæ£‹ç›˜æ•ˆåº”ã€è‰²å½©åç§»ç­‰ï¼‰
 - ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„ç»Ÿè®¡å¯¹æ¯”





## 3.6 å®Œæ•´å®ç°ï¼šæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªDDPM


æœ¬èŠ‚å°†æŠŠå‰é¢å­¦åˆ°çš„æ‰€æœ‰æ¦‚å¿µæ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„DDPMå®ç°ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå¯ä»¥åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å®Œæ•´ç³»ç»Ÿã€‚



### 3.6.1 æ¨¡å‹æ¶æ„


é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªé€‚åˆDDPMçš„U-Netæ¶æ„ã€‚è¿™ä¸ªæ¶æ„éœ€è¦ï¼š



 - æ¥å—å¸¦å™ªå£°çš„å›¾åƒ $x_t$ ä½œä¸ºè¾“å…¥
 - æ¥å—æ—¶é—´æ­¥ $t$ ä½œä¸ºæ¡ä»¶ä¿¡æ¯
 - è¾“å‡ºé¢„æµ‹çš„å™ªå£° $\epsilon_\theta(x_t, t)$




import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SinusoidalPositionalEmbedding(nn.Module):
 """æ­£å¼¦ä½ç½®ç¼–ç ï¼Œç”¨äºæ—¶é—´æ­¥åµŒå…¥"""
 def __init__(self, dim):
 super().__init__()
 self.dim = dim

 def forward(self, time):
 device = time.device
 half_dim = self.dim // 2
 embeddings = math.log(10000) / (half_dim - 1)
 embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
 embeddings = time[:, None] * embeddings[None, :]
 embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
 return embeddings

class ResidualBlock(nn.Module):
 """å¸¦æ—¶é—´åµŒå…¥çš„æ®‹å·®å—"""
 def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):
 super().__init__()
 self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
 self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
 self.time_emb = nn.Linear(time_emb_dim, out_channels)
 self.dropout = nn.Dropout(dropout)
 self.norm1 = nn.GroupNorm(8, out_channels)
 self.norm2 = nn.GroupNorm(8, out_channels)
 self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

 def forward(self, x, t):
 h = self.conv1(x)
 h = self.norm1(h)
 h = F.silu(h)

 # æ·»åŠ æ—¶é—´åµŒå…¥
 h = h + self.time_emb(F.silu(t))[:, :, None, None]

 h = self.conv2(h)
 h = self.norm2(h)
 h = F.silu(h)
 h = self.dropout(h)

 return h + self.shortcut(x)

class AttentionBlock(nn.Module):
 """è‡ªæ³¨æ„åŠ›å—"""
 def __init__(self, channels, num_heads=4):
 super().__init__()
 self.num_heads = num_heads
 self.norm = nn.GroupNorm(8, channels)
 self.qkv = nn.Conv2d(channels, channels * 3, 1)
 self.proj = nn.Conv2d(channels, channels, 1)

 def forward(self, x):
 B, C, H, W = x.shape
 h = self.norm(x)
 qkv = self.qkv(h)
 q, k, v = qkv.chunk(3, dim=1)

 # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
 q = q.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
 k = k.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)
 v = v.view(B, self.num_heads, C // self.num_heads, H * W).transpose(2, 3)

 # è®¡ç®—æ³¨æ„åŠ›
 scale = (C // self.num_heads) ** -0.5
 attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * scale, dim=-1)
 out = torch.matmul(attn, v)

 # é‡å¡‘å›åŸå§‹æ ¼å¼
 out = out.transpose(2, 3).contiguous().view(B, C, H, W)
 return x + self.proj(out)



 æ¶æ„è®¾è®¡è¦ç‚¹


 - **æ—¶é—´åµŒå…¥**ï¼šä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç å°†ç¦»æ•£æ—¶é—´æ­¥è½¬æ¢ä¸ºè¿ç»­è¡¨ç¤º
 - **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸ªå—éƒ½åŒ…å«æ®‹å·®è¿æ¥ï¼Œæœ‰åŠ©äºæ¢¯åº¦æµåŠ¨
 - **æ³¨æ„åŠ›æœºåˆ¶**ï¼šåœ¨ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾ä¸Šä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼Œæ•è·é•¿ç¨‹ä¾èµ–
 - **GroupNorm**ï¼šä½¿ç”¨ç»„å½’ä¸€åŒ–è€Œéæ‰¹å½’ä¸€åŒ–ï¼Œæ›´é€‚åˆå°æ‰¹é‡è®­ç»ƒ





#### è½»é‡çº§DDPM U-Net

 å¯¹äºç®€å•ä»»åŠ¡ï¼ˆå¦‚MNISTï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ›´è½»é‡çš„æ¶æ„ï¼š



class SimpleDDPMUNet(nn.Module):
 """è½»é‡çº§DDPM U-Netï¼Œé€‚ç”¨äºMNISTç­‰ç®€å•æ•°æ®é›†"""
 def __init__(self, image_channels=1, n_channels=32, ch_mults=(1, 2, 2, 4),
 n_blocks=2):
 super().__init__()

 # æ—¶é—´åµŒå…¥
 self.time_emb = nn.Sequential(
 SinusoidalPositionalEmbedding(n_channels),
 nn.Linear(n_channels, n_channels * 4),
 nn.GELU(),
 nn.Linear(n_channels * 4, n_channels * 4)
 )

 # è¾“å…¥å±‚
 self.conv_in = nn.Conv2d(image_channels, n_channels, 3, padding=1)

 # ä¸‹é‡‡æ ·
 self.downs = nn.ModuleList()
 chs = [n_channels]
 now_ch = n_channels

 for i, mult in enumerate(ch_mults):
 out_ch = n_channels * mult
 for _ in range(n_blocks):
 self.downs.append(ResidualBlock(now_ch, out_ch, n_channels * 4))
 now_ch = out_ch
 chs.append(now_ch)

 if i  0:
 self.ups.append(nn.ConvTranspose2d(now_ch, now_ch, 4, stride=2, padding=1))

 # è¾“å‡ºå±‚
 self.conv_out = nn.Sequential(
 nn.GroupNorm(8, now_ch),
 nn.SiLU(),
 nn.Conv2d(now_ch, image_channels, 3, padding=1)
 )

 def forward(self, x, t):
 # è·å–æ—¶é—´åµŒå…¥
 t = self.time_emb(t)

 # åˆå§‹å·ç§¯
 h = self.conv_in(x)

 # ä¸‹é‡‡æ ·
 hs = [h]
 for layer in self.downs:
 if isinstance(layer, ResidualBlock):
 h = layer(h, t)
 else:
 h = layer(h)
 hs.append(h)

 # ä¸­é—´å±‚
 for layer in self.middle:
 h = layer(h, t)

 # ä¸Šé‡‡æ ·
 for layer in self.ups:
 if isinstance(layer, ResidualBlock):
 h = layer(torch.cat([h, hs.pop()], dim=1), t)
 else:
 h = layer(h)

 # è¾“å‡º
 return self.conv_out(h)




ç»ƒä¹  3.6.1ï¼šæ¨¡å‹å‚æ•°è®¡ç®—
 å®ç°ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—U-Netæ¨¡å‹çš„å‚æ•°é‡ï¼Œå¹¶æ¯”è¾ƒä¸åŒé…ç½®çš„æ¨¡å‹å¤§å°ã€‚

def count_parameters(model):
 """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
 return sum(p.numel() for p in model.parameters() if p.requires_grad)

def compare_model_sizes():
 """æ¯”è¾ƒä¸åŒæ¨¡å‹é…ç½®çš„å‚æ•°é‡"""
 configs = [
 {"name": "Tiny", "n_channels": 16, "ch_mults": (1, 2, 2)},
 {"name": "Small", "n_channels": 32, "ch_mults": (1, 2, 2, 4)},
 {"name": "Base", "n_channels": 64, "ch_mults": (1, 2, 4, 8)},
 {"name": "Large", "n_channels": 128, "ch_mults": (1, 2, 4, 8)}
 ]

 for config in configs:
 model = SimpleDDPMUNet(
 n_channels=config["n_channels"],
 ch_mults=config["ch_mults"]
 )
 params = count_parameters(model)
 print(f"{config['name']}: {params:,} parameters ({params/1e6:.2f}M)")

# è¾“å‡ºç¤ºä¾‹ï¼š
# Tiny: 461,729 parameters (0.46M)
# Small: 3,652,481 parameters (3.65M)
# Base: 35,742,785 parameters (35.74M)
# Large: 142,836,097 parameters (142.84M)





### 3.6.2 è®­ç»ƒå¾ªç¯

 ç°åœ¨è®©æˆ‘ä»¬å®ç°å®Œæ•´çš„DDPMè®­ç»ƒå¾ªç¯ã€‚è¿™ä¸ªå®ç°åŒ…å«äº†å‰é¢ç« èŠ‚ä»‹ç»çš„æ‰€æœ‰å…³é”®ç»„ä»¶ã€‚



import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np

class DDPMTrainer:
 """DDPMè®­ç»ƒå™¨"""
 def __init__(self, model, device='cuda', num_timesteps=1000,
 beta_start=1e-4, beta_end=0.02, loss_type='l2'):
 self.model = model.to(device)
 self.device = device
 self.num_timesteps = num_timesteps
 self.loss_type = loss_type

 # è®¾ç½®å™ªå£°è°ƒåº¦
 self.betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)
 self.alphas = 1 - self.betas
 self.alphas_bar = torch.cumprod(self.alphas, dim=0)
 self.sqrt_alphas_bar = torch.sqrt(self.alphas_bar)
 self.sqrt_one_minus_alphas_bar = torch.sqrt(1 - self.alphas_bar)

 # ç”¨äºé‡‡æ ·çš„é¢„è®¡ç®—å€¼
 self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)
 self.sqrt_alphas_bar_prev = torch.sqrt(
 torch.cat([torch.tensor([1.0]).to(device), self.alphas_bar[:-1]])
 )
 self.sqrt_one_minus_alphas_bar_prev = torch.sqrt(
 1 - torch.cat([torch.tensor([1.0]).to(device), self.alphas_bar[:-1]])
 )
 self.posterior_variance = self.betas * (1.0 - self.alphas_bar_prev) / (1.0 - self.alphas_bar)

 def forward_diffusion(self, x_0, t, noise=None):
 """å‰å‘æ‰©æ•£è¿‡ç¨‹"""
 if noise is None:
 noise = torch.randn_like(x_0)

 sqrt_alphas_bar_t = self.sqrt_alphas_bar[t].view(-1, 1, 1, 1)
 sqrt_one_minus_alphas_bar_t = self.sqrt_one_minus_alphas_bar[t].view(-1, 1, 1, 1)

 x_t = sqrt_alphas_bar_t * x_0 + sqrt_one_minus_alphas_bar_t * noise
 return x_t, noise

 def compute_loss(self, x_0, t):
 """è®¡ç®—è®­ç»ƒæŸå¤±"""
 noise = torch.randn_like(x_0)
 x_t, _ = self.forward_diffusion(x_0, t, noise)
 noise_pred = self.model(x_t, t)

 if self.loss_type == 'l2':
 loss = torch.nn.functional.mse_loss(noise_pred, noise)
 elif self.loss_type == 'l1':
 loss = torch.nn.functional.l1_loss(noise_pred, noise)
 else:
 raise ValueError(f"Unknown loss type: {self.loss_type}")

 return loss

 def train_step(self, batch, optimizer):
 """å•æ­¥è®­ç»ƒ"""
 x_0 = batch[0].to(self.device)
 batch_size = x_0.shape[0]

 # éšæœºé‡‡æ ·æ—¶é—´æ­¥
 t = torch.randint(0, self.num_timesteps, (batch_size,), device=self.device)

 # è®¡ç®—æŸå¤±
 loss = self.compute_loss(x_0, t)

 # åå‘ä¼ æ’­
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()

 return loss.item()

 @torch.no_grad()
 def sample(self, num_samples, image_size=(1, 28, 28), return_trajectory=False):
 """DDPMé‡‡æ ·"""
 self.model.eval()

 # ä»çº¯å™ªå£°å¼€å§‹
 x_t = torch.randn(num_samples, *image_size, device=self.device)

 trajectory = [x_t.cpu()] if return_trajectory else None

 # é€æ­¥å»å™ª
 for t in tqdm(reversed(range(self.num_timesteps)), desc="Sampling"):
 t_batch = torch.full((num_samples,), t, device=self.device, dtype=torch.long)

 # é¢„æµ‹å™ªå£°
 noise_pred = self.model(x_t, t_batch)

 # è®¡ç®—å‡å€¼
 beta_t = self.betas[t]
 sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alphas_bar[t]
 sqrt_recip_alpha_t = self.sqrt_recip_alphas[t]

 mean = sqrt_recip_alpha_t * (
 x_t - beta_t / sqrt_one_minus_alpha_bar_t * noise_pred
 )

 # æ·»åŠ å™ªå£°ï¼ˆé™¤äº†æœ€åä¸€æ­¥ï¼‰
 if t > 0:
 noise = torch.randn_like(x_t)
 posterior_variance_t = self.posterior_variance[t]
 x_t = mean + torch.sqrt(posterior_variance_t) * noise
 else:
 x_t = mean

 if return_trajectory and t % 100 == 0:
 trajectory.append(x_t.cpu())

 self.model.train()

 if return_trajectory:
 return x_t, trajectory
 return x_t

def train_ddpm(model, train_loader, num_epochs=100, lr=2e-4,
 device='cuda', save_interval=10):
 """å®Œæ•´çš„DDPMè®­ç»ƒæµç¨‹"""
 trainer = DDPMTrainer(model, device=device)
 optimizer = optim.Adam(model.parameters(), lr=lr)

 # è®­ç»ƒå†å²
 losses = []

 for epoch in range(num_epochs):
 epoch_losses = []
 pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

 for batch in pbar:
 loss = trainer.train_step(batch, optimizer)
 epoch_losses.append(loss)
 pbar.set_postfix({'loss': f"{loss:.4f}"})

 avg_loss = np.mean(epoch_losses)
 losses.append(avg_loss)
 print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")

 # å®šæœŸç”Ÿæˆæ ·æœ¬
 if (epoch + 1) % save_interval == 0:
 samples = trainer.sample(16)
 save_samples(samples, epoch + 1)

 # ä¿å­˜æ£€æŸ¥ç‚¹
 torch.save({
 'epoch': epoch,
 'model_state_dict': model.state_dict(),
 'optimizer_state_dict': optimizer.state_dict(),
 'loss': avg_loss,
 }, f'ddpm_checkpoint_epoch_{epoch+1}.pt')

 return trainer, losses

def save_samples(samples, epoch, save_dir='./samples'):
 """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬"""
 import os
 os.makedirs(save_dir, exist_ok=True)

 # ä¿å­˜ä¸ºPyTorchå¼ é‡æ ¼å¼
 torch.save(samples, os.path.join(save_dir, f'samples_epoch_{epoch}.pt'))

 # å¯é€‰ï¼šä¿å­˜ä¸ºå•ç‹¬çš„å›¾åƒæ–‡ä»¶
 if samples.shape[1] == 1: # å•é€šé“å›¾åƒ
 from torchvision.utils import save_image
 # å°†å€¼åŸŸä»[-1, 1]æ˜ å°„åˆ°[0, 1]
 samples_normalized = (samples + 1) / 2
 save_image(samples_normalized,
 os.path.join(save_dir, f'grid_epoch_{epoch}.png'),
 nrow=4, normalize=False)

 print(f"å·²ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ° {save_dir}")



#### ä½¿ç”¨ç¤ºä¾‹ï¼šåœ¨MNISTä¸Šè®­ç»ƒDDPM



```python
# å‡†å¤‡æ•°æ®é›†
transform = transforms.Compose([
 transforms.ToTensor(),
 transforms.Normalize((0.5,), (0.5,)) # å½’ä¸€åŒ–åˆ°[-1, 1]
])

train_dataset = datasets.MNIST(root='./data', train=True,
 download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128,
 shuffle=True, num_workers=4)

# åˆ›å»ºæ¨¡å‹
model = SimpleDDPMUNet(
 image_channels=1,
 n_channels=32,
 ch_mults=(1, 2, 2, 4),
 n_blocks=2
)

# è®­ç»ƒæ¨¡å‹
trainer, losses = train_ddpm(
 model=model,
 train_loader=train_loader,
 num_epochs=50,
 lr=2e-4,
 device='cuda' if torch.cuda.is_available() else 'cpu',
 save_interval=10
)

# ç”Ÿæˆæ–°æ ·æœ¬
new_samples = trainer.sample(64, image_size=(1, 28, 28))

# åˆ†æè®­ç»ƒæŸå¤±
print("è®­ç»ƒæŸå¤±ç»Ÿè®¡:")
print(f" åˆå§‹æŸå¤±: {losses[0]:.4f}")
print(f" æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
print(f" æœ€ä½æŸå¤±: {min(losses):.4f} (Epoch {losses.index(min(losses)) + 1})")
print(f" æŸå¤±ä¸‹é™: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%")
```




 è®­ç»ƒæŠ€å·§


 - **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨ä½™å¼¦é€€ç«æˆ–çº¿æ€§è¡°å‡å¯ä»¥æå‡è®­ç»ƒç¨³å®šæ€§
 - **EMA**ï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰å¯ä»¥è·å¾—æ›´ç¨³å®šçš„ç”Ÿæˆè´¨é‡
 - **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒåˆæœŸ
 - **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨FP16å¯ä»¥åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨





#### é«˜çº§è®­ç»ƒæŠ€æœ¯



```python
class EMA:
 """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
 def __init__(self, model, decay=0.995):
 self.model = model
 self.decay = decay
 self.shadow = {}
 self.backup = {}
 self.register()

 def register(self):
 for name, param in self.model.named_parameters():
 if param.requires_grad:
 self.shadow[name] = param.data.clone()

 def update(self):
 for name, param in self.model.named_parameters():
 if param.requires_grad:
 self.shadow[name] = self.decay * self.shadow[name] + \
 (1 - self.decay) * param.data

 def apply_shadow(self):
 for name, param in self.model.named_parameters():
 if param.requires_grad:
 self.backup[name] = param.data
 param.data = self.shadow[name]

 def restore(self):
 for name, param in self.model.named_parameters():
 if param.requires_grad:
 param.data = self.backup[name]
 self.backup = {}

def train_ddpm_with_ema(model, train_loader, num_epochs=100):
 """å¸¦EMAçš„DDPMè®­ç»ƒ"""
 trainer = DDPMTrainer(model)
 optimizer = optim.Adam(model.parameters(), lr=2e-4)
 scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
 ema = EMA(model)

 scaler = torch.cuda.amp.GradScaler() # æ··åˆç²¾åº¦è®­ç»ƒ

 for epoch in range(num_epochs):
 for batch in train_loader:
 x_0 = batch[0].to(trainer.device)
 batch_size = x_0.shape[0]
 t = torch.randint(0, trainer.num_timesteps, (batch_size,),
 device=trainer.device)

 # æ··åˆç²¾åº¦è®­ç»ƒ
 with torch.cuda.amp.autocast():
 loss = trainer.compute_loss(x_0, t)

 optimizer.zero_grad()
 scaler.scale(loss).backward()

 # æ¢¯åº¦è£å‰ª
 scaler.unscale_(optimizer)
 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

 scaler.step(optimizer)
 scaler.update()

 # æ›´æ–°EMA
 ema.update()

 scheduler.step()

 # ä½¿ç”¨EMAæƒé‡ç”Ÿæˆæ ·æœ¬
 if (epoch + 1) % 10 == 0:
 ema.apply_shadow()
 samples = trainer.sample(16)
 save_samples(samples, epoch + 1)
 ema.restore()

 return trainer, ema
```





ç»ƒä¹  3.6.2ï¼šå®ç°å­¦ä¹ ç‡é¢„çƒ­
 ä¿®æ”¹è®­ç»ƒä»£ç ï¼Œæ·»åŠ å­¦ä¹ ç‡é¢„çƒ­ï¼ˆwarmupï¼‰åŠŸèƒ½ï¼Œåœ¨è®­ç»ƒåˆæœŸé€æ¸å¢åŠ å­¦ä¹ ç‡ã€‚

class WarmupCosineScheduler(optim.lr_scheduler._LRScheduler):
 """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
 def __init__(self, optimizer, warmup_epochs, total_epochs,
 warmup_lr=1e-5, base_lr=2e-4, min_lr=1e-6):
 self.warmup_epochs = warmup_epochs
 self.total_epochs = total_epochs
 self.warmup_lr = warmup_lr
 self.base_lr = base_lr
 self.min_lr = min_lr
 super().__init__(optimizer)

 def get_lr(self):
 if self.last_epoch





### 3.6.3 è¯„ä¼°ä¸å¯è§†åŒ–

 è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„è´¨é‡æ˜¯ä¸€ä¸ªé‡è¦ä½†å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬èŠ‚ä»‹ç»å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–æ–¹æ³•ã€‚


#### å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡


 ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŒ‡æ ‡


 - **FID (FrÃ©chet Inception Distance)**ï¼šè¡¡é‡ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„è·ç¦»
 - **IS (Inception Score)**ï¼šè¯„ä¼°ç”Ÿæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§
 - **LPIPS**ï¼šæ„ŸçŸ¥ç›¸ä¼¼åº¦ï¼Œæ›´ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥
 - **Precision/Recall**ï¼šåˆ†åˆ«è¡¡é‡è´¨é‡å’Œè¦†ç›–åº¦





#### FIDè®¡ç®—å®ç°



import torch
import numpy as np
from scipy import linalg
from torchvision.models import inception_v3
from torch.nn.functional import adaptive_avg_pool2d

class FIDCalculator:
 """FID (FrÃ©chet Inception Distance) è®¡ç®—å™¨"""
 def __init__(self, device='cuda'):
 self.device = device
 self.inception = inception_v3(pretrained=True, transform_input=False).to(device)
 self.inception.eval()
 # ç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚
 self.inception.fc = torch.nn.Identity()

 @torch.no_grad()
 def extract_features(self, images):
 """æå–Inceptionç‰¹å¾"""
 # ç¡®ä¿å›¾åƒå¤§å°è‡³å°‘ä¸º299x299ï¼ˆInception-v3è¦æ±‚ï¼‰
 if images.shape[2] 



#### Inception Scoreå®ç°



```python
class InceptionScore:
 """Inception Scoreè®¡ç®—å™¨"""
 def __init__(self, device='cuda'):
 self.device = device
 self.inception = inception_v3(pretrained=True, transform_input=False).to(device)
 self.inception.eval()

 @torch.no_grad()
 def compute_is(self, images, batch_size=32, splits=10):
 """è®¡ç®—Inception Score

 Args:
 images: ç”Ÿæˆçš„å›¾åƒå¼ é‡
 batch_size: æ‰¹å¤„ç†å¤§å°
 splits: ç”¨äºè®¡ç®—ISçš„åˆ†å‰²æ•°

 Returns:
 is_mean: ISå‡å€¼
 is_std: ISæ ‡å‡†å·®
 """
 # è·å–é¢„æµ‹
 preds = []
 for i in range(0, len(images), batch_size):
 batch = images[i:i+batch_size].to(self.device)

 # è°ƒæ•´å¤§å°å’Œé€šé“
 if batch.shape[2] = num_samples:
 break
 real_samples = torch.cat(real_samples, dim=0)[:num_samples]

 # è®¡ç®—FID
 print("è®¡ç®—FID...")
 fid_score = self.fid_calculator.compute_fid_from_samples(
 real_samples, generated_samples, batch_size=batch_size
 )

 # è®¡ç®—IS
 print("è®¡ç®—Inception Score...")
 is_mean, is_std = self.is_calculator.compute_is(
 generated_samples, batch_size=batch_size
 )

 # è®¡ç®—æ ·æœ¬å¤šæ ·æ€§
 diversity = self.compute_diversity(generated_samples)

 results = {
 'fid': fid_score,
 'is_mean': is_mean,
 'is_std': is_std,
 'diversity': diversity
 }

 return results, generated_samples

 def compute_diversity(self, samples):
 """è®¡ç®—æ ·æœ¬å¤šæ ·æ€§ï¼ˆä½¿ç”¨LPIPSæˆ–ç®€å•çš„L2è·ç¦»ï¼‰"""
 # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨L2è·ç¦»
 n_samples = min(1000, len(samples))
 indices = torch.randperm(len(samples))[:n_samples]
 subset = samples[indices]

 # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„L2è·ç¦»
 distances = []
 for i in range(n_samples):
 for j in range(i+1, n_samples):
 dist = torch.norm(subset[i] - subset[j], p=2)
 distances.append(dist.item())

 return np.mean(distances)

 def save_results(self, results, samples, save_path='evaluation_results.pt'):
 """ä¿å­˜è¯„ä¼°ç»“æœ"""
 # ä¿å­˜æ ·æœ¬å’Œè¯„ä¼°æŒ‡æ ‡
 torch.save({
 'samples': samples,
 'metrics': results,
 'timestamp': np.datetime64('now')
 }, save_path)

 # æ‰“å°è¯„ä¼°æŠ¥å‘Š
 print("\n" + "="*50)
 print("è¯„ä¼°ç»“æœæŠ¥å‘Š")
 print("="*50)
 print(f"FID Score: {results['fid']:.2f} (è¶Šä½è¶Šå¥½)")
 print(f"Inception Score: {results['is_mean']:.2f} Â± {results['is_std']:.2f} (è¶Šé«˜è¶Šå¥½)")
 print(f"Diversity Score: {results['diversity']:.4f} (è¶Šé«˜è¶Šå¥½)")
 print(f"\nç»“æœå·²ä¿å­˜åˆ°: {save_path}")

 return results
```




#### ä½¿ç”¨ç¤ºä¾‹



```python
# åˆ›å»ºè¯„ä¼°å™¨
evaluator = DDPMEvaluator(trainer, test_loader)

# è¿è¡Œå®Œæ•´è¯„ä¼°
results, generated_samples = evaluator.evaluate(num_samples=5000)

# æ‰“å°ç»“æœ
print(f"FID Score: {results['fid']:.2f}")
print(f"Inception Score: {results['is_mean']:.2f} Â± {results['is_std']:.2f}")
print(f"Diversity Score: {results['diversity']:.4f}")

# ä¿å­˜ç»“æœ
evaluator.save_results(results, generated_samples)

# åˆ†æé‡‡æ ·è½¨è¿¹
def analyze_sampling_trajectory(trainer, num_steps_show=10):
 """åˆ†æé‡‡æ ·è½¨è¿¹"""
 # ç”Ÿæˆå¸¦è½¨è¿¹çš„æ ·æœ¬
 samples, trajectory = trainer.sample(4, return_trajectory=True)

 # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ­¥éª¤
 total_steps = len(trajectory)
 step_indices = np.linspace(0, total_steps-1, num_steps_show, dtype=int)

 print("\né‡‡æ ·è½¨è¿¹åˆ†æ:")
 print("="*50)
 print(f"æ€»æ­¥æ•°: {trainer.num_timesteps}")
 print(f"è½¨è¿¹é‡‡æ ·ç‚¹: {len(step_indices)}")

 # åˆ†ææ¯ä¸ªé˜¶æ®µçš„ç»Ÿè®¡ç‰¹æ€§
 for i, step_idx in enumerate(step_indices):
 t = trainer.num_timesteps - step_idx * 100 if step_idx > 0 else trainer.num_timesteps
 img_batch = trajectory[step_idx]

 stats = {
 'mean': img_batch.mean().item(),
 'std': img_batch.std().item(),
 'min': img_batch.min().item(),
 'max': img_batch.max().item()
 }

 print(f"\næ­¥éª¤ {i+1}/{num_steps_show} (t={t}):")
 print(f" å‡å€¼: {stats['mean']:6.3f}, æ ‡å‡†å·®: {stats['std']:6.3f}")
 print(f" èŒƒå›´: [{stats['min']:6.3f}, {stats['max']:6.3f}]")

 return samples, trajectory

# åˆ†æé‡‡æ ·è½¨è¿¹
final_samples, full_trajectory = analyze_sampling_trajectory(trainer)
```





ç»ƒä¹  3.6.3ï¼šå®ç°Precisionå’ŒRecallæŒ‡æ ‡
 å®ç°æ”¹è¿›çš„Precisionå’ŒRecallæŒ‡æ ‡ï¼Œåˆ†åˆ«è¡¡é‡ç”Ÿæˆè´¨é‡å’Œæ¨¡å¼è¦†ç›–åº¦ã€‚

def compute_precision_recall(real_features, gen_features, k=3):
 """è®¡ç®—æ”¹è¿›çš„Precisionå’ŒRecall

 åŸºäºk-æœ€è¿‘é‚»çš„æ–¹æ³•ï¼š
 - Precision: ç”Ÿæˆæ ·æœ¬ä¸­æœ‰å¤šå°‘è½åœ¨çœŸå®æ•°æ®çš„æ”¯æ’‘é›†å†…
 - Recall: çœŸå®æ•°æ®çš„æ”¯æ’‘é›†æœ‰å¤šå°‘è¢«ç”Ÿæˆæ ·æœ¬è¦†ç›–
 """
 from sklearn.neighbors import NearestNeighbors

 # æ„å»ºk-NNæ¨¡å‹
 nbrs_real = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(real_features)
 nbrs_gen = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(gen_features)

 # è®¡ç®—çœŸå®æ ·æœ¬çš„k-NNè·ç¦»
 distances_real, _ = nbrs_real.kneighbors(real_features)
 distances_real = distances_real[:, -1] # ç¬¬kä¸ªæœ€è¿‘é‚»çš„è·ç¦»

 # è®¡ç®—ç”Ÿæˆæ ·æœ¬çš„k-NNè·ç¦»
 distances_gen, _ = nbrs_gen.kneighbors(gen_features)
 distances_gen = distances_gen[:, -1]

 # è®¡ç®—Precisionï¼šç”Ÿæˆæ ·æœ¬åˆ°çœŸå®æµå½¢çš„è·ç¦»
 distances_gen_to_real, _ = nbrs_real.kneighbors(gen_features, n_neighbors=1)
 distances_gen_to_real = distances_gen_to_real[:, 0]
 precision = np.mean(distances_gen_to_real





## 3.7 DDPMçš„å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘

 è™½ç„¶DDPMåœ¨ç”Ÿæˆè´¨é‡ä¸Šå–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å®ƒä»å­˜åœ¨ä¸€äº›é‡è¦çš„å±€é™æ€§ã€‚ç†è§£è¿™äº›å±€é™æ€§æœ‰åŠ©äºæˆ‘ä»¬ç†è§£åç»­çš„æ”¹è¿›æ–¹æ³•ã€‚


### 3.7.1 ä¸»è¦å±€é™æ€§



 DDPMçš„æ ¸å¿ƒé—®é¢˜


 - **é‡‡æ ·é€Ÿåº¦æ…¢**


 éœ€è¦1000æ­¥è¿­ä»£æ‰èƒ½ç”Ÿæˆä¸€å¼ å›¾åƒ
 - ç›¸æ¯”GANçš„å•æ¬¡å‰å‘ä¼ æ’­ï¼Œæ•ˆç‡å·®è·å·¨å¤§
 - é™åˆ¶äº†å®æ—¶åº”ç”¨çš„å¯èƒ½æ€§


 
 - **å›ºå®šçš„å™ªå£°è°ƒåº¦**


 çº¿æ€§Î²è°ƒåº¦å¹¶éæœ€ä¼˜
 - ä¸åŒæ•°æ®é›†å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒåº¦ç­–ç•¥
 - è®­ç»ƒå’Œé‡‡æ ·å¿…é¡»ä½¿ç”¨ç›¸åŒçš„è°ƒåº¦


 
 - **å›ºå®šçš„åéªŒæ–¹å·®**


 DDPMä½¿ç”¨å›ºå®šçš„åéªŒæ–¹å·® $\sigma_t^2 = \beta_t$
 - è¿™å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
 - é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›


 
 - **è®¡ç®—èµ„æºéœ€æ±‚é«˜**


 è®­ç»ƒéœ€è¦å¤§é‡GPUæ—¶é—´
 - æ¨ç†æ—¶çš„å†…å­˜å ç”¨è¾ƒå¤§
 - éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²


 





### 3.7.2 æ€§èƒ½åˆ†æ




def analyze_ddpm_performance(trainer, num_samples=100):
 """åˆ†æDDPMçš„æ€§èƒ½ç“¶é¢ˆ"""
 import time

 results = {
 'sampling_times': [],
 'memory_usage': [],
 'step_times': []
 }

 # æµ‹è¯•ä¸åŒæ­¥æ•°çš„é‡‡æ ·æ—¶é—´
 for num_steps in [10, 50, 100, 500, 1000]:
 # ä¿®æ”¹é‡‡æ ·æ­¥æ•°
 original_steps = trainer.num_timesteps
 trainer.num_timesteps = num_steps

 # è®¡æ—¶
 start_time = time.time()
 samples = trainer.sample(num_samples, image_size=(1, 28, 28))
 end_time = time.time()

 sampling_time = end_time - start_time
 results['sampling_times'].append({
 'steps': num_steps,
 'total_time': sampling_time,
 'time_per_sample': sampling_time / num_samples,
 'time_per_step': sampling_time / (num_samples * num_steps)
 })

 trainer.num_timesteps = original_steps

 # åˆ†ææ¯æ­¥çš„æ—¶é—´åˆ†å¸ƒ
 with torch.profiler.profile(
 activities=[torch.profiler.ProfilerActivity.CPU,
 torch.profiler.ProfilerActivity.CUDA],
 record_shapes=True
 ) as prof:
 trainer.sample(1, image_size=(1, 28, 28))

 # æ‰“å°åˆ†æç»“æœ
 print("=== DDPM Performance Analysis ===")
 print(f"\nSampling Time vs Steps:")
 for result in results['sampling_times']:
 print(f"Steps: {result['steps']:4d} | "
 f"Total: {result['total_time']:6.2f}s | "
 f"Per Sample: {result['time_per_sample']:6.4f}s | "
 f"Per Step: {result['time_per_step']*1000:6.2f}ms")

 print(f"\nTop operations by time:")
 print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

 return results

# åˆ†ææ€§èƒ½ç»“æœ
def analyze_performance_results(results):
 """åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ"""
 steps = [r['steps'] for r in results['sampling_times']]
 times = [r['time_per_sample'] for r in results['sampling_times']]

 print("\næ€§èƒ½åˆ†ææŠ¥å‘Š:")
 print("="*60)
 print("æ­¥æ•° | æ¯æ ·æœ¬æ—¶é—´(s) | ç›¸å¯¹1000æ­¥åŠ é€Ÿæ¯” | è´¨é‡å½±å“")
 print("-"*60)

 baseline_time = times[-1] # 1000æ­¥çš„æ—¶é—´
 for i, (step, time) in enumerate(zip(steps, times)):
 speedup = baseline_time / time
 quality_impact = "é«˜" if step >= 500 else ("ä¸­" if step >= 100 else "ä½")
 print(f"{step:8d} | {time:13.4f} | {speedup:15.1f}x | {quality_impact}")

 print("\nå…³é”®å‘ç°:")
 print(f"- ä»1000æ­¥å‡å°‘åˆ°50æ­¥å¯è·å¾— {baseline_time/times[1]:.1f}x åŠ é€Ÿ")
 print(f"- æ¯æ­¥å¹³å‡è€—æ—¶: {times[-1]/1000*1000:.2f}ms")
 print(f"- ä¸»è¦ç“¶é¢ˆ: U-Netå‰å‘ä¼ æ’­")

 return results



### 3.7.3 æ”¹è¿›æ–¹å‘æ¦‚è§ˆ



 ä¸»è¦æ”¹è¿›æ–¹å‘
 
 
 
 é—®é¢˜
 æ”¹è¿›æ–¹æ³•
 å…³é”®æ€æƒ³
 ç›¸å…³ç« èŠ‚
 
 
 
 
 é‡‡æ ·é€Ÿåº¦æ…¢
 DDIM
 ç¡®å®šæ€§é‡‡æ ·ï¼Œè·³æ­¥
 ç¬¬8ç« 
 
 
 é‡‡æ ·é€Ÿåº¦æ…¢
 DPM-Solver
 é«˜é˜¶ODEæ±‚è§£å™¨
 ç¬¬8ç« 
 
 
 å›ºå®šå™ªå£°è°ƒåº¦
 Improved DDPM
 ä½™å¼¦è°ƒåº¦ï¼Œå­¦ä¹ æ–¹å·®
 ç¬¬8ç« 
 
 
 ç†è®ºæ¡†æ¶
 Score-based Models
 åˆ†æ•°åŒ¹é…è§†è§’
 ç¬¬4ç« 
 
 
 è¿ç»­æ—¶é—´
 SDE/ODE
 è¿ç»­æ—¶é—´æ¡†æ¶
 ç¬¬5ç« 
 
 
 è®¡ç®—æ•ˆç‡
 Latent Diffusion
 æ½œåœ¨ç©ºé—´æ‰©æ•£
 ç¬¬10ç« 
 
 
 ä¸€æ­¥ç”Ÿæˆ
 Consistency Models
 è‡ªä¸€è‡´æ€§æ˜ å°„
 ç¬¬14ç« 
 
 
 



### 3.7.4 å®éªŒï¼šä¸åŒæ”¹è¿›çš„æ•ˆæœ




```python
class ImprovedDDPMExperiments:
 """å®éªŒä¸åŒçš„DDPMæ”¹è¿›æ–¹æ³•"""

 @staticmethod
 def cosine_beta_schedule(num_timesteps, s=0.008):
 """ä½™å¼¦å™ªå£°è°ƒåº¦ï¼ˆImproved DDPMï¼‰"""
 steps = num_timesteps + 1
 x = torch.linspace(0, num_timesteps, steps)
 alphas_bar = torch.cos(((x / num_timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
 alphas_bar = alphas_bar / alphas_bar[0]
 betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])
 return torch.clip(betas, 0.0001, 0.9999)

 @staticmethod
 def learned_variance_output(model_output, num_channels):
 """å­¦ä¹ æ–¹å·®çš„æ¨¡å‹è¾“å‡ºï¼ˆImproved DDPMï¼‰"""
 # æ¨¡å‹è¾“å‡ºä¸¤å€é€šé“ï¼šå‰åŠéƒ¨åˆ†æ˜¯å‡å€¼ï¼ŒååŠéƒ¨åˆ†æ˜¯å¯¹æ•°æ–¹å·®
 mean, log_variance = torch.split(model_output, num_channels, dim=1)

 # å‚æ•°åŒ–å¯¹æ•°æ–¹å·®åœ¨[beta_t, beta_tilde_t]ä¹‹é—´
 # log_variance = log(beta_t) + v * log(beta_tilde_t / beta_t)
 # å…¶ä¸­væ˜¯æ¨¡å‹é¢„æµ‹çš„æ’å€¼å‚æ•°
 return mean, log_variance

 @staticmethod
 def ddim_sampling_step(x_t, epsilon_pred, t, t_prev, alphas_bar, eta=0):
 """DDIMé‡‡æ ·æ­¥éª¤ï¼ˆå¯è°ƒèŠ‚éšæœºæ€§ï¼‰"""
 alpha_bar_t = alphas_bar[t]
 alpha_bar_t_prev = alphas_bar[t_prev] if t_prev >= 0 else 1.0

 # è®¡ç®—x_0çš„é¢„æµ‹
 x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)

 # è®¡ç®—æ–¹å·®
 sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t)) * \
 torch.sqrt(1 - alpha_bar_t / alpha_bar_t_prev)

 # é¢„æµ‹x_{t-1}
 mean = torch.sqrt(alpha_bar_t_prev) * x_0_pred + \
 torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * epsilon_pred

 if t_prev > 0:
 noise = torch.randn_like(x_t)
 x_t_prev = mean + sigma_t * noise
 else:
 x_t_prev = mean

 return x_t_prev

 @staticmethod
 def compare_sampling_methods(model, device='cuda'):
 """æ¯”è¾ƒä¸åŒé‡‡æ ·æ–¹æ³•çš„æ•ˆæœ"""
 results = {}

 # æ ‡å‡†DDPMé‡‡æ ·
 print("Testing standard DDPM sampling...")
 start_time = time.time()
 ddpm_samples = standard_ddpm_sample(model, num_samples=16, num_steps=1000)
 ddpm_time = time.time() - start_time
 results['ddpm'] = {'samples': ddpm_samples, 'time': ddpm_time}

 # DDIMé‡‡æ ·ï¼ˆ50æ­¥ï¼‰
 print("Testing DDIM sampling (50 steps)...")
 start_time = time.time()
 ddim_samples = ddim_sample(model, num_samples=16, num_steps=50, eta=0)
 ddim_time = time.time() - start_time
 results['ddim'] = {'samples': ddim_samples, 'time': ddim_time}

 # å¸¦éšæœºæ€§çš„DDIMé‡‡æ ·
 print("Testing stochastic DDIM (eta=0.5)...")
 start_time = time.time()
 stochastic_samples = ddim_sample(model, num_samples=16, num_steps=50, eta=0.5)
 stochastic_time = time.time() - start_time
 results['stochastic'] = {'samples': stochastic_samples, 'time': stochastic_time}

 return results
```





ç»ƒä¹  3.7ï¼šå®ç°ç®€åŒ–ç‰ˆDDIM
 åŸºäºæœ¬ç« å­¦åˆ°çš„DDPMçŸ¥è¯†ï¼Œå®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„DDIMé‡‡æ ·å™¨ï¼Œæ”¯æŒå¯å˜æ­¥æ•°é‡‡æ ·ã€‚

@torch.no_grad()
def simplified_ddim_sample(model, shape, num_inference_steps=50,
 num_train_steps=1000, eta=0.0, device='cuda'):
 """ç®€åŒ–ç‰ˆDDIMé‡‡æ ·å®ç°

 Args:
 model: è®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹æ¨¡å‹
 shape: ç”Ÿæˆå›¾åƒçš„å½¢çŠ¶
 num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆ= 0 else 1.0

 # é¢„æµ‹x_0
 x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_pred) / torch.sqrt(alpha_bar_t)
 x_0_pred = torch.clamp(x_0_pred, -1, 1) # æ•°å€¼ç¨³å®šæ€§

 # è®¡ç®—æ–¹å·®
 sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t)) * \
 torch.sqrt(1 - alpha_bar_t / alpha_bar_t_prev) if t_prev >= 0 else 0

 # è®¡ç®—å‡å€¼
 mean = torch.sqrt(alpha_bar_t_prev) * x_0_pred + \
 torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * epsilon_pred

 # æ·»åŠ å™ªå£°
 if t_prev >= 0:
 noise = torch.randn_like(x_t) if eta > 0 else 0
 x_t = mean + sigma_t * noise
 else:
 x_t = mean

 return x_t

# æµ‹è¯•ä¸åŒæ­¥æ•°å’Œetaå€¼
test_configs = [
 {'steps': 10, 'etas': [0.0, 0.3, 0.7, 1.0]},
 {'steps': 25, 'etas': [0.0, 0.3, 0.7, 1.0]},
 {'steps': 50, 'etas': [0.0, 0.3, 0.7, 1.0]}
]

print("DDIMé‡‡æ ·æµ‹è¯•ç»“æœ:")
print("="*60)
print("æ­¥æ•° | Î·å€¼ | é‡‡æ ·æ—¶é—´(s) | ç›¸å¯¹è´¨é‡è¯„ä¼°")
print("-"*60)

for config in test_configs:
 num_steps = config['steps']
 for eta in config['etas']:
 import time
 start = time.time()
 samples = simplified_ddim_sample(
 model, shape=(1, 1, 28, 28),
 num_inference_steps=num_steps,
 eta=eta
 )
 elapsed = time.time() - start

 # ç®€å•çš„è´¨é‡è¯„ä¼°ï¼ˆåŸºäºæ ·æœ¬ç»Ÿè®¡ï¼‰
 quality = "é«˜" if samples.std() > 0.3 else ("ä¸­" if samples.std() > 0.2 else "ä½")

 print(f"{num_steps:4d} | {eta:4.1f} | {elapsed:11.4f} | {quality}")

print("\nå…³é”®è§‚å¯Ÿ:")
print("- Î·=0 (ç¡®å®šæ€§é‡‡æ ·) é€Ÿåº¦æœ€å¿«ï¼Œè´¨é‡ç¨³å®š")
print("- Î·=1 (å®Œå…¨éšæœºï¼Œç­‰åŒäºDDPM) è´¨é‡æœ€é«˜ä½†é€Ÿåº¦æ…¢")
print("- æ­¥æ•°å‡å°‘æ˜¾è‘—æå‡é€Ÿåº¦ï¼Œä½†å¯èƒ½å½±å“è´¨é‡")






## æœ¬ç« å°ç»“

 åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†DDPMï¼ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼‰çš„æ ¸å¿ƒåŸç†å’Œå®ç°ç»†èŠ‚ï¼š



#### ä¸»è¦æ”¶è·



 - **ç†è®ºåŸºç¡€**ï¼šç†è§£äº†å‰å‘æ‰©æ•£è¿‡ç¨‹ã€åå‘å»å™ªè¿‡ç¨‹å’Œå˜åˆ†ä¸‹ç•Œçš„æ¨å¯¼
 - **å®è·µå®ç°**ï¼šæ„å»ºäº†å®Œæ•´çš„DDPMç³»ç»Ÿï¼ŒåŒ…æ‹¬U-Netæ¶æ„ã€è®­ç»ƒå¾ªç¯å’Œé‡‡æ ·ç®—æ³•
 - **è¯„ä¼°æ–¹æ³•**ï¼šå­¦ä¹ äº†FIDã€ISç­‰ç”Ÿæˆæ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—å’Œä½¿ç”¨
 - **å±€é™è®¤è¯†**ï¼šäº†è§£äº†DDPMçš„ä¸»è¦é—®é¢˜ï¼Œä¸ºå­¦ä¹ åç»­æ”¹è¿›æ–¹æ³•æ‰“ä¸‹åŸºç¡€




#### å…³é”®è¦ç‚¹



 - DDPMé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å’Œå­¦ä¹ é€†è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®
 - è®­ç»ƒç›®æ ‡ç®€åŒ–ä¸ºé¢„æµ‹æ¯ä¸€æ­¥æ·»åŠ çš„å™ªå£°
 - é‡‡æ ·è¿‡ç¨‹éœ€è¦å¤šæ­¥è¿­ä»£ï¼Œè¿™æ˜¯ä¸»è¦çš„æ•ˆç‡ç“¶é¢ˆ
 - æ¨¡å‹è´¨é‡é«˜ä½†æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ¨åŠ¨äº†åç»­çš„ä¼—å¤šæ”¹è¿›




#### å±•æœ›


åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ï¼š



 - **ç¬¬4ç« **ï¼šä»åˆ†æ•°åŒ¹é…çš„è§’åº¦é‡æ–°ç†è§£æ‰©æ•£æ¨¡å‹
 - **ç¬¬5ç« **ï¼šè¿ç»­æ—¶é—´æ¡†æ¶ä¸‹çš„SDE/ODEè¡¨è¿°
 - **ç¬¬8ç« **ï¼šDDIMç­‰å¿«é€Ÿé‡‡æ ·æ–¹æ³•çš„åŸç†ä¸å®ç°




DDPMå¥ å®šäº†ç°ä»£æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œç†è§£å®ƒçš„åŸç†å¯¹äºæŒæ¡åç»­çš„é«˜çº§æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç»§ç»­å‰è¿›ï¼Œè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€ç« æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„å¦ä¸€ç§è§†è§’â€”â€”åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹ï¼