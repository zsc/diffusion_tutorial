[← 上一章](chapter2.md) | 第3章 / 共14章 | [下一章 →](chapter4.md)

# 第3章：去噪扩散概率模型 (DDPM)

2020年，Ho等人的论文《Denoising Diffusion Probabilistic Models》是扩散模型发展史上的一个分水岭，它不仅极大地简化了模型的训练过程，更是在多个图像生成基准上达到了与GAN相媲美的生成质量。本章将深入剖析DDPM的数学原理、训练算法和实现细节。通过本章学习，你将掌握DDPM的核心思想，并理解其背后的概率论基础是如何被巧妙地简化为一个优雅的去噪目标的。

## 3.1 DDPM的核心思想：简化与统一

在DDPM之前，扩散模型虽然理论优雅，但实践起来却充满挑战。DDPM的革命性贡献在于：**将复杂的变分推断问题简化为了一个简单直观的去噪任务**。

> **定义：DDPM的三个关键简化**
> 1.  **固定前向过程**：前向加噪过程使用一个预先设定的、固定的方差调度 $\beta_t$，无需学习。
> 2.  **简化反向过程**：假设反向去噪过程也是高斯分布，且其方差也是固定的。因此，模型只需要学习高斯分布的均值。
> 3.  **重参数化目标**：将学习“去噪后的图像均值”这一困难任务，巧妙地转换为学习“添加到图像中的噪声”，极大地稳定了训练过程。

### 3.1.1 为什么预测噪声更好？

这个看似简单的改变是DDPM成功的关键。预测原始图像 $x_0$ 意味着网络需要输出一个具有复杂结构和特定分布的物体，而预测噪声 $\epsilon$ 意味着网络只需要输出一个来自标准正态分布的样本。

> **定义：预测噪声的优势**
> | 方面 | 预测均值 $\mu_\theta$ | 预测噪声 $\epsilon_\theta$ |
> | :--- | :--- | :--- |
> | **输出范围** | 需要匹配数据的复杂分布 | 目标是标准高斯分布（已归一化） |
> | **训练信号** | 随时间步 $t$ 变化剧烈 | 各时间步的训练目标相对一致 |
> | **梯度流** | 在高噪声时可能梯度消失 | 梯度传播更稳定 |
> | **物理意义** | 预测去噪后的图像 | 预测被添加的噪声 |

🔬 **研究线索**：DDPM预测噪声 $\epsilon$，而一些后续工作（如Cold Diffusion）则探索直接预测 $x_0$。这两种参数化方式的优劣在不同场景下仍有争议。例如，在处理视频时，预测帧间差（类似于噪声）可能比预测完整帧更有效。

### 3.1.2 DDPM训练算法概览

得益于上述简化，DDPM的训练过程变得异常简洁。

**DDPM训练伪代码**
1.  从数据集中随机抽取一批原始图像 $x_0$。
2.  为该批次中的每个图像随机选择一个时间步 $t$ (从1到T)。
3.  从标准正态分布中采样一个噪声 $\epsilon$。
4.  使用闭式解计算 $t$ 时刻的噪声图像 $x_t$：$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$。
5.  将 $x_t$ 和 $t$ 输入到神经网络 $\epsilon_\theta$ 中，得到预测的噪声 $\epsilon_{pred}$。
6.  计算损失：$loss = \text{MSE}(\epsilon, \epsilon_{pred})$。
7.  使用梯度下降更新模型参数 $\theta$。

这种端到端的去噪训练方式，是DDPM易于实现和训练稳定的核心原因。

## 3.2 前向过程：从数据到噪声

前向过程定义了一个固定的马尔可夫链，它逐步将数据分布 $q(x_0)$ 转换为一个已知的先验分布（通常是标准正态分布 $\mathcal{N}(0, I)$）。

$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$

### 3.2.1 重参数化技巧

DDPM的一个关键数学技巧是，我们可以直接从 $x_0$ 采样任意时刻的 $x_t$，而无需迭代计算。

> **定理：闭式采样公式**
> 定义 $\alpha_t = 1 - \beta_t$ 和 $\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$，则：
> $q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})$
> 这个公式的证明基于高斯分布的可加性，是一个简单的归纳法练习。它的重要性在于，它使得我们可以在训练中随机采样任意时间步 $t$ 并直接计算出 $x_t$，极大地提高了训练效率。

### 3.2.2 噪声调度 (Noise Schedule)

噪声调度 $ \{\beta_t\} $ 的选择对模型性能有重要影响。

> **定义：调度策略对比**
> | 调度类型 | 特点 | 优势 | 劣势 |
> | :--- | :--- | :--- | :--- |
> | **线性 (Linear)** | $\beta_t$ 线性增长。 | 简单直观，DDPM原始选择。 | 过程早期破坏信息过快，后期变化又太慢。 |
> | **余弦 (Cosine)** | 基于信噪比(SNR)的余弦曲线设计。 | 过程早期缓慢加噪，保留更多结构，感知质量更好。 | 理论相对复杂，末期可能收敛过慢。 |
> | **二次 (Quadratic)** | $\beta_t$ 呈二次方增长。 | 在线性的基础上，进一步减缓早期加噪。 | 后期加噪可能过于激进。 |

💡 **开放问题**：是否存在一个“最优”的噪声调度？理论上，最优调度应与数据的内在属性（如维度、复杂度）相关。目前，设计数据自适应的噪声调度或在训练中学习调度本身，仍然是一个活跃的研究领域。

<details>
<summary><strong>练习 3.1：设计与分析噪声调度</strong></summary>

1.  **S形调度设计**：设计一个“S形”的噪声调度，使得加噪过程满足：a) 前期缓慢；b) 中期快速；c) 后期再次放缓。写出其数学表达式。
2.  **信噪比(SNR)分析**：对于线性和余弦调度，推导并绘制其信噪比 $\text{SNR}(t) = \bar{\alpha}_t / (1 - \bar{\alpha}_t)$ 的对数曲线。从曲线形状解释为什么余弦调度通常能取得更好的生成质量。
3.  **研究思路**：
    *   从信息论的角度出发，将前向过程视为一个信息通道，分析不同调度下的信道容量变化。
    *   探索噪声调度与最优传输理论（Optimal Transport）的联系。前向过程可以看作是从数据分布到噪声分布的一条路径，最优调度是否对应着某种“最短”路径？

</details>

## 3.3 反向过程：从噪声到数据

反向过程是扩散模型的核心——学习如何从纯噪声 $x_T$ 逐步恢复出清晰的数据 $x_0$。

### 3.3.1 反向条件概率的推导

这是DDPM论文中最重要的数学推导之一。通过贝叶斯定理，可以证明在已知 $x_0$ 的条件下，反向的条件概率 $q(x_{t-1}|x_t, x_0)$ 也是一个高斯分布。

> **定理：反向过程的后验分布**
> $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t\mathbf{I})$
> 其中均值 $\tilde{\mu}_t$ 是 $x_t$ 和 $x_0$ 的线性组合，方差 $\tilde{\beta}_t$ 只与预设的 $\beta_t$ 相关。

这个定理的**关键洞察**在于：如果我们能以某种方式从 $x_t$ 中估计出 $x_0$，我们就能近似真实的反向过程。这正是神经网络需要做的事情。

利用 $x_0 = (x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon) / \sqrt{\bar{\alpha}_t}$ 的关系，我们可以将后验均值 $\tilde{\mu}_t$ 完全用 $x_t$ 和噪声 $\epsilon$ 来表达：
$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \boldsymbol{\epsilon}) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)$
这个表达式优雅地揭示了：**学习反向过程等价于学习预测噪声 $\epsilon$**。

### 3.3.2 方差的处理：固定 vs 可学习

DDPM的另一个简化是固定反向过程的方差 $\sigma_t^2$。论文中探讨了两种选择：$\sigma_t^2 = \tilde{\beta}_t$（理论上更优）和 $\sigma_t^2 = \beta_t$（实现上更简单，效果也很好）。后续工作（如Improved DDPM）则探索了让网络同时预测方差，这可以提高对数似然（log-likelihood），但对生成质量的提升通常不明显。对于大多数应用，固定方差是足够的。

<details>
<summary><strong>练习 3.2：参数化的等价性与差异</strong></summary>

1.  **数学推导**：从后验均值 $\tilde{\mu}_t(x_t, x_0)$ 的表达式出发，代入 $x_0$ 与 $x_t, \epsilon$ 的关系式，推导出 $\tilde{\mu}_t(x_t, \epsilon)$ 的表达式，从而证明“预测$x_0$”和“预测$\epsilon$”在数学上是等价的。
2.  **稳定性分析**：从优化的角度，分析为什么预测一个目标为 $\mathcal{N}(0, I)$ 的噪声 $\epsilon$，比预测一个目标为复杂数据分布 $q(x_0)$ 的 $x_0$ 更稳定？（提示：考虑不同时间步 $t$ 下目标函数的尺度和梯度。）
3.  **开放探索**：DDPM选择固定方差。但在某些情况下，让方差可学习可能很重要。设想一个场景（例如，生成具有不同纹理区域的图像），其中自适应的去噪方差可能带来优势，并解释原因。

</details>

## 3.4 训练目标：从变分下界到简单均方误差

DDPM的最终妙笔是将复杂的变分下界（Variational Lower Bound, VLB）损失函数简化为一个简单的均方误差（MSE）。

完整的VLB损失可以写成三项之和：$L_{\text{VLB}} = L_T + \sum_{t>1} L_{t-1} + L_0$。其中 $L_{t-1}$ 是主要的去噪匹配项，可以表示为两个高斯分布（真实后验 $q$ 和模型预测 $p_\theta$）之间的KL散度。通过我们上面的推导，这一项可以简化为对两个均值 $\tilde{\mu}_t$ 和 $\mu_\theta$ 差值的L2损失。

> **🎯 DDPM的简化训练目标**
> Ho等人发现，如果忽略VLB损失中复杂的加权系数，直接优化一个更简单的目标函数，效果反而更好：
> $L_{\text{simple}} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]$
> 这就是DDPM最终的训练目标：**在随机的时间步 $t$，让神经网络 $\epsilon_\theta$ 预测出添加到 $x_0$ 上的原始噪声 $\epsilon$**。

⚡ **实现挑战与研究前沿**：虽然简单损失效果很好，但它对所有时间步 $t$ 的误差一视同仁。后续研究（如Min-SNR-$\gamma$加权策略）表明，对不同 $t$ 的损失进行加权（例如，降低高信噪比、即低噪声区域的损失权重）可以显著提高生成质量，尤其是在高分辨率生成任务中。

## 3.5 采样算法：从理论到实践

训练完成后，我们就可以从随机噪声中生成图像了。采样过程是反向过程的实际执行。

**DDPM标准采样算法**
1.  从标准正态分布中采样一个初始噪声图像 $x_T$。
2.  从 $t = T$ 循环到 $t = 1$：
    a. 将当前的 $x_t$ 和时间步 $t$ 输入模型，得到噪声预测 $\epsilon_\theta(x_t, t)$。
    b. 使用 $\epsilon_\theta$ 和 $x_t$ 计算去噪后的均值 $\mu_\theta(x_t, t)$。
    c. 从标准正态分布中采样一个随机噪声 $z$。
    d. 计算 $x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z$。（其中 $\sigma_t$ 是固定的方差）
3.  最终得到的 $x_0$ 就是生成的图像。

这个迭代过程通常需要1000步，因此速度较慢，这是DDPM的主要缺点之一，也催生了后续大量的快速采样算法研究（将在第8章讨论）。

<details>
<summary><strong>综合练习：DDPM的局限性与改进方向</strong></summary>

DDPM虽然强大，但并非完美。请分析其潜在的局限性，并为每个局限性提出一个可能的研究方向或改进思路。

1.  **局限一：采样速度慢**。标准DDPM需要上千步迭代。
    *   **改进思路**：？（提示：反向过程是否必须是马尔可夫的？）
2.  **局限二：高斯假设**。整个框架基于高斯噪声和高斯转移核。
    *   **改进思路**：？（提示：对于某些具有特定结构噪声的数据，如JPEG压缩伪影，非高斯噪声是否更合适？）
3.  **局限三：固定的前向过程**。前向过程与数据无关。
    *   **改进思路**：？（提示：能否设计一个依赖于数据内容的前向过程，例如，在图像的平滑区域加更多噪声，在纹理区域加更少噪声？）
4.  **研究思路**：
    *   阅读DDIM、DEIS等快速采样算法的论文。
    *   查阅关于非高斯扩散或泊松流生成模型（PFGM）的研究。
    *   探索将扩散模型与自编码器结合（如LDM）或与最优传输理论结合的工作。

</details>

## 本章小结

在本章中，我们深入剖析了DDPM的内部工作原理：
- **核心思想**：通过将复杂的变分下界目标简化为预测噪声的均方误差，DDPM极大地简化了扩散模型的训练。
- **数学推导**：我们理解了前向过程的闭式解、反向过程的后验分布，以及它们如何共同导出了最终的简化损失函数。
- **关键组件**：我们分析了噪声调度、网络参数化（预测噪声vs预测图像）、方差选择等关键设计决策的重要性。
- **训练与采样**：我们掌握了DDPM的完整训练和采样算法流程。

DDPM为后续扩散模型的发展奠定了坚实的基础。下一章，我们将从另一个角度——分数匹配（Score Matching）——来理解扩散过程，并看到这两个看似不同的框架如何最终在统一的SDE/PDE视角下完美融合。
